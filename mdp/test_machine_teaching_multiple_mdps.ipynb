{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Construct the path to the directory you want to import from\n",
    "# '..' goes up one directory level. Adjust as needed.\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "# Add the directory to sys.path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from gridworld_env_layout import GridWorldMDPFromLayoutEnv\n",
    "from gridworld_env import NoisyLinearRewardFeaturizedGridWorldEnv\n",
    "import numpy as np\n",
    "from agent.q_learning_agent import ValueIteration, PolicyEvaluation\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "from utils.machine_teaching_utils import (build_Pi_from_q,\n",
    "compute_successor_features_iterative_from_q,\n",
    " derive_constraints_from_q_ties, \n",
    " remove_redundant_constraints,\n",
    " generate_candidates_from_q,\n",
    " plot_halfspace_intersection_2d)\n",
    "\n",
    "from reward_learning.multi_env_birl import MultiEnvBIRL\n",
    "from utils.common_helper import  bucket_and_dedup\n",
    "\n",
    "from utils.env_helper import print_policy_from_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a37c85",
   "metadata": {},
   "source": [
    "### Making different mdps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08576b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_TRUE = np.array([-10, -2])/np.linalg.norm([-10, -2])\n",
    "\n",
    "layout_1 = [\n",
    "    [\"blue\",  \"red\", \"blue\"],\n",
    "    [\"blue\", \"blue\",  \"blue\"],\n",
    "]\n",
    "\n",
    "layout_2 = [\n",
    "    [\"blue\",  \"red\", \"blue\"],\n",
    "    [\"red\", \"blue\",  \"blue\"],\n",
    "]\n",
    "\n",
    "layout_3 = [\n",
    "    [\"blue\",  \"red\", \"blue\"],\n",
    "    [\"blue\", \"red\",  \"blue\"],\n",
    "]\n",
    "\n",
    "layout_4 = [\n",
    "    [\"blue\",  \"blue\", \"red\"],\n",
    "    [\"blue\", \"blue\",  \"blue\"],\n",
    "]\n",
    "\n",
    "layout_5 = [\n",
    "    [\"blue\",  \"red\", \"blue\"],\n",
    "    [\"blue\", \"red\",  \"blue\"],\n",
    "]\n",
    "\n",
    "\n",
    "color_to_feature_map = {\n",
    "    \"red\":  [1.0, 0.0],\n",
    "    \"blue\": [0.0, 1.0],\n",
    "}\n",
    "\n",
    "rows, cols = 2, 3\n",
    "\n",
    "env_5 = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.99,\n",
    "    layout=layout_1,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0,\n",
    "    terminal_states=[0],\n",
    "    custom_feature_weights=W_TRUE,\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "env_2 = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.99,\n",
    "    layout=layout_2,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0,\n",
    "    terminal_states=[1],\n",
    "    custom_feature_weights=W_TRUE,\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "env_3 = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.99,\n",
    "    layout=layout_3,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0,\n",
    "    terminal_states=[2],\n",
    "    custom_feature_weights=W_TRUE,\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "env_4 = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.99,\n",
    "    layout=layout_4,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0,\n",
    "    terminal_states=[3],\n",
    "    custom_feature_weights=W_TRUE,\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "env_1 = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.99,\n",
    "    layout=layout_5,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0,\n",
    "    terminal_states=[0],\n",
    "    custom_feature_weights=W_TRUE,\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "envs = [env_1, env_2, env_3, env_4, env_5]\n",
    "\n",
    "vis = [ValueIteration(env) for env in envs]\n",
    "\n",
    "envs_V = [vi.run_value_iteration(epsilon=1e-10) for vi in vis]\n",
    "\n",
    "envs_Q = [vi.get_q_values() for vi in vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, env in enumerate(envs):\n",
    "\n",
    "    print_policy_from_q(envs_Q[i], rows=2, cols=3, tie_eps=1e-8, terminal_states=env.terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Pi_from_q_family(envs, Q_list, tie_eps=1e-10):\n",
    "    Pis = []\n",
    "    for env, q in zip(envs, Q_list):\n",
    "        Pis.append(build_Pi_from_q(env, q, tie_eps=tie_eps))\n",
    "    return Pis\n",
    "\n",
    "def compute_successor_features_family(envs, Q_list, **kw):\n",
    "    out = []\n",
    "    for env, q in zip(envs, Q_list):\n",
    "        mu_sa, mu_s, Phi, P_pi = compute_successor_features_iterative_from_q(env, q, **kw)\n",
    "        out.append((mu_sa, mu_s, Phi, P_pi))\n",
    "    return out\n",
    "\n",
    "def derive_constraints_family(SFs, Q_list, envs, tie_eps=1e-10,\n",
    "                              skip_terminals=True, normalize=True, tol=1e-12,\n",
    "                              precision=1e-3, lp_epsilon=1e-4):\n",
    "    U_per_mdp = []\n",
    "    all_H = []\n",
    "    for (mu_sa, _, _, _), q, env in zip(SFs, Q_list, envs):\n",
    "        cons = derive_constraints_from_q_ties(mu_sa, q, env,\n",
    "                                              tie_eps=tie_eps,\n",
    "                                              skip_terminals=skip_terminals,\n",
    "                                              normalize=normalize, tol=tol)\n",
    "        H_i = [c[0] if isinstance(c, tuple) else c for c in cons]  # keep vectors\n",
    "        U_per_mdp.append(H_i)\n",
    "        all_H.extend(H_i)\n",
    "\n",
    "    # spherical dedup by cosine threshold\n",
    "    pre = []\n",
    "    for v in all_H:\n",
    "        if not any(np.dot(v, u) / (np.linalg.norm(v)*np.linalg.norm(u)) > 1-precision for u in pre):\n",
    "            pre.append(v)\n",
    "\n",
    "    # LP redundancy removal on the family set (your existing routine)\n",
    "    U_global = np.array(remove_redundant_constraints(pre, epsilon=lp_epsilon))\n",
    "    return U_per_mdp, U_global\n",
    "\n",
    "def generate_candidates_from_q_family(envs, Q_list, **kw):\n",
    "    C = []\n",
    "    for env, q in zip(envs, Q_list):\n",
    "        C.append(generate_candidates_from_q(env, q, **kw))\n",
    "    return C\n",
    "\n",
    "def scot_greedy_family_unweighted(\n",
    "    U_global,       # np.ndarray of unit normals in R^d (family universe)\n",
    "    candidates,     # List[List[Traj]] per MDP: candidates[i] = list of trajectories for MDP i\n",
    "    SFs,            # List[(mu_sa, mu_s, Phi, P_pi)] per MDP; we use mu_sa only\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Unweighted greedy set cover over family constraints.\n",
    "    Returns:\n",
    "        chosen: List[(mdp_index, trajectory)] until U_global is covered or no progress possible.\n",
    "    \"\"\"\n",
    "    m = len(candidates)\n",
    "    mu_sa_list = [sf[0] for sf in SFs]  # (S_i, A_i, d)\n",
    "\n",
    "    # stable key for a direction vector\n",
    "    def key_for(v):\n",
    "        n = np.linalg.norm(v)\n",
    "        if not np.isfinite(n) or n == 0.0:\n",
    "            return (\"ZERO\",)\n",
    "        vv = v / n if normalize else v\n",
    "        return tuple(np.round(vv, round_decimals))\n",
    "\n",
    "    # index universe by direction key\n",
    "    key_to_uix = {}\n",
    "    for idx, v in enumerate(U_global):\n",
    "        key_to_uix.setdefault(key_for(v), []).append(idx)\n",
    "\n",
    "    universe = set(range(len(U_global)))\n",
    "    covered  = set()\n",
    "    chosen   = []\n",
    "\n",
    "    # precompute coverage set for each candidate trajectory (against U_global)\n",
    "    cov = [[set() for _ in cand_i] for cand_i in candidates]\n",
    "    for i, (cand_i, mu_sa) in enumerate(zip(candidates, mu_sa_list)):\n",
    "        S_i, A_i, _ = mu_sa.shape\n",
    "        for j, tau in enumerate(cand_i):\n",
    "            covered_by_tau = set()\n",
    "            for (s, a) in tau:\n",
    "                psi_star = mu_sa[s, a]\n",
    "                for b in range(A_i):\n",
    "                    if b == a:\n",
    "                        continue\n",
    "                    v = psi_star - mu_sa[s, b]\n",
    "                    k = key_for(v)\n",
    "                    if k in key_to_uix:\n",
    "                        covered_by_tau.update(key_to_uix[k])\n",
    "            cov[i][j] = covered_by_tau\n",
    "\n",
    "    # greedy loop: pick the candidate with the largest new coverage\n",
    "    while True:\n",
    "        uncovered = universe - covered\n",
    "        if not uncovered:\n",
    "            break\n",
    "\n",
    "        best_gain = 0\n",
    "        best_item = None\n",
    "        best_new  = None\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(len(candidates[i])):\n",
    "                if not cov[i][j]:\n",
    "                    continue\n",
    "                new_cover = uncovered & cov[i][j]\n",
    "                gain = len(new_cover)\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_item, best_new = gain, (i, j), new_cover\n",
    "\n",
    "        if best_item is None:  # no candidate adds new coverage\n",
    "            break\n",
    "\n",
    "        i, j = best_item\n",
    "        chosen.append((i, candidates[i][j]))\n",
    "        covered |= best_new\n",
    "\n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Successor features per MDP (from Q) ---\n",
    "SFs = compute_successor_features_family(\n",
    "    envs,\n",
    "    envs_Q,\n",
    "    convention=\"entering\",\n",
    "    zero_terminal_features=True,\n",
    "    tol=1e-10,\n",
    "    max_iters=10000,\n",
    ")\n",
    "\n",
    "# --- 2) Constraints: per-MDP and family (FBEC) ---\n",
    "U_per_mdp, U_global = derive_constraints_family(\n",
    "    SFs,\n",
    "    envs_Q,\n",
    "    envs,\n",
    "    tie_eps=1e-10,\n",
    "    skip_terminals=True,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "    precision=1e-3,\n",
    "    lp_epsilon=1e-4,\n",
    ")\n",
    "\n",
    "# (Optional) If you want to quickly visualize constraints before demos:\n",
    "# my_visualize_constraints(U_per_mdp=U_per_mdp, U_global=U_global, envs=envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a21e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_halfspace_intersection_2d(U_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, u in enumerate(U_per_mdp):\n",
    "\n",
    "    plot_halfspace_intersection_2d(u, title=f\"env{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afec5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3) Candidate demos per MDP (greedy policy rollouts from Q) ---\n",
    "candidates = generate_candidates_from_q_family(\n",
    "    envs,\n",
    "    envs_Q,\n",
    "    num_rollouts_per_state=5,   # adjust as you like\n",
    "    max_steps=1,\n",
    "    tie_eps=1e-10,\n",
    ")\n",
    "\n",
    "# --- 4) Unweighted greedy cover over family constraints ---\n",
    "chosen = scot_greedy_family_unweighted(\n",
    "    U_global=U_global,\n",
    "    candidates=candidates,\n",
    "    SFs=SFs,\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    ")\n",
    "# chosen is: List[(mdp_index, trajectory_list_of_(s,a))]\n",
    "# Example: [(0, [(s0,a2), (s5,a1), ...]), (1, [...]), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b299bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def merge_chosen(chosen):\n",
    "    merged = defaultdict(list)\n",
    "    for env_idx, traj in chosen:\n",
    "        if traj:  # skip empty ones\n",
    "            merged[env_idx].extend(traj)\n",
    "    # convert to sorted list of tuples\n",
    "    return [(i, merged[i]) for i in sorted(merged.keys())]\n",
    "\n",
    "\n",
    "demos_by_env = merge_chosen(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d98356",
   "metadata": {},
   "outputs": [],
   "source": [
    "demos_by_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894cd1b",
   "metadata": {},
   "source": [
    "### running BIRL on multiple mdps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fed962",
   "metadata": {},
   "outputs": [],
   "source": [
    "birlf = MultiEnvBIRL(envs=envs, demos=demos_by_env, beta=10, epsilon=1e-4)\n",
    "birlf.run_mcmc(samples=2000, stepsize=0.1, normalize=True, adaptive=True)\n",
    "w_map  = birlf.get_map_solution()\n",
    "w_mean = birlf.get_mean_solution(burn_frac=0.2, skip_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e90313",
   "metadata": {},
   "source": [
    "### Solving MDPS using learned reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- main: solve & print policies for all envs using MAP weights ----\n",
    "def print_policies_with_map(birl, shapes=None, tie_eps=1e-8):\n",
    "    \"\"\"\n",
    "    birl: MultiEnvBIRL instance already run (has map_sol)\n",
    "    shapes: optional list/dict mapping env index -> (rows, cols)\n",
    "            e.g., shapes={0:(5,7), 1:(4,4)} or list like [(5,7),(4,4),...]\n",
    "    \"\"\"\n",
    "    w_map = birl.get_map_solution()\n",
    "    assert w_map is not None, \"MAP solution is None. Run birl.run_mcmc(...) first.\"\n",
    "\n",
    "    for i, env in enumerate(birl.envs):\n",
    "        # Set weights and solve VI fresh\n",
    "        env.set_feature_weights(w_map)\n",
    "        vi = ValueIteration(env)\n",
    "        vi.run_value_iteration(epsilon=getattr(birl, \"epsilon\", 1e-4))\n",
    "        Q = vi.get_q_values()\n",
    "\n",
    "        # terminals if available\n",
    "        ts = getattr(env, \"terminal_states\", []) or []\n",
    "\n",
    "\n",
    "        rows, cols = (shapes[i] if isinstance(shapes, (list, tuple)) else shapes[i])\n",
    "\n",
    "        #print(f\"=== Env {i} | rows={rows}, cols={cols} ===\")\n",
    "        print_policy_from_q(Q, rows, cols, tie_eps=tie_eps, terminal_states=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14acd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_policies_with_map(birlf, shapes=[(env.rows, env.columns) for env in envs], tie_eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a9bb0",
   "metadata": {},
   "source": [
    "### generating random optimal demonstrations across env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_optimal_sa_pairs(\n",
    "    envs, Q_list, n, *,\n",
    "    tie_eps=1e-10,\n",
    "    skip_terminals=True,\n",
    "    seed=None,\n",
    "    return_shape=\"flat\",   # \"flat\": [(env_i, s, a)], \"scot\": [(env_i, [(s,a)])]\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    assert len(envs) == len(Q_list), \"envs and Q_list must have same length.\"\n",
    "\n",
    "    # Π from Q (already uniform over arg-max ties)\n",
    "    Pis = [build_Pi_from_q(env, q, tie_eps=tie_eps) for env, q in zip(envs, Q_list)]\n",
    "\n",
    "    # Eligible states per env (non-terminal if requested) with nonzero Π mass\n",
    "    eligible_states = []\n",
    "    for env, Pi in zip(envs, Pis):\n",
    "        S = env.get_num_states()\n",
    "        terms = set(getattr(env, \"terminal_states\", []) or [])\n",
    "        if skip_terminals:\n",
    "            mask = np.array([s not in terms for s in range(S)], dtype=bool)\n",
    "        else:\n",
    "            mask = np.ones(S, dtype=bool)\n",
    "        elig = np.flatnonzero(mask & (Pi.sum(axis=1) > 0))\n",
    "        eligible_states.append(elig)\n",
    "\n",
    "    env_pool = [i for i, es in enumerate(eligible_states) if es.size > 0]\n",
    "    if not env_pool:\n",
    "        raise RuntimeError(\"No eligible states found in any env (check terminals/Q).\")\n",
    "\n",
    "    out = []\n",
    "    for _ in range(n):\n",
    "        i = int(rng.choice(env_pool))              # pick env uniformly\n",
    "        s = int(rng.choice(eligible_states[i]))    # pick state uniformly within env\n",
    "        p = Pis[i][s]\n",
    "        p = p / p.sum()                            # defensive normalize\n",
    "        a = int(rng.choice(len(p), p=p))           # sample among co-optimal actions\n",
    "\n",
    "        out.append((i, [(s, a)]) if return_shape == \"scot\" else (i, s, a))\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_optimal_sa_pairs_like_scot(\n",
    "    envs, Q_list, chosen_from_scot, *,\n",
    "    tie_eps=1e-10,\n",
    "    skip_terminals=True,\n",
    "    seed=None,\n",
    "    return_shape=\"scot\",\n",
    "):\n",
    "    n = sum(len(traj) for _, traj in chosen_from_scot)\n",
    "    return sample_optimal_sa_pairs(\n",
    "        envs, Q_list, n,\n",
    "        tie_eps=tie_eps,\n",
    "        skip_terminals=skip_terminals,\n",
    "        seed=seed,\n",
    "        return_shape=return_shape,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765cd21",
   "metadata": {},
   "source": [
    "### evaluating SCOT against random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4214bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimal_sa_pairs_like_scot(envs, envs_Q, demos_by_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f9236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
