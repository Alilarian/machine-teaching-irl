{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321d23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from gridworld_env_layout import GridWorldMDPFromLayoutEnv\n",
    "from gridworld_env import NoisyLinearRewardFeaturizedGridWorldEnv\n",
    "import numpy as np\n",
    "from agent.q_learning_agent import ValueIteration, PolicyEvaluation\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "from utils import (build_Pi_from_q,\n",
    "compute_successor_features_iterative_from_q,\n",
    " derive_constraints_from_q_ties, \n",
    " remove_redundant_constraints,\n",
    " #generate_candidates_from_q,\n",
    " plot_halfspace_intersection_2d,\n",
    " compare_regret_from_Q,\n",
    " #sample_optimal_sa_pairs_like_scot,\n",
    ")\n",
    "\n",
    "from utils.mdp_generator import generate_random_gridworld_envs\n",
    "from reward_learning.multi_env_birl import MultiEnvBIRL\n",
    "from utils.common_helper import  bucket_and_dedup, calculate_expected_value_difference\n",
    "from utils.env_helper import print_policy_from_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a37c85",
   "metadata": {},
   "source": [
    "### Making different mdps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_TRUE = np.array([-10, -2]) / np.linalg.norm([-10, -2])\n",
    "\n",
    "envs, meta = generate_random_gridworld_envs(\n",
    "    n_envs=50,\n",
    "    rows=3, cols=3,\n",
    "    color_to_feature_map={\"red\":[1.0,0.0], \"blue\":[0.0,1.0]},\n",
    "    palette=(\"red\",\"blue\"),\n",
    "    p_color_range={\"red\":(0.2,0.6), \"blue\":(0.4,0.8)},\n",
    "    terminal_policy=dict(kind=\"random_k\", k_min=0, k_max=1, p_no_terminal=0.1),\n",
    "    gamma_range=(0.98, 0.995),\n",
    "    noise_prob_range=(0.0, 0.0),\n",
    "    w_mode=\"fixed\",\n",
    "    W_fixed=W_TRUE,\n",
    "    seed=45,\n",
    "    GridEnvClass=GridWorldMDPFromLayoutEnv,\n",
    ")\n",
    "\n",
    "vis = [ValueIteration(env) for env in envs]\n",
    "envs_V = [vi.run_value_iteration(epsilon=1e-10) for vi in vis]\n",
    "envs_Q = [vi.get_q_values() for vi in vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfa6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs[15].layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ded7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = envs[:]\n",
    "# vis = vis[:]\n",
    "# envs_V = envs_V[:]\n",
    "# envs_Q = envs_Q[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08576b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_TRUE = np.array([-10, -2])/np.linalg.norm([-10, -2])\n",
    "\n",
    "# layout_1 = [\n",
    "#     [\"blue\",  \"red\", \"blue\"],\n",
    "#     [\"blue\", \"blue\",  \"blue\"],\n",
    "# ]\n",
    "\n",
    "# layout_2 = [\n",
    "#     [\"blue\",  \"red\", \"blue\"],\n",
    "#     [\"red\", \"blue\",  \"blue\"],\n",
    "# ]\n",
    "\n",
    "# layout_3 = [\n",
    "#     [\"blue\",  \"red\", \"blue\"],\n",
    "#     [\"blue\", \"red\",  \"blue\"],\n",
    "# ]\n",
    "\n",
    "# layout_4 = [\n",
    "#     [\"blue\",  \"blue\", \"red\"],\n",
    "#     [\"blue\", \"blue\",  \"blue\"],\n",
    "# ]\n",
    "\n",
    "# layout_5 = [\n",
    "#     [\"blue\",  \"red\", \"blue\"],\n",
    "#     [\"blue\", \"red\",  \"blue\"],\n",
    "# ]\n",
    "\n",
    "\n",
    "# color_to_feature_map = {\n",
    "#     \"red\":  [1.0, 0.0],\n",
    "#     \"blue\": [0.0, 1.0],\n",
    "# }\n",
    "\n",
    "# rows, cols = 2, 3\n",
    "\n",
    "# env_5 = GridWorldMDPFromLayoutEnv(\n",
    "#     gamma=0.99,\n",
    "#     layout=layout_1,\n",
    "#     color_to_feature_map=color_to_feature_map,\n",
    "#     noise_prob=0,\n",
    "#     terminal_states=[0],\n",
    "#     custom_feature_weights=W_TRUE,\n",
    "#     render_mode=None,\n",
    "# )\n",
    "\n",
    "# env_2 = GridWorldMDPFromLayoutEnv(\n",
    "#     gamma=0.99,\n",
    "#     layout=layout_2,\n",
    "#     color_to_feature_map=color_to_feature_map,\n",
    "#     noise_prob=0,\n",
    "#     terminal_states=[1],\n",
    "#     custom_feature_weights=W_TRUE,\n",
    "#     render_mode=None,\n",
    "# )\n",
    "\n",
    "# env_3 = GridWorldMDPFromLayoutEnv(\n",
    "#     gamma=0.99,\n",
    "#     layout=layout_3,\n",
    "#     color_to_feature_map=color_to_feature_map,\n",
    "#     noise_prob=0,\n",
    "#     terminal_states=[2],\n",
    "#     custom_feature_weights=W_TRUE,\n",
    "#     render_mode=None,\n",
    "# )\n",
    "\n",
    "# env_4 = GridWorldMDPFromLayoutEnv(\n",
    "#     gamma=0.99,\n",
    "#     layout=layout_4,\n",
    "#     color_to_feature_map=color_to_feature_map,\n",
    "#     noise_prob=0,\n",
    "#     terminal_states=[3],\n",
    "#     custom_feature_weights=W_TRUE,\n",
    "#     render_mode=None,\n",
    "# )\n",
    "\n",
    "# env_1 = GridWorldMDPFromLayoutEnv(\n",
    "#     gamma=0.99,\n",
    "#     layout=layout_5,\n",
    "#     color_to_feature_map=color_to_feature_map,\n",
    "#     noise_prob=0,\n",
    "#     terminal_states=[0],\n",
    "#     custom_feature_weights=W_TRUE,\n",
    "#     render_mode=None,\n",
    "# )\n",
    "\n",
    "# envs = [env_1, env_2, env_3, env_4, env_5]\n",
    "\n",
    "# vis = [ValueIteration(env) for env in envs]\n",
    "\n",
    "# envs_V = [vi.run_value_iteration(epsilon=1e-10) for vi in vis]\n",
    "\n",
    "# envs_Q = [vi.get_q_values() for vi in vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0446549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # True reward: strongly dislike red, slightly dislike blue\n",
    "# W_TRUE = np.array([-10.0, -1.0])  # red: -10, blue: -1\n",
    "# # No need to normalize for the env, but we can if you want\n",
    "# # W_TRUE = W_TRUE / np.linalg.norm(W_TRUE)\n",
    "\n",
    "# color_to_feature_map = {\n",
    "#     \"red\":  [1.0, 0.0],\n",
    "#     \"blue\": [0.0, 1.0],\n",
    "# }\n",
    "\n",
    "# # ============================================================\n",
    "# # MDP 1: Horizontal path — only sees red vs blue horizontally\n",
    "# # ============================================================\n",
    "# # Layout:\n",
    "# # [S] [red] [G]\n",
    "# # Only one path: right through red to goal\n",
    "# # Optimal: go right (but red is costly)\n",
    "# # This teaches: red is worse than blue → w[0] < w[1] ⇒ w[0] < 0 if blue is 0-reference\n",
    "\n",
    "# layout_1 = [\n",
    "#     [\"blue\", \"red\", \"blue\"]   # state 0: start, state 1: red, state 2: goal\n",
    "# ]\n",
    "\n",
    "# env_horizontal = GridWorldMDPFromLayoutEnv(\n",
    "#     gamma=0.99,\n",
    "#     layout=layout_1,\n",
    "#     color_to_feature_map=color_to_feature_map,\n",
    "#     noise_prob=0.0,\n",
    "#     terminal_states=[2],  # goal is index (0,2) → state 2\n",
    "#     custom_feature_weights=W_TRUE,\n",
    "#     render_mode=None,\n",
    "# )\n",
    "\n",
    "# # ============================================================\n",
    "# # MDP 2: Choice between red (up) and goal (right)\n",
    "# # ============================================================\n",
    "# # Layout 2x3:\n",
    "# # [S] [ ] [G]\n",
    "# # [ ] [red] [ ]\n",
    "# #\n",
    "# # Start at (0,0), goal at (0,2)\n",
    "# # From state (1,1) = red, agent can go up to (0,1) or right to (0,2)=G\n",
    "# # But there's a key state: let's put start at (1,0), and red at (1,1)\n",
    "# # Better: force a choice\n",
    "\n",
    "# layout_2 = [\n",
    "#     [\"blue\", \"blue\", \"blue\"],   # row 0: 0=S, 1=, 2=G\n",
    "#     [\"blue\", \"red\",  \"blue\"],   # row 1: 3=, 4=red, 5=\n",
    "# ]\n",
    "\n",
    "# # Start at bottom-left (1,0)=state 3, goal top-right (0,2)=state 2\n",
    "# # Optimal path: up to (0,0), right to (0,1), right to G\n",
    "# # But at state (1,1)=red, agent could go up to (0,1) or left/right\n",
    "# # We need a state where two actions are compared\n",
    "\n",
    "# # Best: start at (1,0), red at (1,1), goal at (0,2)\n",
    "# # From (1,1), going up goes to (0,1) [blue], right goes to (0,2) [blue] but goal\n",
    "\n",
    "# # Let's make it clear:\n",
    "# # Start at state 3 (1,0), from there can go up to state 0 (blue) or right to state 4 (red)\n",
    "# # From state 4 (red), can go up to state 1 (blue) or left back\n",
    "\n",
    "# # So at state 3: choosing right (to red) vs up (to blue then goal)\n",
    "# # This creates: mu_right - mu_up = feature(red) - feature(blue) = [1,0] - [0,1] = [1,-1]\n",
    "\n",
    "# env_choice = GridWorldMDPFromLayoutEnv(\n",
    "#     gamma=0.99,\n",
    "#     layout=layout_2,\n",
    "#     color_to_feature_map=color_to_feature_map,\n",
    "#     noise_prob=0.0,\n",
    "#     terminal_states=[2],  # goal is (0,2)\n",
    "#     custom_feature_weights=W_TRUE,\n",
    "#     render_mode=None,\n",
    "# )\n",
    "\n",
    "# # ============================================================\n",
    "# # Full list\n",
    "# # ============================================================\n",
    "# envs = [env_horizontal, env_choice]\n",
    "\n",
    "# # Solve\n",
    "# envs_V = [ValueIteration(env) for env in envs]\n",
    "# for vi in vis:\n",
    "#     vi.run_value_iteration(epsilon=1e-10)\n",
    "\n",
    "# envs_Q = [vi.get_q_values() for vi in vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, env in enumerate(envs):\n",
    "\n",
    "    print_policy_from_q(envs_Q[i], rows=env.rows, cols=env.columns, tie_eps=1e-8, terminal_states=env.terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Pi_from_q_family(envs, Q_list, tie_eps=1e-10):\n",
    "    Pis = []\n",
    "    for env, q in zip(envs, Q_list):\n",
    "        Pis.append(build_Pi_from_q(env, q, tie_eps=tie_eps))\n",
    "    return Pis\n",
    "\n",
    "def compute_successor_features_family(envs, Q_list, **kw):\n",
    "    out = []\n",
    "    for env, q in zip(envs, Q_list):\n",
    "        mu_sa, mu_s, Phi, P_pi = compute_successor_features_iterative_from_q(env, q, **kw)\n",
    "        out.append((mu_sa, mu_s, Phi, P_pi))\n",
    "    return out\n",
    "\n",
    "def derive_constraints_family(SFs, Q_list, envs, tie_eps=1e-10,\n",
    "                              skip_terminals=True, normalize=True, tol=1e-12,\n",
    "                              precision=1e-3, lp_epsilon=1e-4):\n",
    "    U_per_mdp = []\n",
    "    all_H = []\n",
    "    for (mu_sa, _, _, _), q, env in zip(SFs, Q_list, envs):\n",
    "        cons = derive_constraints_from_q_ties(mu_sa, q, env,\n",
    "                                              tie_eps=tie_eps,\n",
    "                                              skip_terminals=skip_terminals,\n",
    "                                              normalize=normalize, tol=tol)\n",
    "        H_i = [c[0] if isinstance(c, tuple) else c for c in cons]  # keep vectors\n",
    "        U_per_mdp.append(H_i)\n",
    "        all_H.extend(H_i)\n",
    "\n",
    "    # spherical dedup by cosine threshold\n",
    "    pre = []\n",
    "    for v in all_H:\n",
    "        if not any(np.dot(v, u) / (np.linalg.norm(v)*np.linalg.norm(u)) > 1-precision for u in pre):\n",
    "            pre.append(v)\n",
    "\n",
    "    # LP redundancy removal on the family set (your existing routine)\n",
    "    U_global = np.array(remove_redundant_constraints(pre, epsilon=lp_epsilon))\n",
    "    return U_per_mdp, U_global\n",
    "\n",
    "# def derive_constraints_family_unique_per_mdp(SFs, Q_list, envs, tie_eps=1e-10,\n",
    "#                               skip_terminals=True, normalize=True, tol=1e-12,\n",
    "#                               precision=1e-3, lp_epsilon=1e-4):\n",
    "#     U_per_mdp = []\n",
    "#     all_H = []\n",
    "#     for (mu_sa, _, _, _), q, env in zip(SFs, Q_list, envs):\n",
    "#         cons = derive_constraints_from_q_ties(mu_sa, q, env,\n",
    "#                                               tie_eps=tie_eps,\n",
    "#                                               skip_terminals=skip_terminals,\n",
    "#                                               normalize=normalize, tol=tol)\n",
    "#         H_i = [c[0] if isinstance(c, tuple) else c for c in cons]  # keep vectors\n",
    "        \n",
    "#         #U_per_mdp.append(H_i)\n",
    "#         H_i = np.array(remove_redundant_constraints(H_i, epsilon=lp_epsilon))\n",
    "#         U_per_mdp.append(H_i)\n",
    "#         all_H.extend(H_i)\n",
    "\n",
    "#     # spherical dedup by cosine threshold\n",
    "#     pre = []\n",
    "#     for v in all_H:\n",
    "#         if not any(np.dot(v, u) / (np.linalg.norm(v)*np.linalg.norm(u)) > 1-precision for u in pre):\n",
    "#             pre.append(v)\n",
    "\n",
    "#     # LP redundancy removal on the family set (your existing routine)\n",
    "#     #U_global = np.array(remove_redundant_constraints(pre, epsilon=lp_epsilon))\n",
    "#     U_global = np.array(pre)\n",
    "#     return U_per_mdp, U_global\n",
    "\n",
    "def generate_candidates_from_q_family(envs, Q_list, **kw):\n",
    "    C = []\n",
    "    for env, q in zip(envs, Q_list):\n",
    "        C.append(generate_candidates_from_q(env, q, **kw))\n",
    "    return C\n",
    "\n",
    "def scot_greedy_family_unweighted(\n",
    "    U_global,       # np.ndarray of unit normals in R^d (family universe)\n",
    "    candidates,     # List[List[Traj]] per MDP: candidates[i] = list of trajectories for MDP i\n",
    "    SFs,            # List[(mu_sa, mu_s, Phi, P_pi)] per MDP; we use mu_sa only\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Unweighted greedy set cover over family constraints.\n",
    "    Returns:\n",
    "        chosen: List[(mdp_index, trajectory)] until U_global is covered or no progress possible.\n",
    "    \"\"\"\n",
    "    m = len(candidates)\n",
    "    mu_sa_list = [sf[0] for sf in SFs]  # (S_i, A_i, d)\n",
    "\n",
    "    # stable key for a direction vector\n",
    "    def key_for(v):\n",
    "        n = np.linalg.norm(v)\n",
    "        if not np.isfinite(n) or n == 0.0:\n",
    "            return (\"ZERO\",)\n",
    "        vv = v / n if normalize else v\n",
    "        return tuple(np.round(vv, round_decimals))\n",
    "\n",
    "    # index universe by direction key\n",
    "    key_to_uix = {}\n",
    "    for idx, v in enumerate(U_global):\n",
    "        key_to_uix.setdefault(key_for(v), []).append(idx)\n",
    "\n",
    "    universe = set(range(len(U_global)))\n",
    "    covered  = set()\n",
    "    chosen   = []\n",
    "\n",
    "    # precompute coverage set for each candidate trajectory (against U_global)\n",
    "    cov = [[set() for _ in cand_i] for cand_i in candidates]\n",
    "    for i, (cand_i, mu_sa) in enumerate(zip(candidates, mu_sa_list)):\n",
    "        S_i, A_i, _ = mu_sa.shape\n",
    "        for j, tau in enumerate(cand_i):\n",
    "            covered_by_tau = set()\n",
    "            for (s, a) in tau:\n",
    "                psi_star = mu_sa[s, a]\n",
    "                for b in range(A_i):\n",
    "                    if b == a:\n",
    "                        continue\n",
    "                    v = psi_star - mu_sa[s, b]\n",
    "                    k = key_for(v)\n",
    "                    if k in key_to_uix:\n",
    "                        covered_by_tau.update(key_to_uix[k])\n",
    "            cov[i][j] = covered_by_tau\n",
    "\n",
    "    # greedy loop: pick the candidate with the largest new coverage\n",
    "    while True:\n",
    "        uncovered = universe - covered\n",
    "        if not uncovered:\n",
    "            break\n",
    "\n",
    "        best_gain = 0\n",
    "        best_item = None\n",
    "        best_new  = None\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(len(candidates[i])):\n",
    "                if not cov[i][j]:\n",
    "                    continue\n",
    "                new_cover = uncovered & cov[i][j]\n",
    "                gain = len(new_cover)\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_item, best_new = gain, (i, j), new_cover\n",
    "\n",
    "        if best_item is None:  # no candidate adds new coverage\n",
    "            break\n",
    "\n",
    "        i, j = best_item\n",
    "        chosen.append((i, candidates[i][j]))\n",
    "        covered |= best_new\n",
    "\n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFs = compute_successor_features_family(\n",
    "    envs,\n",
    "    envs_Q,\n",
    "    convention=\"entering\",\n",
    "    zero_terminal_features=True,\n",
    "    tol=1e-10,\n",
    "    max_iters=10000,\n",
    ")\n",
    "\n",
    "U_per_mdp, U_global = derive_constraints_family(\n",
    "    SFs,\n",
    "    envs_Q,\n",
    "    envs,\n",
    "    tie_eps=1e-10,\n",
    "    skip_terminals=True,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "    precision=1e-3,\n",
    "    lp_epsilon=1e-4,\n",
    ")\n",
    "\n",
    "# U_per_mdp_, U_global_ = derive_constraints_family_unique_per_mdp(\n",
    "#     SFs,\n",
    "#     envs_Q,\n",
    "#     envs,\n",
    "#     tie_eps=1e-10,\n",
    "#     skip_terminals=True,\n",
    "#     normalize=True,\n",
    "#     tol=1e-12,\n",
    "#     precision=1e-3,\n",
    "#     lp_epsilon=1e-4,\n",
    "# )\n",
    "\n",
    "# (Optional) If you want to quickly visualize constraints before demos:\n",
    "# my_visualize_constraints(U_per_mdp=U_per_mdp, U_global=U_global, envs=envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_global_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a21e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_halfspace_intersection_2d(U_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_halfspace_intersection_2d(U_global_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, u in enumerate(U_per_mdp):\n",
    "    plot_halfspace_intersection_2d(u, title=f\"env{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afec5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Candidate demos per MDP (greedy policy rollouts from Q) ---\n",
    "candidates = generate_candidates_from_q_family(\n",
    "    envs,\n",
    "    envs_Q,\n",
    "    num_rollouts_per_state=5,   # adjust as you like\n",
    "    max_steps=1,\n",
    "    tie_eps=1e-10,\n",
    ")\n",
    "\n",
    "# --- 4) Unweighted greedy cover over family constraints ---\n",
    "chosen = scot_greedy_family_unweighted(\n",
    "    U_global=U_global,\n",
    "    candidates=candidates,\n",
    "    SFs=SFs,\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    ")\n",
    "# chosen is: List[(mdp_index, trajectory_list_of_(s,a))]\n",
    "# Example: [(0, [(s0,a2), (s5,a1), ...]), (1, [...]), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b299bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# def merge_chosen(chosen):\n",
    "#     merged = defaultdict(list)\n",
    "#     for env_idx, traj in chosen:\n",
    "#         if traj:  # skip empty ones\n",
    "#             merged[env_idx].extend(traj)\n",
    "#     # convert to sorted list of tuples\n",
    "#     return [(i, merged[i]) for i in sorted(merged.keys())]\n",
    "\n",
    "\n",
    "# demos_by_env = merge_chosen(chosen)\n",
    "\n",
    "# birlf = MultiEnvBIRL(envs=envs, demos=demos_by_env, beta=10, epsilon=1e-4)\n",
    "# birlf.run_mcmc(samples=2000, stepsize=0.1, normalize=True, adaptive=True)\n",
    "# w_map  = birlf.get_map_solution()\n",
    "# w_mean = birlf.get_mean_solution(burn_frac=0.2, skip_rate=10)\n",
    "\n",
    "# ---- main: solve & print policies for all envs using MAP weights ----\n",
    "# def print_policies_with_map(birl, shapes=None, tie_eps=1e-8):\n",
    "#     \"\"\"\n",
    "#     birl: MultiEnvBIRL instance already run (has map_sol)\n",
    "#     shapes: optional list/dict mapping env index -> (rows, cols)\n",
    "#             e.g., shapes={0:(5,7), 1:(4,4)} or list like [(5,7),(4,4),...]\n",
    "#     \"\"\"\n",
    "#     w_map = birl.get_map_solution()\n",
    "#     assert w_map is not None, \"MAP solution is None. Run birl.run_mcmc(...) first.\"\n",
    "\n",
    "#     for i, env in enumerate(birl.envs):\n",
    "#         # Set weights and solve VI fresh\n",
    "#         env.set_feature_weights(w_map)\n",
    "#         vi = ValueIteration(env)\n",
    "#         vi.run_value_iteration(epsilon=getattr(birl, \"epsilon\", 1e-4))\n",
    "#         Q = vi.get_q_values()\n",
    "\n",
    "#         # terminals if available\n",
    "#         ts = getattr(env, \"terminal_states\", []) or []\n",
    "\n",
    "\n",
    "#         rows, cols = (shapes[i] if isinstance(shapes, (list, tuple)) else shapes[i])\n",
    "\n",
    "#         #print(f\"=== Env {i} | rows={rows}, cols={cols} ===\")\n",
    "#         print_policy_from_q(Q, rows, cols, tie_eps=tie_eps, terminal_states=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765cd21",
   "metadata": {},
   "source": [
    "### evaluating SCOT against random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4214bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chosen(chosen):\n",
    "    merged = defaultdict(list)\n",
    "    for env_idx, traj in chosen:\n",
    "        if traj:  # skip empty ones\n",
    "            merged[env_idx].extend(traj)\n",
    "    # convert to sorted list of tuples\n",
    "    return [(i, merged[i]) for i in sorted(merged.keys())]\n",
    "\n",
    "def compute_Q_from_weights_with_VI(env, w, vi_epsilon=1e-6):\n",
    "    old_w = None\n",
    "    if hasattr(env, \"get_feature_weights\") and callable(getattr(env, \"get_feature_weights\")):\n",
    "        old_w = env.get_feature_weights()\n",
    "    elif hasattr(env, \"feature_weights\"):\n",
    "        old_w = np.array(env.feature_weights, copy=True)\n",
    "\n",
    "    env.set_feature_weights(w)\n",
    "    vi = ValueIteration(env)\n",
    "    vi.run_value_iteration(epsilon=vi_epsilon)\n",
    "    Q = vi.get_q_values()\n",
    "\n",
    "    if old_w is not None:\n",
    "        env.set_feature_weights(old_w)\n",
    "    return Q\n",
    "\n",
    "def regrets_from_Q(envs, Q_list, *, tie_eps=1e-10, epsilon=1e-4, normalize_with_random_policy=False):\n",
    "    if len(Q_list) != len(envs):\n",
    "        raise ValueError(f\"Q_list length {len(Q_list)} must match envs length {len(envs)}\")\n",
    "    regs = []\n",
    "    for idx, (env, Q) in enumerate(zip(envs, Q_list)):\n",
    "        if Q is None:\n",
    "            raise ValueError(f\"Q_list[{idx}] is None\")\n",
    "        pi = build_Pi_from_q(env, Q, tie_eps=tie_eps)\n",
    "        r = calculate_expected_value_difference(\n",
    "            eval_policy=pi,\n",
    "            env=env,\n",
    "            epsilon=epsilon,\n",
    "            normalize_with_random_policy=normalize_with_random_policy,\n",
    "        )\n",
    "        regs.append(float(r))\n",
    "    return np.asarray(regs, dtype=float)\n",
    "\n",
    "def birl_to_Q_lists(envs, demos_by_env, *, beta=10, epsilon=1e-4,\n",
    "                    samples=2000, stepsize=0.1, normalize=True, adaptive=True,\n",
    "                    burn_frac=0.2, skip_rate=10, vi_epsilon=1e-6):\n",
    "    birl = MultiEnvBIRL(envs=envs, demos=demos_by_env, beta=beta, epsilon=epsilon)\n",
    "    birl.run_mcmc(samples=samples, stepsize=stepsize, normalize=normalize, adaptive=adaptive)\n",
    "    w_map  = birl.get_map_solution()\n",
    "    w_mean = birl.get_mean_solution(burn_frac=burn_frac, skip_rate=skip_rate)\n",
    "    if w_map is None or w_mean is None:\n",
    "        raise RuntimeError(\"BIRL did not return weights (w_map or w_mean is None).\")\n",
    "\n",
    "    Q_map_list  = [compute_Q_from_weights_with_VI(env, w_map,  vi_epsilon=vi_epsilon)  for env in envs]\n",
    "    Q_mean_list = [compute_Q_from_weights_with_VI(env, w_mean, vi_epsilon=vi_epsilon) for env in envs]\n",
    "    return (w_map, w_mean, Q_map_list, Q_mean_list, birl)\n",
    "\n",
    "def run_scot_vs_random_Q_regret(\n",
    "    envs,\n",
    "    chosen_scot,                  # [(env_i, [(s,a), ...]), ...]\n",
    "    make_random_chosen,           # callable: seed -> [(env_i, [(s,a), ...]), ...]\n",
    "    *,\n",
    "    n_random_trials=10,\n",
    "    seeds=None,\n",
    "    birl_kwargs=None,\n",
    "    vi_epsilon=1e-6,\n",
    "    tie_eps=1e-10,\n",
    "    regret_epsilon=1e-4,\n",
    "    normalize_regret=False,\n",
    "):\n",
    "    m = len(envs)\n",
    "    birl_kwargs = birl_kwargs or {}\n",
    "\n",
    "    # ----- SCOT -----\n",
    "    #demos_scot = group_demos_by_env(chosen_scot, m)\n",
    "    demos_scot = merge_chosen(chosen_scot)\n",
    "    w_scot_map, w_scot_mean, Q_scot_map, Q_scot_mean, birl_scot = birl_to_Q_lists(\n",
    "        envs, demos_scot, vi_epsilon=vi_epsilon, **birl_kwargs\n",
    "    )\n",
    "\n",
    "    reg_scot_map  = regrets_from_Q(envs, Q_scot_map,\n",
    "                                   tie_eps=tie_eps, epsilon=regret_epsilon,\n",
    "                                   normalize_with_random_policy=normalize_regret)\n",
    "    reg_scot_mean = regrets_from_Q(envs, Q_scot_mean,\n",
    "                                   tie_eps=tie_eps, epsilon=regret_epsilon,\n",
    "                                   normalize_with_random_policy=normalize_regret)\n",
    "\n",
    "    # ----- Random (multi-seed) -----\n",
    "    if seeds is None:\n",
    "        seeds = list(range(n_random_trials))\n",
    "\n",
    "    rand_map_regs_by_seed  = []\n",
    "    rand_mean_regs_by_seed = []\n",
    "\n",
    "    for sd in seeds:\n",
    "        chosen_rand = make_random_chosen(sd)  # must match chosen_scot shape\n",
    "        demos_rand  = merge_chosen(chosen_rand)\n",
    "        \n",
    "        _, _, Q_rand_map, Q_rand_mean, _ = birl_to_Q_lists(\n",
    "            envs, demos_rand, vi_epsilon=vi_epsilon, **birl_kwargs\n",
    "        )\n",
    "\n",
    "        reg_rand_map  = regrets_from_Q(envs, Q_rand_map,\n",
    "                                       tie_eps=tie_eps, epsilon=regret_epsilon,\n",
    "                                       normalize_with_random_policy=normalize_regret)\n",
    "        reg_rand_mean = regrets_from_Q(envs, Q_rand_mean,\n",
    "                                       tie_eps=tie_eps, epsilon=regret_epsilon,\n",
    "                                       normalize_with_random_policy=normalize_regret)\n",
    "\n",
    "        rand_map_regs_by_seed.append(reg_rand_map)\n",
    "        rand_mean_regs_by_seed.append(reg_rand_mean)\n",
    "\n",
    "    rand_map_regs_by_seed  = np.stack(rand_map_regs_by_seed,  axis=0)  # (T, M)\n",
    "    rand_mean_regs_by_seed = np.stack(rand_mean_regs_by_seed, axis=0)  # (T, M)\n",
    "\n",
    "    # Averages (per env)\n",
    "    rand_map_per_env_mean  = rand_map_regs_by_seed.mean(axis=0)\n",
    "    rand_mean_per_env_mean = rand_mean_regs_by_seed.mean(axis=0)\n",
    "\n",
    "    def _stats(x):\n",
    "        return dict(mean=float(np.mean(x)), std=float(np.std(x)),\n",
    "                    median=float(np.median(x)), min=float(np.min(x)), max=float(np.max(x)))\n",
    "\n",
    "    return {\n",
    "        \"SCOT\": {\n",
    "            \"w_map\": w_scot_map,\n",
    "            \"w_mean\": w_scot_mean,\n",
    "            \"regret_map_per_env\":  reg_scot_map,\n",
    "            \"regret_mean_per_env\": reg_scot_mean,\n",
    "            \"summary_map\":  _stats(reg_scot_map),\n",
    "            \"summary_mean\": _stats(reg_scot_mean),\n",
    "        },\n",
    "        \"RANDOM_over_seeds\": {\n",
    "            \"regret_map_by_seed\":  rand_map_regs_by_seed,\n",
    "            \"regret_mean_by_seed\": rand_mean_regs_by_seed,\n",
    "            \"per_env_seed_mean_map\":  rand_map_per_env_mean,\n",
    "            \"per_env_seed_mean_mean\": rand_mean_per_env_mean,\n",
    "            \"summary_seed_mean_map\":  _stats(rand_map_per_env_mean),\n",
    "            \"summary_seed_mean_mean\": _stats(rand_mean_per_env_mean),\n",
    "            \"seeds\": seeds,\n",
    "        },\n",
    "        \"Q_lists\": {\n",
    "            \"SCOT_MAP\":  Q_scot_map,\n",
    "            \"SCOT_MEAN\": Q_scot_mean,\n",
    "        },\n",
    "        \"birl\": {\"SCOT\": birl_scot},\n",
    "        \"config\": {\n",
    "            \"vi_epsilon\": vi_epsilon,\n",
    "            \"regret_epsilon\": regret_epsilon,\n",
    "            \"tie_eps\": tie_eps,\n",
    "            \"normalize_regret\": normalize_regret,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_chosen(seed):\n",
    "    return sample_optimal_sa_pairs_like_scot(\n",
    "        envs, Q_list=envs_Q, chosen_from_scot=chosen,\n",
    "        tie_eps=1e-10, skip_terminals=True, seed=seed, return_shape=\"scot\"\n",
    "    )\n",
    "\n",
    "birl_kwargs = dict(beta=10, epsilon=1e-4, samples=3000, stepsize=0.1,\n",
    "                   normalize=True, adaptive=True, burn_frac=0.1, skip_rate=10)\n",
    "\n",
    "results = run_scot_vs_random_Q_regret(\n",
    "    envs=envs,\n",
    "    chosen_scot=chosen,\n",
    "    make_random_chosen=make_random_chosen,\n",
    "    n_random_trials=5,\n",
    "    birl_kwargs=birl_kwargs,\n",
    "    vi_epsilon=1e-6,\n",
    "    tie_eps=1e-10,\n",
    "    regret_epsilon=1e-4,\n",
    "    normalize_regret=False,\n",
    ")\n",
    "\n",
    "print(\"SCOT mean regret (MAP):\",  results[\"SCOT\"][\"summary_map\"][\"mean\"])\n",
    "print(\"Random mean regret (MAP, avg over seeds):\",\n",
    "      results[\"RANDOM_over_seeds\"][\"summary_seed_mean_map\"][\"mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6926b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
