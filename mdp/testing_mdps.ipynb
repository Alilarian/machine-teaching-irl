{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35edef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Construct the path to the directory you want to import from\n",
    "# '..' goes up one directory level. Adjust as needed.\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "# Add the directory to sys.path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from gridworld_env_layout import GridWorldMDPFromLayoutEnv\n",
    "from gridworld_env import NoisyLinearRewardFeaturizedGridWorldEnv\n",
    "import numpy as np\n",
    "\n",
    "from agent.q_learning_agent import ValueIteration, PolicyEvaluation\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac662f",
   "metadata": {},
   "source": [
    "## Testing transition for GridWorldMDPFromLayoutEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2569dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitions shape: (6, 4, 6)\n",
      "\n",
      "=== Action UP (0) ===\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "=== Action DOWN (1) ===\n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "=== Action LEFT (2) ===\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "=== Action RIGHT (3) ===\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "=== Per-state nonzero entries (s -> next:prob) ===\n",
      "\n",
      "State s=0:\n",
      "  UP    -> 0:1.000 | sum=1.000\n",
      "  DOWN  -> 3:1.000 | sum=1.000\n",
      "  LEFT  -> 0:1.000 | sum=1.000\n",
      "  RIGHT -> 1:1.000 | sum=1.000\n",
      "\n",
      "State s=1:\n",
      "  UP    -> 1:1.000 | sum=1.000\n",
      "  DOWN  -> 4:1.000 | sum=1.000\n",
      "  LEFT  -> 0:1.000 | sum=1.000\n",
      "  RIGHT -> 2:1.000 | sum=1.000\n",
      "\n",
      "State s=2:\n",
      "  UP    -> 2:1.000 | sum=1.000\n",
      "  DOWN  -> 5:1.000 | sum=1.000\n",
      "  LEFT  -> 1:1.000 | sum=1.000\n",
      "  RIGHT -> 2:1.000 | sum=1.000\n",
      "\n",
      "State s=3:\n",
      "  UP    -> 0:1.000 | sum=1.000\n",
      "  DOWN  -> 3:1.000 | sum=1.000\n",
      "  LEFT  -> 3:1.000 | sum=1.000\n",
      "  RIGHT -> 4:1.000 | sum=1.000\n",
      "\n",
      "State s=4:\n",
      "  UP    -> 1:1.000 | sum=1.000\n",
      "  DOWN  -> 4:1.000 | sum=1.000\n",
      "  LEFT  -> 3:1.000 | sum=1.000\n",
      "  RIGHT -> 5:1.000 | sum=1.000\n",
      "\n",
      "State s=5:\n",
      "  UP    -> 2:1.000 | sum=1.000\n",
      "  DOWN  -> 5:1.000 | sum=1.000\n",
      "  LEFT  -> 4:1.000 | sum=1.000\n",
      "  RIGHT -> 5:1.000 | sum=1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layout = [\n",
    "    [\"red\",   \"green\", \"red\"],\n",
    "    [\"red\",\"red\",\"red\"],\n",
    "]\n",
    "color_to_feature_map = {\n",
    "    \"red\":    [1,0,0],\n",
    "    \"green\":  [0,1,0],\n",
    "    \"blue\":   [0,0,1],\n",
    "    \"yellow\": [1,1,0],\n",
    "    \"purple\": [1,0,1],\n",
    "    \"orange\": [0,1,1],\n",
    "}\n",
    "\n",
    "env = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.9,\n",
    "    layout=layout,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0,         # slip prob\n",
    "    terminal_states=[]      # no terminals for this printout\n",
    ")\n",
    "\n",
    "print(\"transitions shape:\", env.transitions.shape)  # (6, 4, 6) for 2x3\n",
    "\n",
    "action_names = [\"UP\",\"DOWN\",\"LEFT\",\"RIGHT\"]\n",
    "\n",
    "# A) Print each action's full transition matrix (rounded)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "for a, name in enumerate(action_names):\n",
    "    print(f\"\\n=== Action {name} ({a}) ===\")\n",
    "    print(env.transitions[:, a, :].round(3))\n",
    "\n",
    "# B) Print only nonzeros for each (state, action) row\n",
    "print(\"\\n=== Per-state nonzero entries (s -> next:prob) ===\")\n",
    "for s in range(env.num_states):\n",
    "    print(f\"\\nState s={s}:\")\n",
    "    for a, name in enumerate(action_names):\n",
    "        row = env.transitions[s, a, :]\n",
    "        nz = np.nonzero(row)[0]\n",
    "        parts = [f\"{j}:{row[j]:.3f}\" for j in nz]\n",
    "        print(f\"  {name:<5} -> \" + \", \".join(parts) + f\" | sum={row.sum():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b51a96",
   "metadata": {},
   "source": [
    "## Testing transition for NoisyLinearRewardFeaturizedGridWorldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8038681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitions shape: (4, 4, 4)\n",
      "\n",
      "=== Action UP (0) ===\n",
      "[[0.9 0.1 0.  0. ]\n",
      " [0.1 0.9 0.  0. ]\n",
      " [0.8 0.  0.1 0.1]\n",
      " [0.  0.8 0.1 0.1]]\n",
      "\n",
      "=== Action DOWN (1) ===\n",
      "[[0.1 0.1 0.8 0. ]\n",
      " [0.1 0.1 0.  0.8]\n",
      " [0.  0.  0.9 0.1]\n",
      " [0.  0.  0.1 0.9]]\n",
      "\n",
      "=== Action LEFT (2) ===\n",
      "[[0.9 0.  0.1 0. ]\n",
      " [0.8 0.1 0.  0.1]\n",
      " [0.1 0.  0.9 0. ]\n",
      " [0.  0.1 0.8 0.1]]\n",
      "\n",
      "=== Action RIGHT (3) ===\n",
      "[[0.1 0.8 0.1 0. ]\n",
      " [0.  0.9 0.  0.1]\n",
      " [0.1 0.  0.1 0.8]\n",
      " [0.  0.1 0.  0.9]]\n",
      "\n",
      "=== Per-state nonzero entries (s -> next:prob) ===\n",
      "\n",
      "State s=0:\n",
      "  UP    -> 0:0.900, 1:0.100 | sum=1.000\n",
      "  DOWN  -> 0:0.100, 1:0.100, 2:0.800 | sum=1.000\n",
      "  LEFT  -> 0:0.900, 2:0.100 | sum=1.000\n",
      "  RIGHT -> 0:0.100, 1:0.800, 2:0.100 | sum=1.000\n",
      "\n",
      "State s=1:\n",
      "  UP    -> 0:0.100, 1:0.900 | sum=1.000\n",
      "  DOWN  -> 0:0.100, 1:0.100, 3:0.800 | sum=1.000\n",
      "  LEFT  -> 0:0.800, 1:0.100, 3:0.100 | sum=1.000\n",
      "  RIGHT -> 1:0.900, 3:0.100 | sum=1.000\n",
      "\n",
      "State s=2:\n",
      "  UP    -> 0:0.800, 2:0.100, 3:0.100 | sum=1.000\n",
      "  DOWN  -> 2:0.900, 3:0.100 | sum=1.000\n",
      "  LEFT  -> 0:0.100, 2:0.900 | sum=1.000\n",
      "  RIGHT -> 0:0.100, 2:0.100, 3:0.800 | sum=1.000\n",
      "\n",
      "State s=3:\n",
      "  UP    -> 1:0.800, 2:0.100, 3:0.100 | sum=1.000\n",
      "  DOWN  -> 2:0.100, 3:0.900 | sum=1.000\n",
      "  LEFT  -> 1:0.100, 2:0.800, 3:0.100 | sum=1.000\n",
      "  RIGHT -> 1:0.100, 3:0.900 | sum=1.000\n",
      "\n",
      "--- With terminal state at index 3 (self-loops) ---\n",
      "Terminal row, action UP: 3:1.000 | sum=1.000\n",
      "Terminal row, action DOWN: 3:1.000 | sum=1.000\n",
      "Terminal row, action LEFT: 3:1.000 | sum=1.000\n",
      "Terminal row, action RIGHT: 3:1.000 | sum=1.000\n"
     ]
    }
   ],
   "source": [
    "# Make a 2x2 env with no terminal states so we can inspect the raw transitions\n",
    "env = NoisyLinearRewardFeaturizedGridWorldEnv(\n",
    "    gamma=0.9,\n",
    "    size=2,                # 2x2 grid ⇒ 4 states\n",
    "    noise_prob=0.1,        # slip probability\n",
    "    num_features=4,\n",
    "    terminal_states=[],    # no terminals for this printout\n",
    "    include_terminal=False # don't force self-loops\n",
    ")\n",
    "\n",
    "print(\"transitions shape:\", env.transitions.shape)  # expect (4, 4, 4)\n",
    "\n",
    "action_names = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# A) Full transition matrix per action\n",
    "for a, name in enumerate(action_names):\n",
    "    print(f\"\\n=== Action {name} ({a}) ===\")\n",
    "    print(env.transitions[:, a, :].round(3))\n",
    "\n",
    "# B) Per-(state, action) row: show only nonzeros + row sum\n",
    "print(\"\\n=== Per-state nonzero entries (s -> next:prob) ===\")\n",
    "for s in range(env.num_states):\n",
    "    print(f\"\\nState s={s}:\")\n",
    "    for a, name in enumerate(action_names):\n",
    "        row = env.transitions[s, a, :]\n",
    "        nz = np.nonzero(row)[0]\n",
    "        parts = [f\"{j}:{row[j]:.3f}\" for j in nz]\n",
    "        print(f\"  {name:<5} -> \" + \", \".join(parts) + f\" | sum={row.sum():.3f}\")\n",
    "\n",
    "# Optional: show the terminal-behavior variant (last state is terminal)\n",
    "env_term = NoisyLinearRewardFeaturizedGridWorldEnv(\n",
    "    gamma=0.9,\n",
    "    size=2,\n",
    "    noise_prob=0.1,\n",
    "    num_features=4,\n",
    "    terminal_states=[3],   # bottom-right as terminal\n",
    "    include_terminal=True\n",
    ")\n",
    "print(\"\\n--- With terminal state at index 3 (self-loops) ---\")\n",
    "for a, name in enumerate(action_names):\n",
    "    row = env_term.transitions[3, a, :]\n",
    "    nz = np.nonzero(row)[0]\n",
    "    parts = [f\"{j}:{row[j]:.3f}\" for j in nz]\n",
    "    print(f\"Terminal row, action {name}: \" + \", \".join(parts) + f\" | sum={row.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcfa0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows x cols: 2 x 3\n",
      "num features: 2\n",
      "feature_weights (normalized by class): [-0.89442719 -0.4472136 ]\n",
      "R(0) [red] = -0.4472135954997579\n",
      "R(1) [blue]= -0.8944271909995158\n",
      "R(5) [goal] = -0.4472135954997579\n",
      "Initial obs: {'agent': array([0, 0], dtype=int32), 'terminal states': [2]}\n",
      "After RIGHT -> reward: -0.8944271909995158 done: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---- 2x3 layout using only names your renderer supports ('red','blue',...) ----\n",
    "layout = [\n",
    "    [\"blue\",  \"red\", \"blue\"],\n",
    "    [\"blue\", \"blue\",  \"blue\"],\n",
    "]\n",
    "\n",
    "# ---- 2 features per color: [feat0, feat1] ----\n",
    "#     Here: 'red' activates feature0, 'blue' activates feature1\n",
    "color_to_feature_map = {\n",
    "    \"red\":  [1.0, 0.0],\n",
    "    \"blue\": [0.0, 1.0],\n",
    "}\n",
    "\n",
    "# ---- Custom feature weights (your class normalizes these internally) ----\n",
    "custom_feature_weights = [-2.0, -1.0]   # favor 'red' cells, penalize 'blue' cells\n",
    "\n",
    "# ---- Optional terminal (goal) at bottom-right: index = 1*3 + 2 = 5 ----\n",
    "goal_idx = 2\n",
    "terminal_states = [goal_idx]           # or [] if you don't want terminals\n",
    "\n",
    "env = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.95,\n",
    "    layout=layout,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0.1,\n",
    "    terminal_states=terminal_states,\n",
    "    custom_feature_weights=custom_feature_weights,   # <--- custom weights in use\n",
    ")\n",
    "\n",
    "# Sanity checks\n",
    "print(\"rows x cols:\", env.rows, \"x\", env.columns)      # 2 x 3\n",
    "print(\"num features:\", env.num_features)                    # 2\n",
    "print(\"feature_weights (normalized by class):\", env.feature_weights)\n",
    "\n",
    "# Reward of a few states (flat indices)\n",
    "print(\"R(0) [red] =\", env.compute_reward(0))           # dot([1,0], w) -> ~ + (normalized)\n",
    "print(\"R(1) [blue]=\", env.compute_reward(1))           # dot([0,1], w) -> ~ - (normalized)\n",
    "print(\"R(5) [goal] =\", env.compute_reward(5))\n",
    "\n",
    "# Start and take a couple of steps\n",
    "obs = env.reset(fixed_start=True)   # starts at (0,0)\n",
    "print(\"Initial obs:\", obs)\n",
    "obs, r, done, _ = env.step(3)       # try RIGHT\n",
    "print(\"After RIGHT -> reward:\", r, \"done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8580d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abda1c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature weights actually used (class normalizes custom weights) ===\n",
      "[-0.89442719 -0.4472136 ]\n",
      "\n",
      "=== State values V(s) as a 2x3 grid ===\n",
      "[[-1.6144 -0.6619  0.    ]\n",
      " [-1.6104 -1.1274 -0.6125]]\n",
      "\n",
      "=== Greedy policy (arrows), 'T' marks terminal ===\n",
      "[['>' '>' 'T']\n",
      " ['>' '>' '^']]\n",
      "\n",
      "=== Q-values per state (rows: states 0..5, cols: actions [UP,DOWN,LEFT,RIGHT]) ===\n",
      "[[-1.9351 -1.9321 -1.9805 -1.6144]\n",
      " [-1.4614 -1.4574 -1.8889 -0.6619]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [-1.9343 -1.9312 -1.9775 -1.6104]\n",
      " [-1.5192 -1.5152 -1.8858 -1.1274]\n",
      " [-0.6125 -1.078  -1.3622 -0.9709]]\n",
      "\n",
      "R(0) (red): -0.4472135954997579\n",
      "R(1) (goal, blue): -0.8944271909995158\n",
      "Transitions shape: (6, 4, 6)\n",
      "Row sums for (state=0, action=RIGHT): 1.0\n"
     ]
    }
   ],
   "source": [
    "rows, cols = 2, 3\n",
    "\n",
    "# ---------- 2) Run Value Iteration ----------\n",
    "vi = ValueIteration(env)\n",
    "V = vi.run_value_iteration(epsilon=1e-10)\n",
    "policy = vi.get_optimal_policy()\n",
    "Q = vi.get_q_values(V)\n",
    "\n",
    "# ---------- 3) Pretty print results ----------\n",
    "print(\"\\n=== Feature weights actually used (class normalizes custom weights) ===\")\n",
    "print(env.feature_weights)\n",
    "\n",
    "print(\"\\n=== State values V(s) as a 2x3 grid ===\")\n",
    "print(np.round(V.reshape(rows, cols), 4))\n",
    "\n",
    "arrow = {0:\"^\", 1:\"v\", 2:\"<\", 3:\">\"}\n",
    "print(\"\\n=== Greedy policy (arrows), 'T' marks terminal ===\")\n",
    "policy_grid = np.full((rows, cols), \"\", dtype=object)\n",
    "for s, a in policy:\n",
    "    r, c = divmod(s, cols)\n",
    "    policy_grid[r, c] = \"T\" if s in env.terminal_states else arrow[a]\n",
    "print(policy_grid)\n",
    "\n",
    "print(\"\\n=== Q-values per state (rows: states 0..5, cols: actions [UP,DOWN,LEFT,RIGHT]) ===\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(Q)\n",
    "\n",
    "# (Optional) quick sanity checks\n",
    "print(\"\\nR(0) (red):\", env.compute_reward(0))\n",
    "print(\"R(1) (goal, blue):\", env.compute_reward(1))\n",
    "print(\"Transitions shape:\", env.transitions.shape)\n",
    "print(\"Row sums for (state=0, action=RIGHT):\", env.transitions[0,3,:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932742ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76664940",
   "metadata": {},
   "source": [
    "## Sanity check for value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a650f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellman residual (∞-norm): 0.9999500037486876\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Bellman residual too large; VI may not have converged.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m bellman_inf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(residual)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBellman residual (∞-norm):\u001b[39m\u001b[38;5;124m\"\u001b[39m, bellman_inf)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bellman_inf \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-7\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBellman residual too large; VI may not have converged.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# ---------- 4) V = max_a Q(s,a) ----------\u001b[39;00m\n\u001b[1;32m     60\u001b[0m Q \u001b[38;5;241m=\u001b[39m vi\u001b[38;5;241m.\u001b[39mget_q_values(V)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Bellman residual too large; VI may not have converged."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Build your 2x3 env (no rendering) ---\n",
    "layout = [\n",
    "    [\"red\",  \"blue\", \"red\"],\n",
    "    [\"blue\", \"red\",  \"blue\"],\n",
    "]\n",
    "color_to_feature_map = {\n",
    "    \"red\":  [1.0, 0.0],\n",
    "    \"blue\": [0.0, 1.0],\n",
    "}\n",
    "goal_idx = 1\n",
    "env = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.95,\n",
    "    layout=layout,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0.1,\n",
    "    terminal_states=[goal_idx],\n",
    "    custom_feature_weights=[-0.01, 1.0],\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "# Convenience\n",
    "S = env.get_num_states()\n",
    "A = env.get_num_actions()\n",
    "W = env.size               # width (columns); used by divmod(s, W)\n",
    "gamma = env.get_discount_factor()\n",
    "\n",
    "# ---------- 1) Transition matrix invariants ----------\n",
    "row_sums = env.transitions.sum(axis=2)\n",
    "assert np.allclose(row_sums, 1.0, atol=1e-12), \"Some (s,a) rows do not sum to 1.\"\n",
    "assert np.all(env.transitions >= -1e-12), \"Negative probabilities found.\"\n",
    "for t in env.terminal_states:\n",
    "    for a in range(A):\n",
    "        row = env.transitions[t, a, :]\n",
    "        assert np.count_nonzero(row) == 1 and np.isclose(row[t], 1.0), \"Terminal not strict self-loop.\"\n",
    "\n",
    "# ---------- 2) Reward mapping check ----------\n",
    "def manual_reward(s):\n",
    "    r, c = divmod(s, W)\n",
    "    feats = env.grid_features[r, c]          # numeric feature vector (new implementation)\n",
    "    return float(np.dot(feats, env.feature_weights))\n",
    "\n",
    "for s in range(S):\n",
    "    assert np.isclose(env.compute_reward(s), manual_reward(s)), f\"Reward mismatch at state {s}\"\n",
    "\n",
    "# ---------- 3) Run VI and check Bellman residual ----------\n",
    "vi = ValueIteration(env)\n",
    "V = vi.run_value_iteration(epsilon=1e-10)\n",
    "residual = []\n",
    "for s in range(S):\n",
    "    backs = [np.dot(env.transitions[s, a, :], V) for a in range(A)]\n",
    "    T_V_s = env.compute_reward(s) + gamma * np.max(backs)\n",
    "    residual.append(abs(T_V_s - V[s]))\n",
    "bellman_inf = np.max(residual)\n",
    "print(\"Bellman residual (∞-norm):\", bellman_inf)\n",
    "assert bellman_inf < 1e-7, \"Bellman residual too large; VI may not have converged.\"\n",
    "\n",
    "# ---------- 4) V = max_a Q(s,a) ----------\n",
    "Q = vi.get_q_values(V)\n",
    "assert np.allclose(V, Q.max(axis=1), atol=1e-8), \"V != max_a Q(s,a)\"\n",
    "\n",
    "# ---------- 5) Greedy policy evaluation ≈ V ----------\n",
    "policy = vi.get_optimal_policy()\n",
    "pe = PolicyEvaluation(env, policy=policy, uniform_random=False)\n",
    "V_pi = pe.run_policy_evaluation(epsilon=1e-10)\n",
    "diff = np.max(np.abs(V_pi - V))\n",
    "print(\"||V_pi - V||_∞:\", diff)\n",
    "assert diff < 1e-7, \"Greedy policy evaluation doesn't match V*.\"\n",
    "\n",
    "# ---------- 6) Analytic toy: 1x3 line, noise=0, terminal with unit reward ----------\n",
    "layout_line = [[\"red\", \"red\", \"blue\"]]   # blue is goal with reward 1, red = 0\n",
    "cmap_line = {\"red\":[0.0, 0.0], \"blue\":[0.0, 1.0]}\n",
    "gamma_line = 0.9\n",
    "goal_line = 2\n",
    "env_line = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=gamma_line,\n",
    "    layout=layout_line,\n",
    "    color_to_feature_map=cmap_line,\n",
    "    noise_prob=0.0,\n",
    "    terminal_states=[goal_line],\n",
    "    custom_feature_weights=[0.0, 1.0],   # only goal rewards\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "vi_line = ValueIteration(env_line)\n",
    "V_line = vi_line.run_value_iteration(epsilon=1e-12)\n",
    "\n",
    "V_goal = 1.0 / (1.0 - gamma_line)               # self-loop with reward 1\n",
    "assert np.isclose(V_line[goal_line], V_goal, atol=1e-8), \"Goal value incorrect in 1x3.\"\n",
    "# one and two steps away (optimal path RIGHT)\n",
    "assert np.isclose(V_line[1], gamma_line * V_goal, atol=1e-8)\n",
    "assert np.isclose(V_line[0], (gamma_line**2) * V_goal, atol=1e-8)\n",
    "\n",
    "# ---------- 7) Edge cases ----------\n",
    "# gamma=0 => V(s) = R(s)\n",
    "# orig_gamma = env.gamma\n",
    "# env.gamma = 0.0\n",
    "# V0 = ValueIteration(env).run_value_iteration(epsilon=1e-12)\n",
    "# R = np.array([env.compute_reward(s) for s in range(env.get_num_states())])\n",
    "# assert np.allclose(V0, R, atol=1e-10), \"With gamma=0, V should equal rewards.\"\n",
    "# env.gamma = orig_gamma  # restore\n",
    "\n",
    "print(\"All sanity checks passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44352a62",
   "metadata": {},
   "source": [
    "## Sanity check for value iteration wiht NoisyLinearRewardFeaturizedGridWorldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690d2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State values (reshape 3x3):\n",
      "[[0.81662704 0.864383   0.91423106]\n",
      " [0.864383   0.92053695 0.98060885]\n",
      " [0.91423106 0.98060885 0.        ]]\n",
      "\n",
      "Greedy policy:\n",
      "v > v\n",
      "v v v\n",
      "> > G\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pi_grid[r]))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# A couple of quick sanity checks\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m policy[goal_state][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# terminal self-loop exists (action value defined)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Values should generally increase toward the goal (not a strict proof because of noise, but indicative)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuick check: Value at start (0,0) =\u001b[39m\u001b[38;5;124m\"\u001b[39m, V[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Value at goal (2,2) =\u001b[39m\u001b[38;5;124m\"\u001b[39m, V[goal_state])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- paste in your ValueIteration class here (with the γ=0 guard fix if you added it) ---\n",
    "\n",
    "# --- import your env class ---\n",
    "# from your_module import NoisyLinearRewardFeaturizedGridWorldEnv\n",
    "\n",
    "# Build a 3x3 goal-reaching env\n",
    "size = 3\n",
    "goal_state = size*size - 1  # bottom-right index 8\n",
    "\n",
    "env = NoisyLinearRewardFeaturizedGridWorldEnv(\n",
    "    gamma=0.95,\n",
    "    size=size,\n",
    "    num_features=2,          # exactly two features\n",
    "    noise_prob=0.10,         # slip noise\n",
    "    terminal_states=[goal_state],\n",
    "    include_terminal=True,\n",
    "    goal_reaching=True,      # not strictly needed once we set custom weights\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "# Make features deterministic: all non-terminal = [1,0], goal = [0,1]\n",
    "grid = np.zeros((size, size, 2), dtype=float)\n",
    "grid[..., 0] = 1.0                      # everyone gets feature 0\n",
    "gr, gc = divmod(goal_state, size)\n",
    "grid[gr, gc] = np.array([0.0, 1.0])     # goal gets feature 1\n",
    "env.grid_features = grid\n",
    "\n",
    "# Set custom feature weights to strongly favor the goal feature\n",
    "env.set_feature_weights(np.array([0.0, 1.0]))  # will be normalized to [0,1]\n",
    "\n",
    "# ----- run value iteration -----\n",
    "VI = ValueIteration(env)\n",
    "V = VI.run_value_iteration(epsilon=1e-10)\n",
    "policy = VI.get_optimal_policy()\n",
    "\n",
    "# Pretty-print state values as 3x3\n",
    "print(\"State values (reshape 3x3):\")\n",
    "print(V.reshape(size, size))\n",
    "\n",
    "# Pretty-print greedy policy as arrows\n",
    "arrow = {0:\"^\", 1:\"v\", 2:\"<\", 3:\">\", None:\"·\"}\n",
    "pi_grid = np.array([arrow[a] for (_,a) in policy]).reshape(size, size)\n",
    "# Mark terminal explicitly\n",
    "pi_grid[gr, gc] = \"G\"\n",
    "print(\"\\nGreedy policy:\")\n",
    "for r in range(size):\n",
    "    print(\" \".join(pi_grid[r]))\n",
    "\n",
    "# A couple of quick sanity checks\n",
    "assert policy[goal_state][1] is not None  # terminal self-loop exists (action value defined)\n",
    "# Values should generally increase toward the goal (not a strict proof because of noise, but indicative)\n",
    "print(\"\\nQuick check: Value at start (0,0) =\", V[0], \"  Value at goal (2,2) =\", V[goal_state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7461629",
   "metadata": {},
   "source": [
    "## Initialize the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d8d4ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, None), (1, 2), (2, 2), (3, 0), (4, 0), (5, 0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build your 2x3 env (no rendering) ---\n",
    "layout = [\n",
    "    [\"blue\",  \"red\", \"blue\"],\n",
    "    [\"blue\", \"blue\",  \"blue\"],\n",
    "]\n",
    "color_to_feature_map = {\n",
    "    \"red\":  [1.0, 0.0],\n",
    "    \"blue\": [0.0, 1.0],\n",
    "}\n",
    "rows, cols = 2, 3\n",
    "#goal_idx = 2\n",
    "W_TRUE = np.array([-1, -1])/np.linalg.norm([-1, -1])\n",
    "env = GridWorldMDPFromLayoutEnv(\n",
    "    gamma=0.99,\n",
    "    layout=layout,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    noise_prob=0,\n",
    "    terminal_states=[0],\n",
    "    custom_feature_weights=W_TRUE,\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "vi = ValueIteration(env)\n",
    "V = vi.run_value_iteration(epsilon=1e-10)  # already uses γ and a norm-based threshold\n",
    "\n",
    "policy = vi.get_optimal_policy()\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a918e",
   "metadata": {},
   "source": [
    "## Differenr way to compute succesor features using q value which counts for multiple optimal actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7df7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_Pi_from_q(env, q_values, tie_eps=1e-10):\n",
    "    \"\"\"\n",
    "    Return Π(s,a) from Q(s,a): uniform over all a achieving max Q(s,·).\n",
    "    Terminal rows are left zero (P_pi fallback makes them self-loops).\n",
    "    \"\"\"\n",
    "    S, A = env.get_num_states(), env.get_num_actions()\n",
    "    Pi = np.zeros((S, A), dtype=float)\n",
    "\n",
    "    terminals = set(getattr(env, \"terminal_states\", []) or [])\n",
    "    for s in range(S):\n",
    "        if s in terminals:\n",
    "            continue\n",
    "        row = np.asarray(q_values[s], dtype=float)\n",
    "        m = np.max(row)\n",
    "        mask = np.abs(row - m) < tie_eps\n",
    "        k = int(mask.sum())\n",
    "        if k > 0:\n",
    "            Pi[s, mask] = 1.0 / k\n",
    "        else:\n",
    "            # extremely rare: all -inf or nan -> fallback to uniform\n",
    "            Pi[s, :] = 1.0 / A\n",
    "    return Pi\n",
    "\n",
    "def compute_successor_features_iterative_from_q(\n",
    "    env,\n",
    "    q_values,\n",
    "    convention: str = \"entering\"\n",
    "    \"\",\n",
    "    zero_terminal_features: bool = True,\n",
    "    tol: float = 1e-10,\n",
    "    max_iters: int = 10000\n",
    "):\n",
    "    S, A, d = env.get_num_states(), env.get_num_actions(), env.num_features\n",
    "    gamma = env.get_discount_factor()\n",
    "\n",
    "    # Φ and T (your env already normalizes; if you prefer, use it directly)\n",
    "    Phi = np.asarray(env.grid_features, float).reshape(S, d)\n",
    "    if zero_terminal_features and getattr(env, \"include_terminal\", False):\n",
    "        for t in (env.terminal_states or []):\n",
    "            Phi[t] = 0.0\n",
    "    T = np.asarray(env.transitions, float)\n",
    "\n",
    "    # π from Q with ties handled\n",
    "    Pi = build_Pi_from_q(env, q_values, tie_eps=1e-10)\n",
    "    print(\"Pi from q: \", Pi)\n",
    "\n",
    "    # P_π(s'|s) = sum_a Π(s,a) T(s'|s,a), with fallback self-loop if a row is zero\n",
    "    P_pi = np.zeros((S, S), dtype=float)\n",
    "    for s in range(S):\n",
    "        P_pi[s] = Pi[s].dot(T[s])\n",
    "        rs = P_pi[s].sum()\n",
    "        if rs == 0.0:\n",
    "            P_pi[s, s] = 1.0\n",
    "        else:\n",
    "            P_pi[s] /= rs\n",
    "\n",
    "    # Iterative policy evaluation for μ(s)\n",
    "    mu_s = np.zeros((S, d), dtype=float)\n",
    "    use_enter = convention.lower().startswith(\"enter\")\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        mu_old = mu_s.copy()\n",
    "        for s in range(S):\n",
    "            exp_mu_next = np.zeros(d)\n",
    "            exp_phi_next = np.zeros(d) if use_enter else None\n",
    "            for a in range(A):\n",
    "                p_next = T[s, a]\n",
    "                w = Pi[s, a]\n",
    "                if w == 0.0:\n",
    "                    continue\n",
    "                exp_mu_next += w * (p_next @ mu_old)\n",
    "                if use_enter:\n",
    "                    exp_phi_next += w * (p_next @ Phi)\n",
    "            mu_s[s] = (exp_phi_next if use_enter else Phi[s]) + gamma * exp_mu_next\n",
    "        if np.max(np.abs(mu_s - mu_old)) < tol:\n",
    "            break\n",
    "\n",
    "    # ψ(s,a)\n",
    "    mu_sa = np.zeros((S, A, d), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            p_next = T[s, a]\n",
    "            exp_mu_next = p_next @ mu_s\n",
    "            if use_enter:\n",
    "                exp_phi_next = p_next @ Phi\n",
    "                mu_sa[s, a] = exp_phi_next + gamma * exp_mu_next\n",
    "            else:\n",
    "                mu_sa[s, a] = Phi[s] + gamma * exp_mu_next\n",
    "\n",
    "    return mu_sa, mu_s, Phi, P_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c6cd1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi from q:  [[0.  0.  0.  0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0. ]\n",
      " [0.5 0.  0.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "# mu_sa, mu_s, Phi, P_pi = compute_successor_features_iterative_from_q(\n",
    "#     env,\n",
    "#     vi.get_q_values(),\n",
    "# )\n",
    "\n",
    "mu_sa, mu_s, Phi, P_pi = compute_successor_features_iterative_from_q(\n",
    "    env,\n",
    "    vi.get_q_values(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c621d5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[-1.40714249 -2.10017785 -0.70710678 -2.10017785]\n",
      "[-2.10017785 -2.78628285 -1.40714249 -2.10017785]\n",
      "[-0.70710678 -1.40714249 -1.40714249 -2.10017785]\n",
      "[-1.40714249 -2.10017785 -1.40714249 -2.78628285]\n",
      "[-2.10017785 -2.78628285 -2.10017785 -2.78628285]\n"
     ]
    }
   ],
   "source": [
    "for i in vi.get_q_values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d9a1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def derive_constraints_from_mu_sa_vec(\n",
    "#     mu_sa: np.ndarray,       # (S, A, d)  action-SFs ψ(s,a)\n",
    "#     pi_pairs,                # list[(state, action)]\n",
    "#     env,\n",
    "#     skip_terminals: bool = True,\n",
    "#     normalize: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Vectorized: build constraints v = ψ(s,a*) - ψ(s,b) for all b != a*.\n",
    "#     Returns a list[(v, s, a_star, b)].\n",
    "#     \"\"\"\n",
    "#     S, A, d = mu_sa.shape\n",
    "\n",
    "#     # map (state, action) pairs -> a*(s); unspecified states set to -1\n",
    "#     a_star = np.full(S, -1, dtype=int)\n",
    "#     for s, a in pi_pairs:\n",
    "#         a_star[int(s)] = int(a)\n",
    "\n",
    "#     # skip terminals if requested\n",
    "#     if skip_terminals and getattr(env, \"include_terminal\", False):\n",
    "#         terms = np.array(env.terminal_states or [], dtype=int)\n",
    "#         if terms.size:\n",
    "#             a_star[terms] = -1\n",
    "\n",
    "#     valid = np.where(a_star >= 0)[0]\n",
    "#     if valid.size == 0:\n",
    "#         return []\n",
    "\n",
    "#     # ψ(s,a*) for valid states (Nv,d)\n",
    "#     psi_star = mu_sa[valid, a_star[valid], :]              # (Nv, d)\n",
    "#     # ψ(s,·) for valid states (Nv,A,d)\n",
    "#     psi_all  = mu_sa[valid, :, :]                          # (Nv, A, d)\n",
    "#     # differences for all b: (Nv, A, d)\n",
    "#     diffs = psi_star[:, None, :] - psi_all\n",
    "\n",
    "#     # mask out b == a*\n",
    "#     mask = np.ones((valid.size, A), dtype=bool)\n",
    "#     mask[np.arange(valid.size), a_star[valid]] = False\n",
    "\n",
    "#     # flatten selections\n",
    "#     v = diffs[mask]                                        # (Ncons, d)\n",
    "#     s_idx = np.repeat(valid, A)[mask.ravel()]              # (Ncons,)\n",
    "#     b_idx = np.tile(np.arange(A), valid.size)[mask.ravel()]\n",
    "#     a_idx = np.repeat(a_star[valid], A)[mask.ravel()]\n",
    "\n",
    "#     # drop near-zero vectors\n",
    "#     nrm = np.linalg.norm(v, axis=1)\n",
    "#     #keep = nrm > tol\n",
    "#     #if not np.any(keep):\n",
    "#     #    return []\n",
    "#     #v, s_idx, a_idx, b_idx, nrm = v[keep], s_idx[keep], a_idx[keep], b_idx[keep], nrm[keep]\n",
    "#     v, s_idx, a_idx, b_idx, nrm = v, s_idx, a_idx, b_idx, nrm\n",
    "\n",
    "#     # normalize if requested\n",
    "#     if normalize:\n",
    "#         v = np.divide(v, nrm[:, None], out=v.copy(), where=nrm[:, None] > 0)\n",
    "\n",
    "#     # pack result\n",
    "#     return [(v[i], int(s_idx[i]), int(a_idx[i]), int(b_idx[i])) for i in range(v.shape[0])]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def derive_constraints_from_q_ties(\n",
    "    mu_sa: np.ndarray,        # (S, A, d) action-SFs ψ(s,a)\n",
    "    q_values: np.ndarray,     # (S, A)     action values used to decide optimal set(s)\n",
    "    env,\n",
    "    tie_eps: float = 1e-10,\n",
    "    skip_terminals: bool = True,\n",
    "    normalize: bool = True,\n",
    "    tol: float = 1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build constraints v = ψ(s,a*) - ψ(s,b) for every state s, for every co-optimal a* in argmax Q(s,·),\n",
    "    and for every non-optimal b ∉ argmax Q(s,·). This preserves all ties.\n",
    "\n",
    "    Returns: list of (v, s, a_star, b) with v in R^d.\n",
    "    \"\"\"\n",
    "    S, A, d = mu_sa.shape\n",
    "    q = np.asarray(q_values, dtype=float)\n",
    "    if q.shape != (S, A):\n",
    "        raise ValueError(f\"q_values shape {q.shape} != (S, A)=({S},{A})\")\n",
    "\n",
    "    # argmax set per state with tie tolerance\n",
    "    m = np.max(q, axis=1, keepdims=True)\n",
    "    argmax_mask = np.abs(q - m) <= tie_eps   # (S, A) True where action is co-optimal\n",
    "\n",
    "    # optionally skip terminals\n",
    "    if skip_terminals and getattr(env, \"include_terminal\", False):\n",
    "        terms = np.array(getattr(env, \"terminal_states\", []) or [], dtype=int)\n",
    "        if terms.size:\n",
    "            argmax_mask[terms] = False  # no constraints emitted from terminals\n",
    "\n",
    "    constraints = []\n",
    "    for s in range(S):\n",
    "        A_star = np.where(argmax_mask[s])[0]\n",
    "        if A_star.size == 0:\n",
    "            continue  # nothing to do (terminal or undefined)\n",
    "        B = np.where(~argmax_mask[s])[0]\n",
    "        if B.size == 0:\n",
    "            # all actions are tied optimal at s -> no informative inequality constraints\n",
    "            continue\n",
    "\n",
    "        psi_s = mu_sa[s]  # (A, d)\n",
    "        # for each co-optimal a*, build differences against all non-optimal b\n",
    "        for a_star in A_star:\n",
    "            diffs = psi_s[a_star][None, :] - psi_s[B, :]   # (|B|, d)\n",
    "            norms = np.linalg.norm(diffs, axis=1)\n",
    "\n",
    "            # optionally normalize and drop ~zero vectors\n",
    "            for i, b in enumerate(B):\n",
    "                if norms[i] <= tol:\n",
    "                    continue\n",
    "                v = diffs[i]\n",
    "                if normalize:\n",
    "                    v = v / norms[i]\n",
    "                constraints.append((v, int(s), int(a_star), int(b)))\n",
    "\n",
    "    return constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6884b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vecs = derive_constraints_from_q_ties(mu_sa, vi.get_q_values(), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82fbc002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.,  0.]),\n",
       " array([-0.31432215, -0.94931638]),\n",
       " array([-0.7035446 , -0.71065111]),\n",
       " array([ 0.0099995, -0.99995  ]),\n",
       " array([ 0.11770932, -0.99304809]),\n",
       " array([ 0.0099995, -0.99995  ]),\n",
       " array([ 0., -1.]),\n",
       " array([ 0., -1.]),\n",
       " array([-0.31432215, -0.94931638]),\n",
       " array([ 0.32002753, -0.94740824]),\n",
       " array([ 0.11770932, -0.99304809]),\n",
       " array([-0.70710678, -0.70710678]),\n",
       " array([-0.51145358, -0.8593109 ]),\n",
       " array([ 0.20215129, -0.97935431]),\n",
       " array([ 0.20215129, -0.97935431]),\n",
       " array([-0.30858249, -0.95119758]),\n",
       " array([-0.30858249, -0.95119758])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_vecs= [norm_vecs[i][0] for i in range(len(norm_vecs))] \n",
    "norm_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a642b",
   "metadata": {},
   "source": [
    "## removing redundant constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df0c3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def _normalize_dir(v, tol=1e-12):\n",
    "    \"\"\"Normalize v up to positive scaling so duplicates collapse to same key.\"\"\"\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    nrm = np.linalg.norm(v)\n",
    "    if nrm < tol:\n",
    "        # Zero vector is not a valid halfspace normal for c^T w >= 0; skip it upstream\n",
    "        return None\n",
    "    v = v / nrm\n",
    "    # Make a canonical sign: first nonzero component positive\n",
    "    for x in v:\n",
    "        if abs(x) > tol:\n",
    "            if x < 0:\n",
    "                v = -v\n",
    "            break\n",
    "    # Round for stable hashing\n",
    "    return tuple(np.round(v, 12))\n",
    "\n",
    "def is_redundant_constraint(h, H, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Return True if the inequality h^T w >= 0 is redundant given H w >= 0.\n",
    "    \"\"\"\n",
    "    h = np.asarray(h, dtype=float)\n",
    "    H = np.asarray(H, dtype=float)\n",
    "\n",
    "    # If there are no other constraints, h cannot be redundant.\n",
    "    if H.size == 0:\n",
    "        return False\n",
    "\n",
    "    # Ensure H is 2D with shape (m, n)\n",
    "    if H.ndim == 1:\n",
    "        H = H.reshape(1, -1)\n",
    "    m, n = H.shape\n",
    "    assert h.shape == (n,), f\"Shape mismatch: h {h.shape} vs H (m={m}, n={n})\"\n",
    "\n",
    "    # Solve: minimize h^T w  s.t.  (-H) w <= 0,  bounds -1 <= w_i <= 1\n",
    "    b = np.zeros(m)\n",
    "    res = linprog(h, A_ub=-H, b_ub=b, bounds=[(-1, 1)]*n, method='highs')\n",
    "\n",
    "    if res.status != 0:\n",
    "        # Numerical hiccup: be safe and treat as necessary (not redundant).\n",
    "        return False\n",
    "\n",
    "    # If we can push h^T w below 0, then h is necessary; otherwise redundant.\n",
    "    return res.fun >= -epsilon\n",
    "\n",
    "def remove_redundant_constraints(halfspaces, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Return a list of non-redundant halfspace normals h such that h^T w >= 0.\n",
    "    - Monotone build: test each h only against the set we've already kept.\n",
    "    - Final cleanup pass removes any that became redundant after later additions.\n",
    "    - Exact/positively-scaled duplicates are removed upfront.\n",
    "    \"\"\"\n",
    "    halfspaces = [np.asarray(h, dtype=float) for h in halfspaces]\n",
    "\n",
    "    # Filter out zeros and deduplicate by direction (up to positive scale)\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for h in halfspaces:\n",
    "        key = _normalize_dir(h)\n",
    "        if key is None:\n",
    "            continue  # skip zero normals\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(h)\n",
    "\n",
    "    kept = []\n",
    "    # First pass: only compare to what we've already kept (prevents \"all dropped\" bug)\n",
    "    for h in unique:\n",
    "        H_keep = np.vstack(kept) if len(kept) else np.empty((0, h.size))\n",
    "        if not is_redundant_constraint(h, H_keep, epsilon):\n",
    "            kept.append(h)\n",
    "\n",
    "    # Final cleanup: check each kept constraint against all others kept\n",
    "    final = []\n",
    "    for i, h in enumerate(kept):\n",
    "        others = [kept[j] for j in range(len(kept)) if j != i]\n",
    "        H_others = np.vstack(others) if len(others) else np.empty((0, h.size))\n",
    "        if not is_redundant_constraint(h, H_others, epsilon):\n",
    "            final.append(h)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "non_redundant_vecs = remove_redundant_constraints(np.array(norm_vecs), epsilon = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f1a2634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.,  0.]),\n",
       " array([-0.31432215, -0.94931638]),\n",
       " array([-0.7035446 , -0.71065111]),\n",
       " array([ 0.0099995, -0.99995  ]),\n",
       " array([ 0.11770932, -0.99304809]),\n",
       " array([ 0.0099995, -0.99995  ]),\n",
       " array([ 0., -1.]),\n",
       " array([ 0., -1.]),\n",
       " array([-0.31432215, -0.94931638]),\n",
       " array([ 0.32002753, -0.94740824]),\n",
       " array([ 0.11770932, -0.99304809]),\n",
       " array([-0.70710678, -0.70710678]),\n",
       " array([-0.51145358, -0.8593109 ]),\n",
       " array([ 0.20215129, -0.97935431]),\n",
       " array([ 0.20215129, -0.97935431]),\n",
       " array([-0.30858249, -0.95119758]),\n",
       " array([-0.30858249, -0.95119758])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b01e4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.,  0.]), array([ 0.32002753, -0.94740824])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_redundant_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be754c2",
   "metadata": {},
   "source": [
    "## visualize constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfb85268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.        ]\n",
      " [-0.         -1.        ]\n",
      " [-0.         -0.        ]\n",
      " [-1.         -0.33779264]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def _intersection_polygon_2d(V, box=1.0, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Return vertices (k,2) of the bounded intersection:\n",
    "        { w : w^T v_i >= 0 for all i }  ∩  { |w1|<=box, |w2|<=box }\n",
    "    as a convex polygon ordered counter-clockwise.\n",
    "    \"\"\"\n",
    "    V = np.asarray(V, float).reshape(-1, 2)\n",
    "\n",
    "    # Convert to a_i x + b_i y + c_i <= 0 form (halfspace format)\n",
    "    # w^T v >= 0  <=>  (-v)^T w <= 0  -> a=-vx, b=-vy, c=0\n",
    "    half = [(-vx, -vy, 0.0) for (vx, vy) in V]\n",
    "\n",
    "    # Bounding box (keeps region bounded for plotting)\n",
    "    half += [( 1, 0, -box), (-1, 0, -box), (0, 1, -box), (0, -1, -box)]\n",
    "\n",
    "    pts = []\n",
    "    m = len(half)\n",
    "    for i in range(m):\n",
    "        a1, b1, c1 = half[i]\n",
    "        for j in range(i + 1, m):\n",
    "            a2, b2, c2 = half[j]\n",
    "            D = a1 * b2 - a2 * b1\n",
    "            if abs(D) < tol:\n",
    "                continue  # parallel lines\n",
    "            x = (b1 * c2 - b2 * c1) / D\n",
    "            y = (c1 * a2 - c2 * a1) / D\n",
    "            # keep if satisfies all halfspaces\n",
    "            if all(a * x + b * y + c <= tol for (a, b, c) in half):\n",
    "                pts.append((x, y))\n",
    "\n",
    "    if not pts:\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "    pts = np.unique(np.round(pts, 12), axis=0)  # dedup numerically\n",
    "    c = pts.mean(axis=0)\n",
    "    ang = np.arctan2(pts[:, 1] - c[1], pts[:, 0] - c[0])\n",
    "    order = np.argsort(ang)\n",
    "    return pts[order]\n",
    "\n",
    "def plot_halfspace_intersection_2d(\n",
    "    V,\n",
    "    *,\n",
    "    box=1.0,\n",
    "    colors=None,\n",
    "    labels=None,\n",
    "    w_true=W_TRUE,\n",
    "    title=\"Intersection of half-spaces\"\n",
    "):\n",
    "    \"\"\"\n",
    "    V: (m,2) array of normals; each row v defines w^T v >= 0.\n",
    "    box: plot window [-box, box]^2 and bounding halfspaces.\n",
    "    colors/labels: optional per-constraint styling.\n",
    "    w_true: optional (w1, w2) to mark with a star.\n",
    "    \"\"\"\n",
    "    V = np.asarray(V, float).reshape(-1, 2)\n",
    "    m = len(V)\n",
    "    xs = np.linspace(-box, box, 400)\n",
    "\n",
    "    if colors is None:\n",
    "        colors = [\"#d81b60\", \"#008080\", \"#1f77b4\", \"#ff7f0e\"]  # magenta/teal first\n",
    "    if labels is None:\n",
    "        labels = [f\"Constraint {i+1}\" for i in range(m)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # Draw each boundary v_x x + v_y y = 0\n",
    "    handles = []\n",
    "    for i, (vx, vy) in enumerate(V):\n",
    "        col = colors[i % len(colors)]\n",
    "        if abs(vy) < 1e-12:\n",
    "            h = ax.axvline(0, color=col, lw=4, label=labels[i])\n",
    "        else:\n",
    "            y_line = -(vx / vy) * xs\n",
    "            \n",
    "            h, = ax.plot(xs, y_line, color=col, lw=1, label=labels[i])\n",
    "        handles.append(h)\n",
    "\n",
    "    # Compute and shade feasible polygon (hatched)\n",
    "    poly = _intersection_polygon_2d(V, box=box)\n",
    "    print(poly)\n",
    "    if poly.shape[0] > 0:\n",
    "        patch = Polygon(\n",
    "            poly, closed=True, facecolor=\"#f5bd23\", alpha=0.9,\n",
    "            edgecolor=\"none\", hatch=\"///\"\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    # Axes, limits, labels\n",
    "    ax.axhline(0, color=\"k\", lw=1)\n",
    "    ax.axvline(0, color=\"k\", lw=1)\n",
    "    ax.set_xlim(-box, box)\n",
    "    ax.set_ylim(-box, box)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.set_xlabel(\"w1\")\n",
    "    ax.set_ylabel(\"w2\")\n",
    "\n",
    "    # True reward (optional)\n",
    "    if w_true is not None:\n",
    "        ax.plot(w_true[0], w_true[1], marker=\"*\", color=\"k\", ms=10, label=\"True Reward\")\n",
    "        ax.plot(-0.96 , -0.26 , marker=\"*\", color=\"#23f523\", ms=10, label=\"MAP from SCOT\")\n",
    "        \n",
    "    # Build a clean legend (constraint lines + star if present)\n",
    "    if w_true is not None:\n",
    "        ax.legend(loc=\"upper right\")\n",
    "    else:\n",
    "        ax.legend(handles=handles, loc=\"upper right\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "# non_redundant_vecs = [[-0.06628492, -0.99780074],[ 0., -1.],\n",
    "# [-0.10710142,  0.9942481 ],\n",
    "# [-0.25864806, -0.96597163],]\n",
    "\n",
    "plot_halfspace_intersection_2d(non_redundant_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3260844",
   "metadata": {},
   "source": [
    "## generate trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e18b1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_candidates(env, pi_star, num_rollouts_per_state=1, max_steps=15):\n",
    "#     \"\"\"\n",
    "#     Generate trajectories as lists of (state, action) pairs by following a\n",
    "#     deterministic policy given as a list of (state, action) pairs.\n",
    "#     \"\"\"\n",
    "#     S = env.get_num_states()\n",
    "#     A = env.get_num_actions()\n",
    "\n",
    "#     # Build a deterministic action map; -1 means \"no action specified for this state\"\n",
    "#     a_star = np.full(S, -1, dtype=int)\n",
    "#     for s, a in pi_star:\n",
    "#         a_star[int(s)] = int(a)\n",
    "\n",
    "#     terminals = set(env.terminal_states or [])\n",
    "#     T = env.transitions  # already row-stochastic, terminals are absorbing\n",
    "\n",
    "#     trajectories = []\n",
    "#     for start_s in range(S):\n",
    "#         if start_s in terminals or a_star[start_s] < 0:\n",
    "#             continue  # skip terminal starts or states without a specified action\n",
    "\n",
    "#         for _ in range(num_rollouts_per_state):\n",
    "#             tau, s, steps = [], int(start_s), 0\n",
    "#             while steps < max_steps and s not in terminals:\n",
    "#                 a = a_star[s]\n",
    "#                 if a < 0:\n",
    "#                     break  # hit a state without a specified action; stop this rollout\n",
    "#                 tau.append((s, a))\n",
    "#                 # Sample next state from T[s, a, :]\n",
    "#                 s = int(np.random.choice(S, p=T[s, a]))\n",
    "#                 steps += 1\n",
    "#             trajectories.append(tau)\n",
    "\n",
    "#     return trajectories\n",
    "\n",
    "\n",
    "def generate_candidates_from_q(\n",
    "    env,\n",
    "    q_values,                      # shape (S, A)\n",
    "    num_rollouts_per_state=10,\n",
    "    max_steps=15,\n",
    "    tie_eps=1e-10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate trajectories as lists of (state, action) pairs by following a greedy\n",
    "    policy derived from q_values, sampling next states from env.transitions.\n",
    "    \"\"\"\n",
    "    S = env.get_num_states()\n",
    "    A = env.get_num_actions()\n",
    "    terminals = set(env.terminal_states or [])\n",
    "    T = env.transitions   # already row-stochastic per your env\n",
    "\n",
    "    # Precompute greedy action sets (allowing ties within tie_eps)\n",
    "    opt_actions = [[] for _ in range(S)]\n",
    "    for s in range(S):\n",
    "        if s in terminals:\n",
    "            continue\n",
    "        row = q_values[s]\n",
    "        max_q = np.max(row)\n",
    "        opt_actions[s] = [a for a in range(A) if abs(row[a] - max_q) < tie_eps]\n",
    "\n",
    "    \n",
    "    #print(opt_actions)\n",
    "    trajectories = []\n",
    "    for start_s in range(S):\n",
    "        if start_s in terminals or not opt_actions[start_s]:\n",
    "            continue  # skip terminal starts or states with no valid action\n",
    "\n",
    "        for _ in range(num_rollouts_per_state):\n",
    "            tau, s, steps = [], int(start_s), 0\n",
    "            print(\"starting state: \", s)\n",
    "            while steps < max_steps and s not in terminals:\n",
    "                acts = opt_actions[s]\n",
    "                if not acts:\n",
    "                    break\n",
    "                print(\"optimal actions: \", acts)\n",
    "                a = int(np.random.choice(acts))           # pick among optimal actions\n",
    "                tau.append((s, a))\n",
    "                s = int(np.random.choice(S, p=T[s, a]))   # sample next state from transitions\n",
    "                steps += 1\n",
    "            trajectories.append(tau)\n",
    "\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73cff700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  1\n",
      "optimal actions:  [2]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  2\n",
      "optimal actions:  [1]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  3\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  4\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n",
      "starting state:  5\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [2]\n",
      "optimal actions:  [0]\n"
     ]
    }
   ],
   "source": [
    "T_q = generate_candidates_from_q(env, vi.get_q_values(), max_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7083c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-1.09354587, -0.295536  , -0.09950372, -0.48766723],\n",
       "       [-0.48766723, -0.39208435, -1.09354587, -0.48766723],\n",
       "       [-0.09950372, -0.1980124 , -0.1980124 , -0.295536  ],\n",
       "       [-1.09354587, -0.295536  , -0.1980124 , -0.39208435],\n",
       "       [-0.48766723, -0.39208435, -0.295536  , -0.39208435]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi.get_q_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d27fa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(1, 2)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(2, 1), (5, 2), (4, 2), (3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)],\n",
       " [(5, 2), (4, 2), (3, 0)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8417ce",
   "metadata": {},
   "source": [
    "## SCOT algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74865977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scot_greedy(U, T, mu_sa, normalize=True, round_decimals=12):\n",
    "    \"\"\"\n",
    "    Greedy select a subset of trajectories D ⊆ T that covers as many constraint\n",
    "    directions from U as possible. Coverage is by matching direction vectors.\n",
    "\n",
    "    U: list[(v_hat, s, a_star, b)]  # constraint directions (v_hat usually unit-norm)\n",
    "    T: list[list[(s, a)]]           # trajectories as (state, action) pairs\n",
    "    mu_sa: np.ndarray, shape (S, A, d)\n",
    "    Returns\n",
    "    -------\n",
    "    D : list of selected trajectories (subset of T)\n",
    "    \"\"\"\n",
    "\n",
    "    S, A, d = mu_sa.shape\n",
    "\n",
    "    # --- helper to build a hashable key for a direction vector ---\n",
    "    def key_for(v):\n",
    "        n = np.linalg.norm(v)\n",
    "        if not np.isfinite(n) or n == 0.0:\n",
    "            return (\"ZERO\",)                      # special bucket for all-zero constraints\n",
    "        vv = v / n if normalize else v\n",
    "        return tuple(np.round(vv, round_decimals))\n",
    "\n",
    "    # --- index U by direction key (handle duplicates mapping to same key) ---\n",
    "    key_to_indices = {}\n",
    "    for idx, v_hat in enumerate(U):\n",
    "        k = key_for(np.asarray(v_hat, dtype=float))\n",
    "        key_to_indices.setdefault(k, []).append(idx)\n",
    "\n",
    "    universe = set(range(len(U)))   # all constraint indices\n",
    "    covered = set()                 # covered indices so far\n",
    "    D = []                          # selected trajectories\n",
    "    remaining_T = list(T)           # we may remove chosen ones to speed up\n",
    "    print(T)\n",
    "    # --- main greedy loop ---\n",
    "    while True:\n",
    "        uncovered = universe - covered\n",
    "        if not uncovered or not remaining_T:\n",
    "            break\n",
    "\n",
    "        best_gain = 0\n",
    "        best_idx = -1\n",
    "        best_covered_by_tau = None\n",
    "\n",
    "        # evaluate coverage gain for each candidate trajectory\n",
    "        for i, tau in enumerate(remaining_T):\n",
    "            covered_by_tau = set()\n",
    "            for s, a in tau:\n",
    "                # form differences ψ(s,a) - ψ(s,b) for all b != a\n",
    "                psi_star = mu_sa[s, a]                  # (d,)\n",
    "                for b in range(A):\n",
    "                    if b == a:\n",
    "                        continue\n",
    "                    v = psi_star - mu_sa[s, b]          # (d,)\n",
    "                    k = key_for(v)\n",
    "                    if k in key_to_indices:\n",
    "                        covered_by_tau.update(key_to_indices[k])\n",
    "\n",
    "            gain = len(uncovered & covered_by_tau)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_idx = i\n",
    "                best_covered_by_tau = covered_by_tau\n",
    "\n",
    "        if best_idx < 0 or best_gain == 0:\n",
    "            # no trajectory adds new coverage; stop\n",
    "            break\n",
    "\n",
    "        # select the best trajectory and update covered set\n",
    "        tau_greedy = remaining_T.pop(best_idx)\n",
    "        D.append(tau_greedy)\n",
    "        covered |= best_covered_by_tau\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c8bae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 2)], [(1, 2)], [(1, 2)], [(1, 2)], [(1, 2)], [(1, 2)], [(1, 2)], [(1, 2)], [(1, 2)], [(1, 2)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(2, 1), (5, 2), (4, 2), (3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)], [(5, 2), (4, 2), (3, 0)]]\n"
     ]
    }
   ],
   "source": [
    "D = scot_greedy(non_redundant_vecs, T_q, mu_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73c6dcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 1), (5, 2), (4, 2), (3, 0)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3afb1",
   "metadata": {},
   "source": [
    "## Train BIRL on data from non-terminal MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d08431e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_learning.birl import BIRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7e36428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 1), (5, 2), (4, 2), (3, 0)]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [list(dict.fromkeys(D[0]))]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58e82471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 1), (5, 2), (4, 2), (3, 0)]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfc526bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo = [(0,1), (3,3), (4,3), (5, 0)]\n",
    "\n",
    "birl = BIRL(env, result[0], beta=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb73c904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP INSIDE the MCMC:  [-0.37955562  0.92516892]\n",
      "MAP INSIDE the MCMC:  [-0.54579618  0.83791798]\n",
      "MAP INSIDE the MCMC:  [-0.56971876  0.82183973]\n",
      "MAP INSIDE the MCMC:  [-0.67740615  0.73560921]\n",
      "MAP INSIDE the MCMC:  [-0.84499982  0.53476659]\n",
      "MAP INSIDE the MCMC:  [-0.90826484  0.41839572]\n",
      "MAP INSIDE the MCMC:  [-0.92520622  0.37946468]\n",
      "MAP INSIDE the MCMC:  [-0.95073538  0.3100036 ]\n",
      "MAP INSIDE the MCMC:  [-0.9556596   0.29447363]\n",
      "MAP INSIDE the MCMC:  [-0.96470108  0.26334736]\n",
      "MAP INSIDE the MCMC:  [-0.97623203  0.21672801]\n",
      "MAP INSIDE the MCMC:  [-0.97202809 -0.23486465]\n",
      "MAP INSIDE the MCMC:  [-0.96986095 -0.24365906]\n",
      "MAP INSIDE the MCMC:  [-0.96950728 -0.2450625 ]\n",
      "MAP INSIDE the MCMC:  [-0.9667046  -0.25589492]\n",
      "MAP INSIDE the MCMC:  [-0.96505051 -0.26206396]\n",
      "MAP INSIDE the MCMC:  [-0.96371419 -0.26693626]\n",
      "MAP INSIDE the MCMC:  [-0.96360745 -0.26732132]\n",
      "MAP INSIDE the MCMC:  [-0.96364358 -0.26719104]\n"
     ]
    }
   ],
   "source": [
    "birl.run_mcmc(samples=3000, stepsize=0.15, adaptive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ef3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "birl.map_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c6ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c1d4d73",
   "metadata": {},
   "source": [
    "## Compute the policy loss for that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3e0c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a40cc2",
   "metadata": {},
   "source": [
    "## Using learned reward function on the MDP with terminal state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
