{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccba47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#from utils import remove_redundant_constraints\n",
    "\n",
    "#from __future__ import annotations\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.mission import MissionSpace\n",
    "\n",
    "# test_lavaworld_manual.py\n",
    "from utils import (generate_lavaworld, \n",
    "                    generate_demos_from_policies_multi, \n",
    "                    constraints_from_demos_next_state_multi,\n",
    "                    generate_trajectory_pools_multi,\n",
    "                    generate_feedback_multi,\n",
    "                    value_iteration_next_state_multi,\n",
    "                    policy_evaluation_next_state_multi,\n",
    "                    compute_successor_features_multi,\n",
    "                    value_iteration_next_state,\n",
    "                    policy_evaluation_next_state,\n",
    "                    compute_successor_features_from_q_next_state,\n",
    "                    l2_normalize,\n",
    "                    enumerate_states,\n",
    "                    constraints_from_demos_next_state,\n",
    "                    constraints_from_demos_next_state_multi,\n",
    "                    generate_state_action_demos,\n",
    "                    trajectory_return)\n",
    "\n",
    "from minigrid.manual_control import ManualControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53a000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 envs\n"
     ]
    }
   ],
   "source": [
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "envs, mdps, meta = generate_lavaworld(\n",
    "    n_envs=1,\n",
    "    size=8,\n",
    "    seed=420,\n",
    ")\n",
    "\n",
    "print(\"Generated\", len(envs), \"envs\")\n",
    "\n",
    "#env = envs[9]\n",
    "#env.reset()\n",
    "\n",
    "# print(\"Goal at:\", meta[\"goals\"][0])\n",
    "# print(\"Lava cells:\")\n",
    "# print(meta[\"lava_masks\"][0].astype(int))\n",
    "\n",
    "# manual = ManualControl(env, seed=0)\n",
    "# manual.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0e4dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.157952  , -0.019744  , -0.98720002, -0.009872  ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdps[0]['true_w']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f17e8",
   "metadata": {},
   "source": [
    "### Testing VI, Policy Eval, Succesor Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef28413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DP + SF tests in notebook...\n",
      "✓ Test 1 passed: value iteration shapes\n",
      "✓ Test 2 passed: terminal states have zero value\n",
      "✓ Test 3 passed: policy eval matches value iteration\n",
      "✓ Test 4 passed: successor features reconstruct V\n",
      "✓ Test 5 passed: multi-env value iteration\n",
      "✓ Test 6 passed: multi-env policy evaluation\n",
      "✓ Test 7 passed: multi-env successor features\n",
      "\n",
      "ALL TESTS PASSED ✅\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - All functions are already defined in the notebook\n",
    "# - generate_lavaworld is available\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Running DP + SF tests in notebook...\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Setup\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "envs, mdps, meta = generate_lavaworld(\n",
    "    n_envs=5,\n",
    "    size=6,\n",
    "    seed=123,\n",
    ")\n",
    "mdp = mdps[0]\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "D = mdp[\"Phi\"].shape[1]\n",
    "rng = np.random.default_rng(0)\n",
    "theta = l2_normalize(rng.normal(size=D))\n",
    "\n",
    "S, A, _ = mdp[\"T\"].shape\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 1: Value Iteration shapes\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "V, Q, pi = value_iteration_next_state(\n",
    "    mdp=mdp,\n",
    "    theta=theta,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "assert V.shape == (S,)\n",
    "assert Q.shape == (S, A)\n",
    "assert pi.shape == (S,)\n",
    "\n",
    "print(\"✓ Test 1 passed: value iteration shapes\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 2: Terminal states have zero value\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "terminal = mdp[\"terminal\"]\n",
    "assert np.allclose(V[terminal], 0.0)\n",
    "\n",
    "print(\"✓ Test 2 passed: terminal states have zero value\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 3: Policy evaluation matches value iteration\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "V_pe = policy_evaluation_next_state(\n",
    "    mdp=mdp,\n",
    "    theta=theta,\n",
    "    policy=pi,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "assert np.allclose(V_pe, V, atol=1e-6)\n",
    "\n",
    "print(\"✓ Test 3 passed: policy eval matches value iteration\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 4: Successor features reconstruct value\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "Psi_sa, Psi_s = compute_successor_features_from_q_next_state(\n",
    "    T=mdp[\"T\"],\n",
    "    Phi=mdp[\"Phi\"],\n",
    "    Q=Q,\n",
    "    terminal_mask=mdp[\"terminal\"],\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "V_hat = Psi_s @ theta\n",
    "assert np.allclose(V_hat, V, atol=1e-5)\n",
    "\n",
    "print(\"✓ Test 4 passed: successor features reconstruct V\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 5: Multi-env value iteration matches single-env\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "V_list, Q_list, pi_list = value_iteration_next_state_multi(\n",
    "    mdps=[mdp],\n",
    "    theta=theta,\n",
    "    gamma=gamma,\n",
    "    n_jobs=1,   # IMPORTANT for notebooks\n",
    ")\n",
    "\n",
    "assert np.allclose(V_list[0], V)\n",
    "assert np.allclose(Q_list[0], Q)\n",
    "assert np.all(pi_list[0] == pi)\n",
    "\n",
    "print(\"✓ Test 5 passed: multi-env value iteration\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 6: Multi-env policy evaluation matches single\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "V_multi_pe = policy_evaluation_next_state_multi(\n",
    "    mdps=[mdp],\n",
    "    theta=theta,\n",
    "    policy_list=[pi],\n",
    "    gamma=gamma,\n",
    "    n_jobs=1,\n",
    ")[0]\n",
    "\n",
    "assert np.allclose(V_multi_pe, V_pe)\n",
    "\n",
    "print(\"✓ Test 6 passed: multi-env policy evaluation\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 7: Multi-env successor features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "Psi_sa_list, Psi_s_list = compute_successor_features_multi(\n",
    "    mdps=[mdp],\n",
    "    Q_list=[Q],\n",
    "    gamma=gamma,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "V_hat_multi = Psi_s_list[0] @ theta\n",
    "assert np.allclose(V_hat_multi, V, atol=1e-5)\n",
    "\n",
    "print(\"✓ Test 7 passed: multi-env successor features\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Done\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nALL TESTS PASSED ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2feb5f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running demo + constraint pipeline tests...\n",
      "✓ Test 1 passed: generate_state_action_demos\n",
      "✓ Test 3 passed: generate_demos_from_policies_multi\n",
      "✓ Test 4 passed: constraints_from_demos_next_state\n",
      "✓ Test 5 passed: constraint semantic correctness\n",
      "✓ Test 6 passed: constraints_from_demos_next_state_multi\n",
      "\n",
      "ALL DEMO + CONSTRAINT TESTS PASSED ✅\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Running demo + constraint pipeline tests...\")\n",
    "\n",
    "# ============================================================\n",
    "# Setup environment and prerequisites\n",
    "# ============================================================\n",
    "\n",
    "envs, mdps, meta = generate_lavaworld(\n",
    "    n_envs=5,\n",
    "    size=6,\n",
    "    seed=123,\n",
    ")\n",
    "mdp = mdps[0]\n",
    "\n",
    "gamma = 0.99\n",
    "S, A, _ = mdp[\"T\"].shape\n",
    "D = mdp[\"Phi\"].shape[1]\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "theta = l2_normalize(rng.normal(size=D))\n",
    "\n",
    "# Compute optimal policy and successor features\n",
    "V, Q, pi = value_iteration_next_state(\n",
    "    mdp=mdp,\n",
    "    theta=theta,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "Psi_sa, Psi_s = compute_successor_features_from_q_next_state(\n",
    "    T=mdp[\"T\"],\n",
    "    Phi=mdp[\"Phi\"],\n",
    "    Q=Q,\n",
    "    terminal_mask=mdp[\"terminal\"],\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Test 1: generate_state_action_demos (single env)\n",
    "# ============================================================\n",
    "\n",
    "states = enumerate_states(mdp[\"size\"], mdp[\"wall_mask\"])\n",
    "\n",
    "demos = generate_state_action_demos(\n",
    "    states=states,\n",
    "    pi=pi,\n",
    "    terminal_mask=mdp[\"terminal\"],\n",
    "    idx_of=mdp[\"idx_of\"],\n",
    ")\n",
    "\n",
    "# 1a. demos are valid indices\n",
    "for s, a in demos:\n",
    "    assert 0 <= s < S\n",
    "    assert 0 <= a < A\n",
    "\n",
    "# 1b. no terminal states\n",
    "for s, _ in demos:\n",
    "    assert not mdp[\"terminal\"][s]\n",
    "\n",
    "# 1c. action matches policy\n",
    "for s, a in demos:\n",
    "    assert a == pi[s]\n",
    "\n",
    "# 1d. every nonterminal state in states appears\n",
    "for s_tuple in states:\n",
    "    i = mdp[\"idx_of\"][s_tuple]\n",
    "    if not mdp[\"terminal\"][i]:\n",
    "        assert (i, int(pi[i])) in demos\n",
    "\n",
    "print(\"✓ Test 1 passed: generate_state_action_demos\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 2: _generate_demos_only_worker\n",
    "# ============================================================\n",
    "\n",
    "# demos_worker = _generate_demos_only_worker((mdp, pi))\n",
    "# assert demos_worker == demos\n",
    "\n",
    "# print(\"✓ Test 2 passed: _generate_demos_only_worker\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 3: generate_demos_from_policies_multi\n",
    "# ============================================================\n",
    "\n",
    "demos_list = generate_demos_from_policies_multi(\n",
    "    mdps=[mdp],\n",
    "    pi_list=[pi],\n",
    "    n_jobs=1,   # notebook-safe\n",
    ")\n",
    "\n",
    "assert len(demos_list) == 1\n",
    "assert demos_list[0] == demos\n",
    "\n",
    "print(\"✓ Test 3 passed: generate_demos_from_policies_multi\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 4: constraints_from_demos_next_state (single env)\n",
    "# ============================================================\n",
    "\n",
    "constraints = constraints_from_demos_next_state(\n",
    "    demos=demos,\n",
    "    Psi_sa=Psi_sa,\n",
    "    terminal_mask=mdp[\"terminal\"],\n",
    "    normalize=True,\n",
    ")\n",
    "\n",
    "# 4a. constraints exist\n",
    "assert len(constraints) > 0\n",
    "\n",
    "# 4b. correct shape\n",
    "for c in constraints:\n",
    "    assert c.shape == (D,)\n",
    "\n",
    "# 4c. normalized\n",
    "for c in constraints:\n",
    "    assert np.isclose(np.linalg.norm(c), 1.0, atol=1e-6)\n",
    "\n",
    "print(\"✓ Test 4 passed: constraints_from_demos_next_state\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 5: constraint semantic validity\n",
    "# (ψ(s,a*) − ψ(s,a)) · θ ≥ 0\n",
    "# ============================================================\n",
    "\n",
    "for v in constraints:\n",
    "    assert np.dot(v, theta) >= -1e-8\n",
    "\n",
    "print(\"✓ Test 5 passed: constraint semantic correctness\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 6: constraints_from_demos_next_state_multi\n",
    "# ============================================================\n",
    "\n",
    "constraints_multi = constraints_from_demos_next_state_multi(\n",
    "    demos_list=[demos],\n",
    "    Psi_sa_list=[Psi_sa],\n",
    "    terminal_mask_list=[mdp[\"terminal\"]],\n",
    "    normalize=True,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "assert len(constraints_multi) == 1\n",
    "assert len(constraints_multi[0]) == len(constraints)\n",
    "\n",
    "for c1, c2 in zip(constraints_multi[0], constraints):\n",
    "    assert np.allclose(c1, c2)\n",
    "\n",
    "print(\"✓ Test 6 passed: constraints_from_demos_next_state_multi\")\n",
    "\n",
    "# ============================================================\n",
    "# Final confirmation\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nALL DEMO + CONSTRAINT TESTS PASSED ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f889345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9395ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef18912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_pools = generate_trajectory_pools_multi(\n",
    "    mdps=mdps,\n",
    "    n_trajs_per_state=1,\n",
    "    max_horizon=25,\n",
    "    n_jobs=4,\n",
    ")\n",
    "\n",
    "pairwise_list, correction_list, estop_list = generate_feedback_multi(\n",
    "    traj_pools=traj_pools,\n",
    "    mdps=mdps,\n",
    "    gamma=0.99,\n",
    "    n_pairs=200,\n",
    "    num_random_trajs=8,\n",
    "    estop_beta=10.0,\n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "922c1ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trajectory + feedback validity tests...\n",
      "✓ Test 1 passed: trajectory pool structural validity\n",
      "✓ Test 1b passed: trajectory returns finite\n",
      "✓ Test 2a passed: pairwise preferences reward-consistent\n",
      "✓ Test 2b passed: correction feedback improves reward\n",
      "✓ Test 2c passed: E-stop prefix reward ≥ full reward\n",
      "\n",
      "ALL TRAJECTORY + FEEDBACK TESTS PASSED ✅\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Running trajectory + feedback validity tests...\")\n",
    "\n",
    "# ============================================================\n",
    "# Setup environment\n",
    "# ============================================================\n",
    "\n",
    "envs, mdps, meta = generate_lavaworld(\n",
    "    n_envs=1,\n",
    "    size=6,\n",
    "    seed=123,\n",
    ")\n",
    "mdp = mdps[0]\n",
    "\n",
    "gamma = 0.99\n",
    "theta_true = mdp[\"true_w\"]\n",
    "\n",
    "# ============================================================\n",
    "# Test 1: generate_trajectory_pools_multi\n",
    "# ============================================================\n",
    "\n",
    "traj_pools = generate_trajectory_pools_multi(\n",
    "    mdps=[mdp],\n",
    "    n_trajs_per_state=3,\n",
    "    max_horizon=20,\n",
    "    n_jobs=1,   # notebook-safe\n",
    ")\n",
    "\n",
    "assert len(traj_pools) == 1\n",
    "trajectories = traj_pools[0]\n",
    "assert len(trajectories) > 0\n",
    "\n",
    "idx_of = mdp[\"idx_of\"]\n",
    "\n",
    "# --- structural checks ---\n",
    "for traj in trajectories:\n",
    "    if len(traj) == 0:\n",
    "        continue\n",
    "\n",
    "    # start state valid and non-terminal\n",
    "    s0 = traj[0][0]\n",
    "    assert s0 in idx_of\n",
    "    assert not mdp[\"terminal\"][idx_of[s0]]\n",
    "\n",
    "    terminal_reached = False\n",
    "\n",
    "    for (s, a, sp) in traj:\n",
    "        assert s in idx_of\n",
    "        assert sp in idx_of\n",
    "        assert a in ACTIONS\n",
    "\n",
    "        if terminal_reached:\n",
    "            raise AssertionError(\"Transition after terminal state\")\n",
    "\n",
    "        if mdp[\"terminal\"][idx_of[sp]]:\n",
    "            terminal_reached = True\n",
    "\n",
    "print(\"✓ Test 1 passed: trajectory pool structural validity\")\n",
    "\n",
    "# --- reward sanity ---\n",
    "returns = [\n",
    "    trajectory_return(traj, mdp, theta_true, gamma)\n",
    "    for traj in trajectories\n",
    "]\n",
    "\n",
    "assert all(np.isfinite(r) for r in returns)\n",
    "\n",
    "print(\"✓ Test 1b passed: trajectory returns finite\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 2: generate_feedback_multi\n",
    "# ============================================================\n",
    "\n",
    "pairwise_list, correction_list, estop_list = generate_feedback_multi(\n",
    "    traj_pools=traj_pools,\n",
    "    mdps=[mdp],\n",
    "    gamma=gamma,\n",
    "    n_pairs=50,\n",
    "    num_random_trajs=5,\n",
    "    estop_beta=5.0,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "pairwise = pairwise_list[0]\n",
    "corrections = correction_list[0]\n",
    "estops = estop_list[0]\n",
    "\n",
    "# ============================================================\n",
    "# Test 2a: pairwise preference validity\n",
    "# ============================================================\n",
    "\n",
    "for tau_good, tau_bad in pairwise:\n",
    "    R_good = trajectory_return(tau_good, mdp, theta_true, gamma)\n",
    "    R_bad  = trajectory_return(tau_bad,  mdp, theta_true, gamma)\n",
    "\n",
    "    assert R_good >= R_bad - 1e-8\n",
    "\n",
    "print(\"✓ Test 2a passed: pairwise preferences reward-consistent\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 2b: correction feedback validity\n",
    "# ============================================================\n",
    "\n",
    "for tau_improved, tau_orig in corrections:\n",
    "    # same start state\n",
    "    assert tau_improved[0][0] == tau_orig[0][0]\n",
    "\n",
    "    R_new = trajectory_return(tau_improved, mdp, theta_true, gamma)\n",
    "    R_old = trajectory_return(tau_orig,     mdp, theta_true, gamma)\n",
    "\n",
    "    assert R_new > R_old + 1e-8\n",
    "\n",
    "print(\"✓ Test 2b passed: correction feedback improves reward\")\n",
    "\n",
    "# ============================================================\n",
    "# Correct Test 2c: E-stop feedback validity\n",
    "# Prefix reward >= full trajectory reward\n",
    "# ============================================================\n",
    "\n",
    "for traj, t_stop in estops:\n",
    "    assert isinstance(t_stop, int)\n",
    "    assert 0 <= t_stop < len(traj)\n",
    "\n",
    "    # compute prefix reward up to stop\n",
    "    prefix_reward = 0.0\n",
    "    for i in range(t_stop + 1):\n",
    "        _, _, sp = traj[i]\n",
    "        prefix_reward += mdp[\"Phi\"][mdp[\"idx_of\"][sp]] @ theta_true\n",
    "\n",
    "    # compute full reward\n",
    "    full_reward = sum(\n",
    "        mdp[\"Phi\"][mdp[\"idx_of\"][sp]] @ theta_true\n",
    "        for (_, _, sp) in traj\n",
    "    )\n",
    "\n",
    "    # E-stop semantics:\n",
    "    # prefix is preferred to full\n",
    "    assert prefix_reward > full_reward - 1e-8, (\n",
    "        \"E-stop prefix reward is worse than full trajectory\"\n",
    "    )\n",
    "\n",
    "print(\"✓ Test 2c passed: E-stop prefix reward ≥ full reward\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Done\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nALL TRAJECTORY + FEEDBACK TESTS PASSED ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8dfed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 atoms generated\n"
     ]
    }
   ],
   "source": [
    "from utils import GenerationSpec, DemoSpec, FeedbackSpec\n",
    "from utils import *\n",
    "\n",
    "# --- you already have these ---\n",
    "# mdps\n",
    "# pi_list\n",
    "# enumerate_states\n",
    "# generate_trajectory_pools_multi\n",
    "# generate_pairwise_preferences\n",
    "# simulate_human_estop_one_mdp\n",
    "# generate_correction_feedback\n",
    "\n",
    "spec = GenerationSpec(\n",
    "    seed=123,\n",
    "    demo=DemoSpec(\n",
    "        enabled=True,\n",
    "        env_fraction=0.6,\n",
    "        state_fraction=0.4,\n",
    "    ),\n",
    "    pairwise=FeedbackSpec(\n",
    "        enabled=True,\n",
    "        total_budget=10,\n",
    "        alloc_method=\"dirichlet\",\n",
    "        alloc_params={\"alpha\": 0.3},\n",
    "    ),\n",
    "    estop=FeedbackSpec(\n",
    "        enabled=True,\n",
    "        total_budget=200,\n",
    "        alloc_method=\"sparse_poisson\",\n",
    "        alloc_params={\"p_active\": 0.4, \"mean\": 400},\n",
    "    ),\n",
    "    improvement=FeedbackSpec(\n",
    "        enabled=True,\n",
    "        total_budget=300,\n",
    "        alloc_method=\"dirichlet\",\n",
    "        alloc_params={\"alpha\": 0.5},\n",
    "    ),\n",
    ")\n",
    "\n",
    "atoms_per_env = generate_candidate_atoms_for_scot_minigrid(\n",
    "    mdps=mdps,\n",
    "    pi_list=pi_list,\n",
    "    spec=spec,\n",
    "    enumerate_states=enumerate_states,\n",
    "    generate_trajectory_pools_multi=generate_trajectory_pools_multi,\n",
    "    pairwise_fn=generate_pairwise_preferences,\n",
    "    estop_fn=simulate_human_estop_one_mdp,\n",
    "    improvement_fn=generate_correction_feedback,\n",
    ")\n",
    "\n",
    "print(sum(len(a) for a in atoms_per_env), \"atoms generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97b7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55aac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dc529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9214838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
