{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccba47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#from utils import remove_redundant_constraints\n",
    "\n",
    "#from __future__ import annotations\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.mission import MissionSpace\n",
    "\n",
    "# test_lavaworld_manual.py\n",
    "from utils import (generate_lavaworld, \n",
    "                    generate_demos_from_policies_multi, \n",
    "                    constraints_from_demos_next_state_multi,\n",
    "                    generate_trajectory_pools_multi,\n",
    "                    generate_feedback_multi)\n",
    "\n",
    "from minigrid.manual_control import ManualControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "envs, mdps, meta = generate_lavaworld(\n",
    "    n_envs=5,\n",
    "    size=8,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Generated\", len(envs), \"envs\")\n",
    "\n",
    "#env = envs[9]\n",
    "#env.reset()\n",
    "\n",
    "# print(\"Goal at:\", meta[\"goals\"][0])\n",
    "# print(\"Lava cells:\")\n",
    "# print(meta[\"lava_masks\"][0].astype(int))\n",
    "\n",
    "# manual = ManualControl(env, seed=0)\n",
    "# manual.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta_true_list = []\n",
    "for mdp in mdps:\n",
    "    D = mdp[\"Phi\"].shape[1]\n",
    "    theta_true_list.append(np.random.randn(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_pools = generate_trajectory_pools_multi(\n",
    "    mdps=mdps,\n",
    "    n_trajs_per_state=3,\n",
    "    max_horizon=25,\n",
    "    n_jobs=4,\n",
    ")\n",
    "\n",
    "pairwise_list, correction_list, estop_list = generate_feedback_multi(\n",
    "    traj_pools=traj_pools,\n",
    "    mdps=mdps,\n",
    "    theta_true_list=theta_true_list,\n",
    "    gamma=0.99,\n",
    "    n_pairs=200,\n",
    "    num_random_trajs=8,\n",
    "    estop_beta=10.0,\n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a fixed policy with NEXT-state reward:\n",
    "      V(s) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "    Terminal states are kept at V=0 (consistent with your VI done-cutoff).\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)  # 1 if nonterminal, 0 if terminal\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "            a = int(policy[s])\n",
    "            v_new = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def value_iteration_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    "):\n",
    "    \"\"\"\n",
    "    NEXT-state reward value iteration:\n",
    "      Q(s,a) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "      V(s) = max_a Q(s,a)\n",
    "    Terminal states fixed at V=0.\n",
    "    Returns: V, Q, pi\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "    Q = np.zeros((S, A), dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            # compute Q(s,a) for all a\n",
    "            for a in range(A):\n",
    "                Q[s, a] = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "\n",
    "            v_new = float(np.max(Q[s]))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # greedy policy\n",
    "    pi = np.zeros(S, dtype=int)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            pi[s] = ACT_FORWARD\n",
    "        else:\n",
    "            pi[s] = int(np.argmax(Q[s]))\n",
    "\n",
    "    return V, Q, pi\n",
    "\n",
    "def compute_successor_features_from_q_next_state(\n",
    "    T: np.ndarray,\n",
    "    Phi: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    tol: float = 1e-10,\n",
    "    max_iters: int = 100000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Successor Features with NEXT-STATE (entering) convention, consistent with your code.\n",
    "\n",
    "    Definitions:\n",
    "      π(s)      = argmax_a Q(s,a)\n",
    "      ψ(s)      = E_π [ sum_t γ^t φ(s_{t+1}) | s0 = s ]\n",
    "      ψ(s,a)    = E [ φ(s1) + γ ψ(s1) | s0=s, a0=a ]\n",
    "\n",
    "    Bellman equation:\n",
    "      ψ(s) = Σ_{s'} P_π(s,s') [ φ(s') + γ * 1[~terminal(s')] * ψ(s') ]\n",
    "\n",
    "    Inputs:\n",
    "      T             : (S,A,S) transition matrix\n",
    "      Phi           : (S,D) state feature matrix (φ(s))\n",
    "      Q             : (S,A) Q-values (used to extract greedy policy)\n",
    "      terminal_mask : (S,) boolean\n",
    "      gamma         : discount factor\n",
    "\n",
    "    Returns:\n",
    "      Psi_sa : (S,A,D) successor features for state-action\n",
    "      Psi_s  : (S,D)   successor features for state\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    D = Phi.shape[1]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Greedy policy from Q\n",
    "    # -----------------------------\n",
    "    Pi = np.zeros((S, A), dtype=float)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            continue\n",
    "        Pi[s, np.argmax(Q[s])] = 1.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Policy transition matrix\n",
    "    # P_pi[s,s'] = Σ_a π(a|s) T[s,a,s']\n",
    "    # -----------------------------\n",
    "    P_pi = np.zeros((S, S), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            if Pi[s, a] > 0:\n",
    "                P_pi[s] += Pi[s, a] * T[s, a]\n",
    "\n",
    "        # absorbing fallback (safety)\n",
    "        if P_pi[s].sum() == 0:\n",
    "            P_pi[s, s] = 1.0\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Iterative policy SFs ψ(s)\n",
    "    # -----------------------------\n",
    "    Psi_s = np.zeros((S, D), dtype=float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        Psi_old = Psi_s.copy()\n",
    "\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            exp_phi_next = P_pi[s] @ Phi\n",
    "            exp_psi_next = P_pi[s] @ Psi_old\n",
    "\n",
    "            Psi_s[s] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "        if np.max(np.abs(Psi_s - Psi_old)) < tol:\n",
    "            break\n",
    "\n",
    "    # -----------------------------\n",
    "    # State–action successor features ψ(s,a)\n",
    "    # -----------------------------\n",
    "    Psi_sa = np.zeros((S, A, D), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            p_next = T[s, a]\n",
    "            exp_phi_next = p_next @ Phi\n",
    "            exp_psi_next = p_next @ Psi_s\n",
    "            Psi_sa[s, a] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "    return Psi_sa, Psi_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _policy_eval_worker(args):\n",
    "    T, r_next, policy, terminal_mask, gamma, theta, max_iters = args\n",
    "    return policy_evaluation_next_state(\n",
    "        T=T,\n",
    "        r_next=r_next,\n",
    "        policy=policy,\n",
    "        terminal_mask=terminal_mask,\n",
    "        gamma=gamma,\n",
    "        theta=theta,\n",
    "        max_iters=max_iters,\n",
    "    )\n",
    "\n",
    "def policy_evaluation_next_state_multi(\n",
    "    mdps,\n",
    "    r_next_list,\n",
    "    policy_list,\n",
    "    gamma,\n",
    "    theta=1e-8,\n",
    "    max_iters=200000,\n",
    "    n_jobs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel policy evaluation over multiple envs.\n",
    "\n",
    "    mdps        : list of mdp dicts\n",
    "    r_next_list : list of r_next vectors (one per env)\n",
    "    policy_list : list of policies (one per env)\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = cpu_count()\n",
    "\n",
    "    args = [\n",
    "        (\n",
    "            mdp[\"T\"],\n",
    "            r_next,\n",
    "            policy,\n",
    "            mdp[\"terminal\"],\n",
    "            gamma,\n",
    "            theta,\n",
    "            max_iters,\n",
    "        )\n",
    "        for mdp, r_next, policy in zip(mdps, r_next_list, policy_list)\n",
    "    ]\n",
    "\n",
    "    with Pool(n_jobs) as pool:\n",
    "        Vs = pool.map(_policy_eval_worker, args)\n",
    "\n",
    "    return Vs\n",
    "\n",
    "def _vi_worker(args):\n",
    "    T, r_next, terminal_mask, gamma, theta, max_iters = args\n",
    "    return value_iteration_next_state(\n",
    "        T=T,\n",
    "        r_next=r_next,\n",
    "        terminal_mask=terminal_mask,\n",
    "        gamma=gamma,\n",
    "        theta=theta,\n",
    "        max_iters=max_iters,\n",
    "    )\n",
    "\n",
    "def value_iteration_next_state_multi(\n",
    "    mdps,\n",
    "    r_next_list,\n",
    "    gamma,\n",
    "    theta=1e-8,\n",
    "    max_iters=200000,\n",
    "    n_jobs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel value iteration over multiple envs.\n",
    "\n",
    "    Returns:\n",
    "        V_list, Q_list, pi_list\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = cpu_count()\n",
    "\n",
    "    args = [\n",
    "        (\n",
    "            mdp[\"T\"],\n",
    "            r_next,\n",
    "            mdp[\"terminal\"],\n",
    "            gamma,\n",
    "            theta,\n",
    "            max_iters,\n",
    "        )\n",
    "        for mdp, r_next in zip(mdps, r_next_list)\n",
    "    ]\n",
    "\n",
    "    with Pool(n_jobs) as pool:\n",
    "        results = pool.map(_vi_worker, args)\n",
    "\n",
    "    V_list, Q_list, pi_list = zip(*results)\n",
    "    return list(V_list), list(Q_list), list(pi_list)\n",
    "\n",
    "def _sf_worker(args):\n",
    "    T, Phi, Q, terminal_mask, gamma, tol, max_iters = args\n",
    "    return compute_successor_features_from_q_next_state(\n",
    "        T=T,\n",
    "        Phi=Phi,\n",
    "        Q=Q,\n",
    "        terminal_mask=terminal_mask,\n",
    "        gamma=gamma,\n",
    "        tol=tol,\n",
    "        max_iters=max_iters,\n",
    "    )\n",
    "\n",
    "def compute_successor_features_multi(\n",
    "    mdps,\n",
    "    Q_list,\n",
    "    gamma,\n",
    "    tol=1e-10,\n",
    "    max_iters=100000,\n",
    "    n_jobs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel successor feature computation.\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = cpu_count()\n",
    "\n",
    "    args = [\n",
    "        (\n",
    "            mdp[\"T\"],\n",
    "            mdp[\"Phi\"],\n",
    "            Q,\n",
    "            mdp[\"terminal\"],\n",
    "            gamma,\n",
    "            tol,\n",
    "            max_iters,\n",
    "        )\n",
    "        for mdp, Q in zip(mdps, Q_list)\n",
    "    ]\n",
    "\n",
    "    with Pool(n_jobs) as pool:\n",
    "        results = pool.map(_sf_worker, args)\n",
    "\n",
    "    Psi_sa_list, Psi_s_list = zip(*results)\n",
    "    return list(Psi_sa_list), list(Psi_s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline_end_to_end(mdps, r_next_list, gamma=0.99, tol=1e-8):\n",
    "    \"\"\"\n",
    "    End-to-end test:\n",
    "    VI → SF → demos → constraints\n",
    "    Passing outputs strictly forward.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Value Iteration\n",
    "    # --------------------------------------------------\n",
    "    V_list, Q_list, pi_list = value_iteration_next_state_multi(\n",
    "        mdps=mdps,\n",
    "        r_next_list=r_next_list,\n",
    "        gamma=gamma,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    assert len(V_list) == len(mdps)\n",
    "    assert len(Q_list) == len(mdps)\n",
    "    assert len(pi_list) == len(mdps)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2) Successor Features (from Q)\n",
    "    # --------------------------------------------------\n",
    "    Psi_sa_list, Psi_s_list = compute_successor_features_multi(\n",
    "        mdps=mdps,\n",
    "        Q_list=Q_list,\n",
    "        gamma=gamma,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    assert len(Psi_sa_list) == len(mdps)\n",
    "    assert len(Psi_s_list) == len(mdps)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3) Demos (from policy)\n",
    "    # --------------------------------------------------\n",
    "    demos_list = generate_demos_from_policies_multi(\n",
    "        mdps=mdps,\n",
    "        pi_list=pi_list,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    assert len(demos_list) == len(mdps)\n",
    "\n",
    "    # Each demo must agree with π and avoid terminals\n",
    "    for mdp, demos, pi in zip(mdps, demos_list, pi_list):\n",
    "        terminal = mdp[\"terminal\"]\n",
    "        for s, a in demos:\n",
    "            assert not terminal[s]\n",
    "            assert a == pi[s]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4) Constraints (from demos + SFs)\n",
    "    # --------------------------------------------------\n",
    "    terminal_mask_list = [mdp[\"terminal\"] for mdp in mdps]\n",
    "\n",
    "    constraints_per_env = constraints_from_demos_next_state_multi(\n",
    "        demos_list=demos_list,\n",
    "        Psi_sa_list=Psi_sa_list,\n",
    "        terminal_mask_list=terminal_mask_list,\n",
    "        normalize=True,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    assert len(constraints_per_env) == len(mdps)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5) Constraint sanity checks\n",
    "    # --------------------------------------------------\n",
    "    for i, (mdp, constraints) in enumerate(zip(mdps, constraints_per_env)):\n",
    "        D = mdp[\"Phi\"].shape[1]\n",
    "\n",
    "        for c in constraints:\n",
    "            assert c.shape == (D,)\n",
    "            assert np.linalg.norm(c) > tol\n",
    "\n",
    "    print(\"✅ END-TO-END PIPELINE TEST PASSED\")\n",
    "\n",
    "r_next_list = [\n",
    "    np.random.randn(mdp[\"T\"].shape[0])\n",
    "    for mdp in mdps\n",
    "]\n",
    "\n",
    "test_pipeline_end_to_end(\n",
    "    mdps=mdps,\n",
    "    r_next_list=r_next_list,\n",
    "    gamma=0.99,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55aac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dc529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9214838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0131c5f7",
   "metadata": {},
   "source": [
    "### testing above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def max_abs(x):\n",
    "    return float(np.max(np.abs(x)))\n",
    "\n",
    "def assert_close(a, b, tol, name=\"\"):\n",
    "    err = max_abs(a - b)\n",
    "    print(f\"[{name}] max error = {err:.3e}\")\n",
    "    assert err < tol, f\"{name} failed (err={err})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ed348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vi_vs_policy_eval(mdps, gamma, tol=1e-6):\n",
    "    print(\"\\n=== Test: VI vs Policy Evaluation ===\")\n",
    "\n",
    "    for i, mdp in enumerate(mdps):\n",
    "        T = mdp[\"T\"]\n",
    "        terminal = mdp[\"terminal\"]\n",
    "\n",
    "        # random NEXT-state reward\n",
    "        r_next = np.random.randn(T.shape[0])\n",
    "\n",
    "        V, Q, pi = value_iteration_next_state(\n",
    "            T=T,\n",
    "            r_next=r_next,\n",
    "            terminal_mask=terminal,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        V_eval = policy_evaluation_next_state(\n",
    "            T=T,\n",
    "            r_next=r_next,\n",
    "            policy=pi,\n",
    "            terminal_mask=terminal,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        assert_close(V, V_eval, tol, name=f\"env {i}\")\n",
    "\n",
    "    print(\"✅ Passed: VI and policy evaluation match\")\n",
    "\n",
    "def test_bellman_optimality(mdps, gamma, tol=1e-6):\n",
    "    print(\"\\n=== Test: Bellman Optimality ===\")\n",
    "\n",
    "    for i, mdp in enumerate(mdps):\n",
    "        T = mdp[\"T\"]\n",
    "        terminal = mdp[\"terminal\"]\n",
    "        r_next = np.random.randn(T.shape[0])\n",
    "\n",
    "        V, Q, pi = value_iteration_next_state(\n",
    "            T=T,\n",
    "            r_next=r_next,\n",
    "            terminal_mask=terminal,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        cont = (~terminal).astype(float)\n",
    "\n",
    "        for s in range(T.shape[0]):\n",
    "            if terminal[s]:\n",
    "                continue\n",
    "            q_star = np.max([\n",
    "                np.sum(T[s, a] * (r_next + gamma * cont * V))\n",
    "                for a in range(T.shape[1])\n",
    "            ])\n",
    "            assert abs(V[s] - q_star) < tol\n",
    "\n",
    "    print(\"✅ Passed: Bellman optimality\")\n",
    "\n",
    "def test_successor_features_value_reconstruction(mdps, gamma, tol=1e-5):\n",
    "    print(\"\\n=== Test: Successor Features → Value ===\")\n",
    "\n",
    "    for i, mdp in enumerate(mdps):\n",
    "        T = mdp[\"T\"]\n",
    "        Phi = mdp[\"Phi\"]\n",
    "        terminal = mdp[\"terminal\"]\n",
    "\n",
    "        D = Phi.shape[1]\n",
    "        w = np.random.randn(D)\n",
    "\n",
    "        r_next = Phi @ w\n",
    "\n",
    "        V, Q, pi = value_iteration_next_state(\n",
    "            T=T,\n",
    "            r_next=r_next,\n",
    "            terminal_mask=terminal,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        Psi_sa, Psi_s = compute_successor_features_from_q_next_state(\n",
    "            T=T,\n",
    "            Phi=Phi,\n",
    "            Q=Q,\n",
    "            terminal_mask=terminal,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        V_hat = Psi_s @ w\n",
    "\n",
    "        assert_close(V, V_hat, tol, name=f\"env {i}\")\n",
    "\n",
    "    print(\"✅ Passed: SF value reconstruction\")\n",
    "\n",
    "def test_terminal_states(mdps):\n",
    "    print(\"\\n=== Test: Terminal States ===\")\n",
    "\n",
    "    for i, mdp in enumerate(mdps):\n",
    "        terminal = mdp[\"terminal\"]\n",
    "        T = mdp[\"T\"]\n",
    "        r_next = np.random.randn(T.shape[0])\n",
    "\n",
    "        V, Q, pi = value_iteration_next_state(\n",
    "            T=T,\n",
    "            r_next=r_next,\n",
    "            terminal_mask=terminal,\n",
    "            gamma=0.99,\n",
    "        )\n",
    "\n",
    "        assert np.all(V[terminal] == 0.0)\n",
    "        assert np.all(Q[terminal] == 0.0)\n",
    "\n",
    "    print(\"✅ Passed: terminal states fixed at zero\")\n",
    "\n",
    "def test_multi_vs_single(mdps, gamma, tol=1e-6):\n",
    "    print(\"\\n=== Test: Multi-env vs Single-env ===\")\n",
    "\n",
    "    r_list = [np.random.randn(mdp[\"T\"].shape[0]) for mdp in mdps]\n",
    "\n",
    "    V_single, Q_single, pi_single = [], [], []\n",
    "    for mdp, r in zip(mdps, r_list):\n",
    "        V, Q, pi = value_iteration_next_state(\n",
    "            T=mdp[\"T\"],\n",
    "            r_next=r,\n",
    "            terminal_mask=mdp[\"terminal\"],\n",
    "            gamma=gamma,\n",
    "        )\n",
    "        V_single.append(V)\n",
    "        Q_single.append(Q)\n",
    "        pi_single.append(pi)\n",
    "\n",
    "    V_multi, Q_multi, pi_multi = value_iteration_next_state_multi(\n",
    "        mdps=mdps,\n",
    "        r_next_list=r_list,\n",
    "        gamma=gamma,\n",
    "        n_jobs=2,\n",
    "    )\n",
    "\n",
    "    for i in range(len(mdps)):\n",
    "        assert_close(V_single[i], V_multi[i], tol, f\"V env {i}\")\n",
    "        assert_close(Q_single[i], Q_multi[i], tol, f\"Q env {i}\")\n",
    "        assert np.all(pi_single[i] == pi_multi[i])\n",
    "\n",
    "    print(\"✅ Passed: multi-env consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03536d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_tests(mdps, gamma=0.99):\n",
    "    test_vi_vs_policy_eval(mdps, gamma)\n",
    "    test_bellman_optimality(mdps, gamma)\n",
    "    test_successor_features_value_reconstruction(mdps, gamma)\n",
    "    test_terminal_states(mdps)\n",
    "    test_multi_vs_single(mdps, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs, mdps, meta = generate_lavaworld(\n",
    "    n_envs=10,\n",
    "    size=8,\n",
    "    seed=42,\n",
    ")\n",
    "run_all_tests(mdps, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da775dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
