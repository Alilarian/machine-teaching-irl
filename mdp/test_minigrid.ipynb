{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ae0d00",
   "metadata": {},
   "source": [
    "## Manually control env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4c6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import gymnasium as gym\n",
    "# import minigrid\n",
    "# from minigrid.manual_control import ManualControl\n",
    "# from minigrid.wrappers import FullyObsWrapper\n",
    "\n",
    "# def run_manual_crossing(size=9, crossings=1):\n",
    "#     env = gym.make(\n",
    "#         #f\"MiniGrid-CrossingS{size}N{crossings}-v0\",\n",
    "#         \"MiniGrid-SimpleCrossingS11N5-v0\",\n",
    "#         render_mode=\"human\"\n",
    "#     )\n",
    "\n",
    "#     # Optional: full observability (recommended for debugging)\n",
    "#     env = FullyObsWrapper(env)\n",
    "\n",
    "#     manual = ManualControl(env)\n",
    "#     manual.start()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_manual_crossing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6d67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils import remove_redundant_constraints\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.mission import MissionSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14332a3f",
   "metadata": {},
   "source": [
    "### Set cover algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17db8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_key(v, *, normalize=True, round_decimals=12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = np.linalg.norm(v)\n",
    "    if n == 0.0 or not np.isfinite(n):\n",
    "        return (\"ZERO\",)\n",
    "    if normalize:\n",
    "        v = v / n\n",
    "    return tuple(np.round(v, round_decimals))\n",
    "\n",
    "\n",
    "def scot_greedy_single_env(\n",
    "    *,\n",
    "    atoms,               # list of atoms (e.g., demo indices or (s,a) pairs)\n",
    "    constraints_per_atom,# list[list[np.ndarray]]\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal SCOT greedy set cover for ONE environment.\n",
    "\n",
    "    Universe:\n",
    "        unique constraint directions across all atoms\n",
    "\n",
    "    Atom coverage:\n",
    "        atom covers a constraint if it generates that direction\n",
    "\n",
    "    Returns:\n",
    "        chosen_atoms\n",
    "        chosen_constraints (stacked)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Build universe of unique constraints\n",
    "    # --------------------------------------------------\n",
    "    key_to_uid = {}\n",
    "    uid_to_vec = {}\n",
    "    uid = 0\n",
    "\n",
    "    atom_constraint_keys = []\n",
    "\n",
    "    for atom_constraints in constraints_per_atom:\n",
    "        keys = set()\n",
    "        for v in atom_constraints:\n",
    "            k = constraint_key(v,\n",
    "                               normalize=normalize,\n",
    "                               round_decimals=round_decimals)\n",
    "            if k not in key_to_uid:\n",
    "                key_to_uid[k] = uid\n",
    "                uid_to_vec[uid] = v\n",
    "                uid += 1\n",
    "            keys.add(key_to_uid[k])\n",
    "        atom_constraint_keys.append(keys)\n",
    "\n",
    "    universe = set(range(uid))\n",
    "    covered = set()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Greedy set cover\n",
    "    # --------------------------------------------------\n",
    "    chosen_atoms = []\n",
    "    chosen_constraints = []\n",
    "\n",
    "    while covered != universe:\n",
    "        best_gain = 0\n",
    "        best_idx = None\n",
    "        best_new = None\n",
    "\n",
    "        for i, cov in enumerate(atom_constraint_keys):\n",
    "            gain_set = cov - covered\n",
    "            if len(gain_set) > best_gain:\n",
    "                best_gain = len(gain_set)\n",
    "                best_idx = i\n",
    "                best_new = gain_set\n",
    "\n",
    "        if best_idx is None:\n",
    "            break  # cannot cover remaining constraints\n",
    "\n",
    "        chosen_atoms.append(atoms[best_idx])\n",
    "        for uid in best_new:\n",
    "            chosen_constraints.append(uid_to_vec[uid])\n",
    "\n",
    "        covered |= best_new\n",
    "\n",
    "    if chosen_constraints:\n",
    "        chosen_constraints = np.vstack(chosen_constraints)\n",
    "    else:\n",
    "        chosen_constraints = np.zeros((0, atoms[0][0].shape[-1]))\n",
    "\n",
    "    return chosen_atoms, chosen_constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a63f1",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96202c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUCCESSOR FEATURES DEBUG =====\n",
      "State 000 (1, 1, 0)\n",
      "  ψ(s)        = [3. 0. 1. 1.]\n",
      "  ψ(s)·θ      = -1.470928\n",
      "  V_true(s)   = -1.470928\n",
      "State 001 (1, 1, 1)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 002 (1, 1, 2)\n",
      "  ψ(s)        = [11.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.926563\n",
      "  V_true(s)   = -1.926563\n",
      "State 003 (1, 1, 3)\n",
      "  ψ(s)        = [6.97 1.   0.99 1.99]\n",
      "  ψ(s)·θ      = -2.117643\n",
      "  V_true(s)   = -2.117643\n",
      "State 008 (1, 3, 0)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 009 (1, 3, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 010 (1, 3, 2)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 011 (1, 3, 3)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 012 (2, 1, 0)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 013 (2, 1, 1)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 014 (2, 1, 2)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 015 (2, 1, 3)\n",
      "  ψ(s)        = [10.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.768611\n",
      "  V_true(s)   = -1.768611\n",
      "State 020 (2, 3, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 021 (2, 3, 1)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 022 (2, 3, 2)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 023 (2, 3, 3)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 024 (3, 1, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 025 (3, 1, 1)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 026 (3, 1, 2)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 027 (3, 1, 3)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 028 (3, 2, 0)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 029 (3, 2, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 030 (3, 2, 2)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 031 (3, 2, 3)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Utils\n",
    "# ======================================================\n",
    "def l2_normalize(w, eps=1e-8):\n",
    "    n = np.linalg.norm(w)\n",
    "    return w if n < eps else w / n\n",
    "\n",
    "# ======================================================\n",
    "# Feature extraction (state-based, canonical)\n",
    "# ======================================================\n",
    "\n",
    "DIR_TO_VEC = {\n",
    "    0: (1, 0),   # right\n",
    "    1: (0, 1),   # down\n",
    "    2: (-1, 0),  # left\n",
    "    3: (0, -1),  # up\n",
    "}\n",
    "\n",
    "W_MAP = {\n",
    "    \"L1.2\": np.array([-0.05, -2.0, -0.01]),                       # [dist, on_lava, step]\n",
    "    \"L1.3\": np.array([-0.8, -0.1, -5.0, -0.05]),                  # [dist, lava_ahead, on_lava, step]\n",
    "    \"L2.1\": np.array([-0.05, -0.3, -2.0, -0.01]),                 # [dist, num_lava4, on_lava, step]\n",
    "    \"L2.3\": np.array([-0.05, -0.4, -2.0, -0.01]),                 # [dist, lava_nearby8, on_lava, step]\n",
    "    \"L3.1\": np.array([-0.02, -0.02, -0.5, -2.0, -0.01]),          # [dx, dy, lava_ahead, on_lava, step]\n",
    "    \"L4.1\": np.array([-0.05,  0.10, -2.0, -0.01]),                # [dist, min_dist_lava, on_lava, step]\n",
    "}\n",
    "\n",
    "FEATURE_SET = \"L1.3\"\n",
    "\n",
    "def manhattan(p, q):\n",
    "    return abs(p[0] - q[0]) + abs(p[1] - q[1])\n",
    "\n",
    "def lava_ahead_state(lava_mask: np.ndarray, y: int, x: int, direction: int) -> int:\n",
    "    dx, dy = DIR_TO_VEC[direction]\n",
    "    ny, nx = y + dy, x + dx\n",
    "    if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "        return int(lava_mask[ny, nx])\n",
    "    return 0\n",
    "\n",
    "def on_lava_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    return int(lava_mask[y, x])\n",
    "\n",
    "def num_lava_4_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    cnt = 0\n",
    "    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "        ny, nx = y+dy, x+dx\n",
    "        if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "            cnt += int(lava_mask[ny, nx])\n",
    "    return cnt\n",
    "\n",
    "def lava_nearby_8_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    for dy in [-1,0,1]:\n",
    "        for dx in [-1,0,1]:\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            ny, nx = y+dy, x+dx\n",
    "            if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "                if lava_mask[ny, nx]:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "def min_dist_to_lava_state(lava_cells: np.ndarray, y: int, x: int, size: int) -> float:\n",
    "    if lava_cells.size == 0:\n",
    "        return float(2 * size)\n",
    "    return float(np.min(np.abs(lava_cells[:, 0] - y) + np.abs(lava_cells[:, 1] - x)))\n",
    "\n",
    "def phi_from_state(state, goal_yx, lava_mask, lava_cells, size):\n",
    "    \"\"\"\n",
    "    state = (y, x, dir)\n",
    "    goal_yx = (gy, gx)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "    gy, gx = goal_yx\n",
    "\n",
    "    dist = manhattan((y, x), (gy, gx))\n",
    "    step = 1.0\n",
    "\n",
    "    if FEATURE_SET == \"L1.2\":\n",
    "        return np.array([dist, on_lava_state(lava_mask, y, x), step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L1.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.1\":\n",
    "        return np.array([dist,\n",
    "                         num_lava_4_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_nearby_8_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L3.1\":\n",
    "        dx = gx - x\n",
    "        dy = gy - y\n",
    "        return np.array([dx, dy,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L4.1\":\n",
    "        return np.array([dist,\n",
    "                         min_dist_to_lava_state(lava_cells, y, x, size),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    raise ValueError(f\"Unknown FEATURE_SET: {FEATURE_SET}\")\n",
    "\n",
    "def reward_from_state(state, goal_yx, lava_mask, lava_cells, size, w=None) -> float:\n",
    "    \"\"\"\n",
    "    State-based reward for manual debugging/printing.\n",
    "    If w is None, uses normalized W_MAP[FEATURE_SET].\n",
    "    \"\"\"\n",
    "    phi = phi_from_state(state, goal_yx, lava_mask, lava_cells, size)\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    else:\n",
    "        w = l2_normalize(w)\n",
    "    return float(w @ phi)\n",
    "\n",
    "# ======================================================\n",
    "# Planning dynamics model (left/right/forward)\n",
    "# ======================================================\n",
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "\n",
    "# ======================================================\n",
    "# SimpleEnv (same as your previous; minimal here)\n",
    "# ======================================================\n",
    "mission_space = MissionSpace(mission_func=lambda: \"reach the goal\")\n",
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            width=size,\n",
    "            height=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        from minigrid.core.grid import Grid\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Example layout: you can replace with yours\n",
    "        # Put goal at bottom-right inside the walls\n",
    "        self.put_obj(Goal(), width-2, height-2)\n",
    "\n",
    "        # Optional: lava example\n",
    "        \n",
    "        self.put_obj(Lava(), 2, 2)\n",
    "        self.put_obj(Lava(), 2, 1)\n",
    "        \n",
    "        self.agent_pos = self.agent_start_pos\n",
    "        self.agent_dir = self.agent_start_dir\n",
    "        self.mission = \"reach the goal\"\n",
    "\n",
    "# ======================================================\n",
    "# Static map extraction\n",
    "# ======================================================\n",
    "def build_static_maps(env: SimpleEnv):\n",
    "    \"\"\"\n",
    "    Returns everything in NumPy (y,x) convention.\n",
    "    \"\"\"\n",
    "    size = env.width\n",
    "    wall_mask = np.zeros((size, size), dtype=bool)\n",
    "    lava_mask = np.zeros((size, size), dtype=bool)\n",
    "    goal_yx = None\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            obj = env.grid.get(x, y)  # MiniGrid uses (x,y)\n",
    "            if obj is None:\n",
    "                continue\n",
    "            if isinstance(obj, Wall):\n",
    "                wall_mask[y, x] = True\n",
    "            elif isinstance(obj, Lava):\n",
    "                lava_mask[y, x] = True\n",
    "            elif isinstance(obj, Goal):\n",
    "                goal_yx = (y, x)\n",
    "\n",
    "    if goal_yx is None:\n",
    "        raise RuntimeError(\"Goal not found in grid\")\n",
    "\n",
    "    lava_cells = np.argwhere(lava_mask)\n",
    "    return size, wall_mask, lava_mask, lava_cells, goal_yx\n",
    "\n",
    "# def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "#     y, x, _ = state\n",
    "#     return (y, x) == goal_yx\n",
    "\n",
    "def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "    y, x, _ = state\n",
    "    return (y, x) == goal_yx or lava_mask[y, x]\n",
    "\n",
    "def step_model(state, action, wall_mask, goal_yx, lava_mask):\n",
    "    \"\"\"\n",
    "    Deterministic transition for planning.\n",
    "    state = (y, x, dir)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "\n",
    "    if is_terminal_state(state, goal_yx, lava_mask):\n",
    "        return state, True\n",
    "\n",
    "    if action == ACT_LEFT:\n",
    "        nstate = (y, x, (direction - 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_RIGHT:\n",
    "        nstate = (y, x, (direction + 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_FORWARD:\n",
    "        dx, dy = DIR_TO_VEC[direction]\n",
    "        ny, nx = y + dy, x + dx\n",
    "\n",
    "        # bounds/wall => no move\n",
    "        if ny < 0 or ny >= wall_mask.shape[0] or nx < 0 or nx >= wall_mask.shape[1]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        if wall_mask[ny, nx]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        nstate = (ny, nx, direction)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "def enumerate_states(size, wall_mask):\n",
    "    \"\"\"\n",
    "    All non-wall cells for all 4 directions.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if wall_mask[y, x]:\n",
    "                continue\n",
    "            for d in range(4):\n",
    "                states.append((y, x, d))\n",
    "    return states\n",
    "\n",
    "def build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Tabular MDP: deterministic T, features Phi(s).\n",
    "    \"\"\"\n",
    "    S = len(states)\n",
    "    A = len(ACTIONS)\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "    terminal_mask = np.zeros(S, dtype=bool)\n",
    "    Phi = np.zeros((S, len(W_MAP[FEATURE_SET])), dtype=float)\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        terminal_mask[i] = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        Phi[i] = phi_from_state(s, goal_yx, lava_mask, lava_cells, size)\n",
    "        for a_idx, a in enumerate(ACTIONS):\n",
    "            sp, _done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            T[i, a_idx, idx_of[sp]] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"idx_of\": idx_of,\n",
    "        \"T\": T,\n",
    "        \"Phi\": Phi,\n",
    "        \"terminal\": terminal_mask,\n",
    "        \"gamma\": gamma,\n",
    "        \"goal_yx\": goal_yx,\n",
    "        \"lava_mask\": lava_mask,\n",
    "        \"wall_mask\": wall_mask,\n",
    "        \"lava_cells\": lava_cells,\n",
    "        \"size\": size,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# ONE CONSISTENT next-state Policy Eval + Value Iteration\n",
    "# ======================================================\n",
    "\n",
    "def policy_evaluation_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a fixed policy with NEXT-state reward:\n",
    "      V(s) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "    Terminal states are kept at V=0 (consistent with your VI done-cutoff).\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)  # 1 if nonterminal, 0 if terminal\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "            a = int(policy[s])\n",
    "            v_new = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def value_iteration_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    "):\n",
    "    \"\"\"\n",
    "    NEXT-state reward value iteration:\n",
    "      Q(s,a) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "      V(s) = max_a Q(s,a)\n",
    "    Terminal states fixed at V=0.\n",
    "    Returns: V, Q, pi\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "    Q = np.zeros((S, A), dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            # compute Q(s,a) for all a\n",
    "            for a in range(A):\n",
    "                Q[s, a] = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "\n",
    "            v_new = float(np.max(Q[s]))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # greedy policy\n",
    "    pi = np.zeros(S, dtype=int)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            pi[s] = ACT_FORWARD\n",
    "        else:\n",
    "            pi[s] = int(np.argmax(Q[s]))\n",
    "\n",
    "    return V, Q, pi\n",
    "\n",
    "def compute_successor_features_from_q_next_state(\n",
    "    T: np.ndarray,\n",
    "    Phi: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    tol: float = 1e-10,\n",
    "    max_iters: int = 100000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Successor Features with NEXT-STATE (entering) convention, consistent with your code.\n",
    "\n",
    "    Definitions:\n",
    "      π(s)      = argmax_a Q(s,a)\n",
    "      ψ(s)      = E_π [ sum_t γ^t φ(s_{t+1}) | s0 = s ]\n",
    "      ψ(s,a)    = E [ φ(s1) + γ ψ(s1) | s0=s, a0=a ]\n",
    "\n",
    "    Bellman equation:\n",
    "      ψ(s) = Σ_{s'} P_π(s,s') [ φ(s') + γ * 1[~terminal(s')] * ψ(s') ]\n",
    "\n",
    "    Inputs:\n",
    "      T             : (S,A,S) transition matrix\n",
    "      Phi           : (S,D) state feature matrix (φ(s))\n",
    "      Q             : (S,A) Q-values (used to extract greedy policy)\n",
    "      terminal_mask : (S,) boolean\n",
    "      gamma         : discount factor\n",
    "\n",
    "    Returns:\n",
    "      Psi_sa : (S,A,D) successor features for state-action\n",
    "      Psi_s  : (S,D)   successor features for state\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    D = Phi.shape[1]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Greedy policy from Q\n",
    "    # -----------------------------\n",
    "    Pi = np.zeros((S, A), dtype=float)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            continue\n",
    "        Pi[s, np.argmax(Q[s])] = 1.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Policy transition matrix\n",
    "    # P_pi[s,s'] = Σ_a π(a|s) T[s,a,s']\n",
    "    # -----------------------------\n",
    "    P_pi = np.zeros((S, S), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            if Pi[s, a] > 0:\n",
    "                P_pi[s] += Pi[s, a] * T[s, a]\n",
    "\n",
    "        # absorbing fallback (safety)\n",
    "        if P_pi[s].sum() == 0:\n",
    "            P_pi[s, s] = 1.0\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Iterative policy SFs ψ(s)\n",
    "    # -----------------------------\n",
    "    Psi_s = np.zeros((S, D), dtype=float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        Psi_old = Psi_s.copy()\n",
    "\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            exp_phi_next = P_pi[s] @ Phi\n",
    "            exp_psi_next = P_pi[s] @ Psi_old\n",
    "\n",
    "            Psi_s[s] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "        if np.max(np.abs(Psi_s - Psi_old)) < tol:\n",
    "            break\n",
    "\n",
    "    # -----------------------------\n",
    "    # State–action successor features ψ(s,a)\n",
    "    # -----------------------------\n",
    "    Psi_sa = np.zeros((S, A, D), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            p_next = T[s, a]\n",
    "            exp_phi_next = p_next @ Phi\n",
    "            exp_psi_next = p_next @ Psi_s\n",
    "            Psi_sa[s, a] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "    return Psi_sa, Psi_s\n",
    "\n",
    "\n",
    "# Convenience wrapper if you still want \"state-based VI\" signature:\n",
    "def value_iteration_from_env_next_state(\n",
    "    states,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    lava_cells,\n",
    "    size,\n",
    "    gamma=0.99,\n",
    "    theta=1e-8,\n",
    "    max_iters=20000,\n",
    "    w=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds T and r_next from the env step model, then calls the unified tabular VI.\n",
    "    Keeps your original external interface for minimal disruption.\n",
    "    \"\"\"\n",
    "    mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=gamma)\n",
    "    # reward on NEXT state: r_next[s'] = R(s') (state reward evaluated at s')\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    r_next = mdp[\"Phi\"] @ l2_normalize(w)\n",
    "    V, Q, pi = value_iteration_next_state(mdp[\"T\"], r_next, mdp[\"terminal\"], gamma, theta=theta, max_iters=max_iters)\n",
    "    return V, Q, pi, mdp[\"idx_of\"], mdp\n",
    "\n",
    "# ======================================================\n",
    "# Demos\n",
    "# ======================================================\n",
    "def generate_state_action_demos(states, pi, terminal_mask):\n",
    "    demos = []\n",
    "    for i, _s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "        demos.append((i, int(pi[i])))\n",
    "    return demos\n",
    "\n",
    "# ======================================================\n",
    "# BIRL (demo-only, MCMC over theta)\n",
    "# ======================================================\n",
    "class DemoOnlyBIRL:\n",
    "    def __init__(self, T, Phi, gamma, terminal_mask, demos, beta=10.0):\n",
    "        self.T = T\n",
    "        self.Phi = Phi\n",
    "        self.gamma = gamma\n",
    "        self.terminal_mask = terminal_mask\n",
    "        self.demos = demos\n",
    "        self.beta = beta\n",
    "        self.S, self.A, _ = T.shape\n",
    "        self.D = Phi.shape[1]\n",
    "\n",
    "    def _q_values(self, theta):\n",
    "        \"\"\"\n",
    "        Uses the ONE unified value iteration with NEXT-state reward.\n",
    "        Interprets r_next[s'] = Phi[s'] @ theta  (reward of next state).\n",
    "        \"\"\"\n",
    "        theta = l2_normalize(theta)\n",
    "        r_next = self.Phi @ theta\n",
    "        V, Q, _pi = value_iteration_next_state(\n",
    "            self.T, r_next, self.terminal_mask, self.gamma, theta=1e-8, max_iters=200000\n",
    "        )\n",
    "        return Q\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        Q = self._q_values(theta)\n",
    "\n",
    "        ll = 0.0\n",
    "        for s, a in self.demos:\n",
    "            logits = self.beta * Q[s]\n",
    "            ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=200, step_size=0.3, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        theta = l2_normalize(rng.normal(0, 1, self.D))\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            prop = l2_normalize(theta + rng.normal(0, step_size, self.D))\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "class TrajectoryPreferenceBIRL:\n",
    "    \"\"\"\n",
    "    BIRL over trajectory preferences:\n",
    "      (tau_pos, tau_neg)\n",
    "    Works for:\n",
    "      - pairwise preferences\n",
    "      - correction feedback (same start state)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mdp, preferences, beta=5.0):\n",
    "        self.mdp = mdp\n",
    "        self.preferences = preferences\n",
    "        self.beta = beta\n",
    "        self.D = mdp[\"Phi\"].shape[1]\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        ll = 0.0\n",
    "\n",
    "        for tau_pos, tau_neg in self.preferences:\n",
    "            r_pos = trajectory_return(tau_pos, self.mdp, theta)\n",
    "            r_neg = trajectory_return(tau_neg, self.mdp, theta)\n",
    "\n",
    "            diff = r_pos - r_neg\n",
    "            ll += self.beta * diff - np.log1p(np.exp(self.beta * diff))\n",
    "\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=1000, step_size=0.25, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        theta = l2_normalize(rng.normal(0, 1, self.D))\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            prop = l2_normalize(theta + rng.normal(0, step_size, self.D))\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "# ======================================================\n",
    "# Evaluation (EVD) using unified policy eval\n",
    "# ======================================================\n",
    "def expected_value_difference(T, gamma, r_next_true, pi_true, pi_eval, terminal_mask):\n",
    "    \"\"\"\n",
    "    EVD with NEXT-state reward, consistent everywhere:\n",
    "      EVD = mean(V^pi_true) - mean(V^pi_eval)\n",
    "    \"\"\"\n",
    "    V_opt = policy_evaluation_next_state(T, r_next_true, pi_true, terminal_mask, gamma)\n",
    "    V_ev  = policy_evaluation_next_state(T, r_next_true, pi_eval, terminal_mask, gamma)\n",
    "    return float(np.mean(V_opt) - np.mean(V_ev))\n",
    "\n",
    "# ======================================================\n",
    "# Debug printing (state-based)\n",
    "# ======================================================\n",
    "def debug_print_values_and_demos_from_tabular(\n",
    "    states,\n",
    "    V,\n",
    "    Q,\n",
    "    pi,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "):\n",
    "    print(\"\\n===== DEBUG: V(s), Q(s,a), DEMOS =====\")\n",
    "    for i, s in enumerate(states):\n",
    "        y, x, d = s\n",
    "\n",
    "        if wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        term = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        term_str = \"TERMINAL\" if term else \"\"\n",
    "\n",
    "        print(f\"\\nState {i:03d} (y={y}, x={x}, d={d}) [{cell_type}] {term_str}\")\n",
    "        print(f\"  V(s) = {V[i]: .6f}\")\n",
    "\n",
    "        if term:\n",
    "            print(\"  (terminal state, no actions)\")\n",
    "            continue\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            print(f\"  Q(s,a={a}) = {Q[i, a]: .6f}\")\n",
    "\n",
    "        print(f\"  OPTIMAL ACTION (demo) = {pi[i]}\")\n",
    "\n",
    "# ======================================================\n",
    "# Optional manual control (no SymbolicObsWrapper)\n",
    "# ======================================================\n",
    "class ManualControlStateReward(ManualControl):\n",
    "    def __init__(self, env, wall_mask, lava_mask, lava_cells, goal_yx, size):\n",
    "        super().__init__(env)\n",
    "        self.wall_mask = wall_mask\n",
    "        self.lava_mask = lava_mask\n",
    "        self.lava_cells = lava_cells\n",
    "        self.goal_yx = goal_yx\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, action):\n",
    "        _obs, _rew, terminated, truncated, _info = self.env.step(action)\n",
    "\n",
    "        x, y = self.env.unwrapped.agent_pos  # env uses (x,y)\n",
    "        d = int(self.env.unwrapped.agent_dir)\n",
    "        s = (y, x, d)\n",
    "\n",
    "        if self.wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == self.goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif self.lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        phi = phi_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "        R = reward_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "\n",
    "        print(f\"STATE {s} [{cell_type}]  φ(s)={phi}  R(s)={R:.4f}\")\n",
    "\n",
    "        return _obs, 0.0, terminated, truncated, _info\n",
    "\n",
    "def constraints_from_demos_next_state(\n",
    "    demos,\n",
    "    Psi_sa,\n",
    "    terminal_mask=None,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds linear reward constraints from demos using successor features.\n",
    "\n",
    "    Each constraint is:\n",
    "        (ψ(s,a*) - ψ(s,a)) · θ >= 0     for all a != a*\n",
    "\n",
    "    Inputs:\n",
    "      demos         : list of (s, a_star) pairs (state index, optimal action)\n",
    "      Psi_sa        : (S, A, D) successor features (NEXT-state convention)\n",
    "      terminal_mask : optional (S,) boolean mask\n",
    "      normalize     : L2-normalize constraint vectors\n",
    "      tol           : skip near-zero constraints\n",
    "\n",
    "    Returns:\n",
    "      constraints : list of constraint vectors v ∈ R^D\n",
    "    \"\"\"\n",
    "    Psi_sa = np.asarray(Psi_sa)\n",
    "    S, A, D = Psi_sa.shape\n",
    "    constraints = []\n",
    "\n",
    "    if demos is None:\n",
    "        return constraints\n",
    "\n",
    "    for s, a_star in demos:\n",
    "        if s is None or a_star is None:\n",
    "            continue\n",
    "\n",
    "        s = int(s)\n",
    "        a_star = int(a_star)\n",
    "\n",
    "        if not (0 <= s < S) or not (0 <= a_star < A):\n",
    "            continue\n",
    "\n",
    "        if terminal_mask is not None and terminal_mask[s]:\n",
    "            continue\n",
    "\n",
    "        psi_star = Psi_sa[s, a_star]\n",
    "\n",
    "        for a in range(A):\n",
    "            if a == a_star:\n",
    "                continue\n",
    "\n",
    "            diff = psi_star - Psi_sa[s, a]\n",
    "            norm = np.linalg.norm(diff)\n",
    "\n",
    "            if norm <= tol:\n",
    "                continue\n",
    "\n",
    "            constraints.append(diff / norm if normalize else diff)\n",
    "\n",
    "    return constraints\n",
    "\n",
    "def trajectory_feature_sum(traj, mdp):\n",
    "    D = mdp[\"Phi\"].shape[1]\n",
    "    out = np.zeros(D)\n",
    "    g = 1.0\n",
    "\n",
    "    for (_s, _a, sp) in traj:\n",
    "        sp_idx = mdp[\"idx_of\"][sp]\n",
    "        out += g * mdp[\"Phi\"][sp_idx]\n",
    "        g *= mdp[\"gamma\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "def constraints_from_trajectory_preferences(\n",
    "    preferences,\n",
    "    mdp,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "):\n",
    "    constraints = []\n",
    "\n",
    "    for tau_pos, tau_neg in preferences:\n",
    "        phi_pos = trajectory_feature_sum(tau_pos, mdp)\n",
    "        phi_neg = trajectory_feature_sum(tau_neg, mdp)\n",
    "\n",
    "        diff = phi_pos - phi_neg\n",
    "        norm = np.linalg.norm(diff)\n",
    "\n",
    "        if norm <= tol:\n",
    "            continue\n",
    "\n",
    "        constraints.append(diff / norm if normalize else diff)\n",
    "\n",
    "    return constraints\n",
    "\n",
    "# ======================================================\n",
    "# Main (BIRL experiment)\n",
    "# ======================================================\n",
    "#def birl_main():\n",
    "planning_env = SimpleEnv(render_mode=None)\n",
    "planning_env.reset(seed=0)\n",
    "\n",
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(planning_env)\n",
    "states = enumerate_states(size, wall_mask)\n",
    "\n",
    "# Build tabular MDP once\n",
    "mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99)\n",
    "\n",
    "# TRUE reward as NEXT-state reward vector over state indices\n",
    "theta_true = l2_normalize(W_MAP[FEATURE_SET])\n",
    "r_next_true = mdp[\"Phi\"] @ theta_true\n",
    "\n",
    "# TRUE optimal policy via unified VI\n",
    "V_true, Q_true, pi_true = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_next_true, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    ")\n",
    "\n",
    "Psi_sa, Psi_s = compute_successor_features_from_q_next_state(\n",
    "    mdp[\"T\"],\n",
    "    mdp[\"Phi\"],\n",
    "    Q_true,\n",
    "    mdp[\"terminal\"],\n",
    "    mdp[\"gamma\"],\n",
    ")\n",
    "\n",
    "print(\"\\n===== SUCCESSOR FEATURES DEBUG =====\")\n",
    "for i, s in enumerate(states):\n",
    "    if mdp[\"terminal\"][i]:\n",
    "        continue\n",
    "    print(f\"State {i:03d} {s}\")\n",
    "    print(f\"  ψ(s)        = {Psi_s[i]}\")\n",
    "    print(f\"  ψ(s)·θ      = {Psi_s[i] @ theta_true:.6f}\")\n",
    "    print(f\"  V_true(s)   = {V_true[i]:.6f}\")\n",
    "\n",
    "# demos\n",
    "demos = generate_state_action_demos(states, pi_true, mdp[\"terminal\"])\n",
    "\n",
    "\n",
    "# Optional manual control\n",
    "# env = SimpleEnv(render_mode=\"human\")\n",
    "# env.reset(seed=0)\n",
    "# manual = ManualControlStateReward(env, wall_mask, lava_mask, lava_cells, goal_yx, size)\n",
    "# manual.start()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     birl_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c90eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee617b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CONSTRAINTS DEBUG =====\n",
      "Num constraints: 47\n",
      "Dim: (4,)\n",
      "\n",
      "===== CONSTRAINT PRUNING =====\n",
      "Original constraints : 47\n",
      "After pruning        : 5\n",
      "\n",
      "===== BIRL RESULTS (SYMBOLIC-FREE) =====\n",
      "FEATURE_SET : L1.3\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "Learned θ   : [-0.54382213 -0.62544988 -0.52946151  0.18094322]\n",
      "EVD         : 0.0013839260312477064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2379795/2256351287.py:548: RuntimeWarning: overflow encountered in exp\n",
      "  ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SCOT + BIRL RESULTS =====\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "SCOT θ̂     : [-0.2145758  -0.45881618 -0.85507385  0.11087675]\n",
      "Num demos   : 24\n",
      "SCOT demos  : 13\n",
      "SCOT EVD    : 0.0\n",
      "\n",
      "===== RANDOM BASELINE RESULTS =====\n",
      "Random EVD mean : 0.6524653538893673\n",
      "Random EVD std  : 2.54457487978724\n"
     ]
    }
   ],
   "source": [
    "constraints = constraints_from_demos_next_state(\n",
    "demos=demos,\n",
    "Psi_sa=Psi_sa,\n",
    "terminal_mask=mdp[\"terminal\"],\n",
    "normalize=True,\n",
    ")\n",
    "print(\"\\n===== CONSTRAINTS DEBUG =====\")\n",
    "print(\"Num constraints:\", len(constraints))\n",
    "print(\"Dim:\", constraints[0].shape if constraints else None)\n",
    "constraints_pruned = remove_redundant_constraints(\n",
    "constraints,\n",
    "epsilon=1e-4,\n",
    ")\n",
    "\n",
    "# for i, c in enumerate(constraints[:5]):\n",
    "#     print(f\"c[{i}] = {c}\")\n",
    "\n",
    "print(\"\\n===== CONSTRAINT PRUNING =====\")\n",
    "print(\"Original constraints :\", len(constraints))\n",
    "print(\"After pruning        :\", len(constraints_pruned))\n",
    "\n",
    "\n",
    "# BIRL\n",
    "birl = DemoOnlyBIRL(mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], demos, beta=1.0)\n",
    "theta_hat = birl.run(num_samples=1000, step_size=0.25, seed=0)\n",
    "\n",
    "# LEARNED policy via the SAME unified VI\n",
    "r_next_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "V_hat, Q_hat, pi_hat = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_next_hat, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    ")\n",
    "\n",
    "# EVD (policy eval uses unified next-state policy eval)\n",
    "evd = expected_value_difference(mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_hat, mdp[\"terminal\"])\n",
    "\n",
    "print(\"\\n===== BIRL RESULTS (SYMBOLIC-FREE) =====\")\n",
    "print(\"FEATURE_SET :\", FEATURE_SET)\n",
    "print(\"True θ      :\", theta_true)\n",
    "print(\"Learned θ   :\", l2_normalize(theta_hat))\n",
    "print(\"EVD         :\", evd)\n",
    "\n",
    "# Debug: print V/Q/demos from the SAME VI output\n",
    "# debug_print_values_and_demos_from_tabular(\n",
    "#     states=states,\n",
    "#     V=V_true,\n",
    "#     Q=Q_true,\n",
    "#     pi=pi_true,\n",
    "#     wall_mask=wall_mask,\n",
    "#     goal_yx=goal_yx,\n",
    "#     lava_mask=lava_mask,\n",
    "# )\n",
    "\n",
    "demos = [(i, pi_true[i]) for i in range(len(states)) if not mdp[\"terminal\"][i]]\n",
    "\n",
    "constraints_per_atom = []\n",
    "for s, a in demos:\n",
    "    psi_star = Psi_sa[s, a]\n",
    "    atom_constraints = []\n",
    "    for a2 in range(Psi_sa.shape[1]):\n",
    "        if a2 != a:\n",
    "            diff = psi_star - Psi_sa[s, a2]\n",
    "            if np.linalg.norm(diff) > 1e-12:\n",
    "                atom_constraints.append(diff)\n",
    "    constraints_per_atom.append(atom_constraints)\n",
    "\n",
    "scot_demos, _ = scot_greedy_single_env(atoms=demos, constraints_per_atom=constraints_per_atom)\n",
    "\n",
    "birl_scot = DemoOnlyBIRL(\n",
    "    mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], scot_demos\n",
    ")\n",
    "theta_scot = birl_scot.run()\n",
    "\n",
    "r_scot = mdp[\"Phi\"] @ l2_normalize(theta_scot)\n",
    "_, _, pi_scot = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_scot, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    ")\n",
    "\n",
    "evd_scot = expected_value_difference(\n",
    "    mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_scot, mdp[\"terminal\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== SCOT + BIRL RESULTS =====\")\n",
    "print(\"True θ      :\", theta_true)\n",
    "print(\"SCOT θ̂     :\", l2_normalize(theta_scot))\n",
    "print(\"Num demos   :\", len(demos))\n",
    "print(\"SCOT demos  :\", len(scot_demos))\n",
    "print(\"SCOT EVD    :\", evd_scot)\n",
    "\n",
    "def evd_from_demos(\n",
    "    demos,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    birl_kwargs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a demo set:\n",
    "      demos = [(s, a), ...]\n",
    "    learn reward via DemoOnlyBIRL, derive policy, compute EVD.\n",
    "    \"\"\"\n",
    "    if birl_kwargs is None:\n",
    "        birl_kwargs = {}\n",
    "\n",
    "    birl = DemoOnlyBIRL(\n",
    "        mdp[\"T\"],\n",
    "        mdp[\"Phi\"],\n",
    "        mdp[\"gamma\"],\n",
    "        mdp[\"terminal\"],\n",
    "        demos,\n",
    "        **birl_kwargs,\n",
    "    )\n",
    "\n",
    "    theta_hat = birl.run()\n",
    "\n",
    "    r_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "    _, _, pi_hat = value_iteration_next_state(\n",
    "        mdp[\"T\"],\n",
    "        r_hat,\n",
    "        mdp[\"terminal\"],\n",
    "        mdp[\"gamma\"],\n",
    "    )\n",
    "\n",
    "    evd = expected_value_difference(\n",
    "        mdp[\"T\"],\n",
    "        mdp[\"gamma\"],\n",
    "        mdp[\"Phi\"] @ theta_true,\n",
    "        pi_true,\n",
    "        pi_hat,\n",
    "        mdp[\"terminal\"],\n",
    "    )\n",
    "\n",
    "    return evd\n",
    "\n",
    "\n",
    "def random_demo_baseline_evd(\n",
    "    full_demos,\n",
    "    num_select,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    "    birl_kwargs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly select `num_select` demos from full_demos,\n",
    "    repeat `num_trials` times, return mean/std EVD.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    evds = []\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        sel = rng.choice(\n",
    "            len(full_demos),\n",
    "            size=num_select,\n",
    "            replace=False,\n",
    "        )\n",
    "        demos_rand = [full_demos[i] for i in sel]\n",
    "\n",
    "        evd = evd_from_demos(\n",
    "            demos_rand,\n",
    "            mdp,\n",
    "            theta_true,\n",
    "            pi_true,\n",
    "            birl_kwargs=birl_kwargs,\n",
    "        )\n",
    "        evds.append(evd)\n",
    "\n",
    "    evds = np.asarray(evds)\n",
    "    return {\n",
    "        \"mean\": float(evds.mean()),\n",
    "        \"std\": float(evds.std()),\n",
    "        \"all\": evds,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# RANDOM BASELINE (same demo budget as SCOT)\n",
    "# ======================================================\n",
    "\n",
    "rand_stats = random_demo_baseline_evd(\n",
    "    full_demos=demos,                 # all possible demos\n",
    "    num_select=len(scot_demos),        # SAME budget\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    pi_true=pi_true,\n",
    "    num_trials=50,                     # repeat\n",
    "    seed=0,\n",
    "    birl_kwargs={\"beta\": 1.0},\n",
    ")\n",
    "\n",
    "print(\"\\n===== RANDOM BASELINE RESULTS =====\")\n",
    "print(\"Random EVD mean :\", rand_stats[\"mean\"])\n",
    "print(\"Random EVD std  :\", rand_stats[\"std\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e8788",
   "metadata": {},
   "source": [
    "### Generating pairwise preference and correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_random_trajectory(\n",
    "    start_state,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    max_horizon=30,\n",
    "    rng=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Roll out a random trajectory from a fixed start state.\n",
    "    Returns list of (s, a, s_next).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    traj = []\n",
    "    s = start_state\n",
    "\n",
    "    for _ in range(max_horizon):\n",
    "        if is_terminal_state(s, goal_yx, lava_mask):\n",
    "            break\n",
    "\n",
    "        a = rng.choice(ACTIONS)\n",
    "        sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "\n",
    "        traj.append((s, a, sp))\n",
    "        s = sp\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return traj\n",
    "\n",
    "def generate_random_trajectories_from_state(\n",
    "    start_state,\n",
    "    n_trajs,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    max_horizon=30,\n",
    "    seed=0,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return [\n",
    "        rollout_random_trajectory(\n",
    "            start_state,\n",
    "            wall_mask,\n",
    "            goal_yx,\n",
    "            lava_mask,\n",
    "            max_horizon=max_horizon,\n",
    "            rng=rng,\n",
    "        )\n",
    "        for _ in range(n_trajs)\n",
    "    ]\n",
    "\n",
    "def generate_trajectory_pool(\n",
    "    states,\n",
    "    terminal_mask,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    n_trajs_per_state=5,\n",
    "    max_horizon=30,\n",
    "):\n",
    "    pool = []\n",
    "    for i, s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "\n",
    "        trajs = generate_random_trajectories_from_state(\n",
    "            start_state=s,\n",
    "            n_trajs=n_trajs_per_state,\n",
    "            wall_mask=wall_mask,\n",
    "            goal_yx=goal_yx,\n",
    "            lava_mask=lava_mask,\n",
    "            max_horizon=max_horizon,\n",
    "            seed=i,\n",
    "        )\n",
    "        pool.extend(trajs)\n",
    "\n",
    "    return pool\n",
    "\n",
    "def trajectory_return(\n",
    "    traj,\n",
    "    Phi,\n",
    "    theta,\n",
    "    gamma=0.99,\n",
    "):\n",
    "    \"\"\"\n",
    "    traj: list of (s, a, s_next)\n",
    "    \"\"\"\n",
    "    theta = l2_normalize(theta)\n",
    "    ret = 0.0\n",
    "    g = 1.0\n",
    "\n",
    "    for (_s, _a, sp) in traj:\n",
    "        sp_idx = Phi[\"idx_of\"][sp]\n",
    "        r = Phi[\"Phi\"][sp_idx] @ theta\n",
    "        ret += g * r\n",
    "        g *= gamma\n",
    "\n",
    "    return ret\n",
    "\n",
    "def generate_pairwise_preferences(\n",
    "    trajectories,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    gamma=0.99,\n",
    "    n_pairs=1000,\n",
    "    seed=0,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    prefs = []\n",
    "\n",
    "    returns = [\n",
    "        trajectory_return(traj, mdp, theta_true, gamma)\n",
    "        for traj in trajectories\n",
    "    ]\n",
    "\n",
    "    N = len(trajectories)\n",
    "\n",
    "    for _ in range(n_pairs):\n",
    "        i, j = rng.choice(N, size=2, replace=False)\n",
    "\n",
    "        if returns[i] == returns[j]:\n",
    "            continue\n",
    "\n",
    "        if returns[i] > returns[j]:\n",
    "            prefs.append((trajectories[i], trajectories[j]))\n",
    "        else:\n",
    "            prefs.append((trajectories[j], trajectories[i]))\n",
    "\n",
    "    return prefs\n",
    "\n",
    "def simulate_correction_one(\n",
    "    traj,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    num_random_trajs=10,\n",
    "    max_horizon=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given an existing trajectory τ, attempt to find a better trajectory\n",
    "    starting from the SAME start state.\n",
    "\n",
    "    Returns:\n",
    "        (tau_improved, tau_original)\n",
    "        or None if no improvement found\n",
    "    \"\"\"\n",
    "    start_state = traj[0][0]\n",
    "\n",
    "    # use original length unless overridden\n",
    "    horizon = len(traj) if max_horizon is None else max_horizon\n",
    "\n",
    "    original_return = trajectory_return(traj, mdp, theta_true)\n",
    "    best_traj = traj\n",
    "    best_return = original_return\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for _ in range(num_random_trajs):\n",
    "        new_traj = rollout_random_trajectory(\n",
    "            start_state=start_state,\n",
    "            wall_mask=mdp[\"wall_mask\"],\n",
    "            goal_yx=mdp[\"goal_yx\"],\n",
    "            lava_mask=mdp[\"lava_mask\"],\n",
    "            max_horizon=horizon,\n",
    "            rng=rng,\n",
    "        )\n",
    "\n",
    "        if len(new_traj) == 0:\n",
    "            continue\n",
    "\n",
    "        new_return = trajectory_return(new_traj, mdp, theta_true)\n",
    "\n",
    "        if new_return > best_return:\n",
    "            best_return = new_return\n",
    "            best_traj = new_traj\n",
    "\n",
    "    if best_traj is traj:\n",
    "        return None  # no improvement found\n",
    "\n",
    "    return (best_traj, traj)\n",
    "\n",
    "def generate_correction_feedback(\n",
    "    trajectories,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    num_random_trajs=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate correction (improvement) feedback:\n",
    "    (tau_improved ≻ tau_original), same start state.\n",
    "    \"\"\"\n",
    "    corrections = []\n",
    "\n",
    "    for traj in trajectories:\n",
    "        if len(traj) == 0:\n",
    "            continue\n",
    "\n",
    "        corr = simulate_correction_one(\n",
    "            traj=traj,\n",
    "            mdp=mdp,\n",
    "            theta_true=theta_true,\n",
    "            num_random_trajs=num_random_trajs,\n",
    "        )\n",
    "\n",
    "        if corr is not None:\n",
    "            corrections.append(corr)\n",
    "\n",
    "    return corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Generate random trajectory pool\n",
    "# --------------------------------------------------\n",
    "traj_pool = generate_trajectory_pool(\n",
    "    states=states,\n",
    "    terminal_mask=mdp[\"terminal\"],\n",
    "    wall_mask=mdp[\"wall_mask\"],\n",
    "    goal_yx=mdp[\"goal_yx\"],\n",
    "    lava_mask=mdp[\"lava_mask\"],\n",
    "    n_trajs_per_state=50,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Pairwise preferences\n",
    "# --------------------------------------------------\n",
    "pairwise_prefs = generate_pairwise_preferences(\n",
    "    trajectories=traj_pool,\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    n_pairs=3000,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Correction feedback\n",
    "# --------------------------------------------------\n",
    "correction_prefs = generate_correction_feedback(\n",
    "    trajectories=traj_pool,\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    num_random_trajs=20,\n",
    ")\n",
    "\n",
    "print(\"Num pairwise prefs  :\", len(pairwise_prefs))\n",
    "print(\"Num correction prefs:\", len(correction_prefs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf15032",
   "metadata": {},
   "outputs": [],
   "source": [
    "birl_pairwise = TrajectoryPreferenceBIRL(\n",
    "    mdp=mdp,\n",
    "    preferences=pairwise_prefs,\n",
    "    beta=5.0,\n",
    ")\n",
    "\n",
    "theta_pairwise = birl_pairwise.run()\n",
    "\n",
    "birl_correction = TrajectoryPreferenceBIRL(\n",
    "    mdp=mdp,\n",
    "    preferences=correction_prefs,\n",
    "    beta=5.0,\n",
    ")\n",
    "\n",
    "theta_correction = birl_correction.run()\n",
    "\n",
    "# TRUE optimal policy already computed: pi_true\n",
    "\n",
    "# Pairwise-learned policy\n",
    "r_pairwise = mdp[\"Phi\"] @ l2_normalize(theta_pairwise)\n",
    "_, _, pi_pairwise = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_pairwise, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    ")\n",
    "\n",
    "evd_pairwise = expected_value_difference(\n",
    "    mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_pairwise, mdp[\"terminal\"]\n",
    ")\n",
    "\n",
    "# Correction-learned policy\n",
    "r_correction = mdp[\"Phi\"] @ l2_normalize(theta_correction)\n",
    "_, _, pi_correction = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_correction, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    ")\n",
    "\n",
    "evd_correction = expected_value_difference(\n",
    "    mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_correction, mdp[\"terminal\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== FEEDBACK COMPARISON RESULTS =====\")\n",
    "print(\"True θ         :\", theta_true)\n",
    "print(\"Pairwise θ̂    :\", l2_normalize(theta_pairwise))\n",
    "print(\"Correction θ̂  :\", l2_normalize(theta_correction))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"EVD Pairwise   :\", evd_pairwise)\n",
    "print(\"EVD Correction :\", evd_correction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7561f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_atoms_with_constraints(\n",
    "    preferences,\n",
    "    mdp,\n",
    "    normalize=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Each preference is an atom with exactly one constraint:\n",
    "        Φ(τ⁺) − Φ(τ⁻)\n",
    "    \"\"\"\n",
    "    atoms = []\n",
    "    constraints_per_atom = []\n",
    "\n",
    "    for pref in preferences:\n",
    "        cons = constraints_from_trajectory_preferences(\n",
    "            [pref],\n",
    "            mdp,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        if len(cons) == 0:\n",
    "            continue\n",
    "\n",
    "        atoms.append(pref)\n",
    "        constraints_per_atom.append(cons)\n",
    "\n",
    "    return atoms, constraints_per_atom\n",
    "\n",
    "def scot_on_preferences(\n",
    "    preferences,\n",
    "    mdp,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs SCOT selection on trajectory preferences.\n",
    "    Returns selected subset of preferences.\n",
    "    \"\"\"\n",
    "    atoms, constraints_per_atom = preference_atoms_with_constraints(\n",
    "        preferences,\n",
    "        mdp,\n",
    "        normalize=True,\n",
    "    )\n",
    "\n",
    "    scot_atoms, _ = scot_greedy_single_env(\n",
    "        atoms=atoms,\n",
    "        constraints_per_atom=constraints_per_atom,\n",
    "    )\n",
    "\n",
    "    return scot_atoms\n",
    "\n",
    "def evd_from_preferences(\n",
    "    preferences,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    beta=5.0,\n",
    "):\n",
    "    birl = TrajectoryPreferenceBIRL(\n",
    "        mdp=mdp,\n",
    "        preferences=preferences,\n",
    "        beta=beta,\n",
    "    )\n",
    "\n",
    "    theta_hat = birl.run()\n",
    "\n",
    "    r_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "    _, _, pi_hat = value_iteration_next_state(\n",
    "        mdp[\"T\"],\n",
    "        r_hat,\n",
    "        mdp[\"terminal\"],\n",
    "        mdp[\"gamma\"],\n",
    "    )\n",
    "\n",
    "    evd = expected_value_difference(\n",
    "        mdp[\"T\"],\n",
    "        mdp[\"gamma\"],\n",
    "        mdp[\"Phi\"] @ theta_true,\n",
    "        pi_true,\n",
    "        pi_hat,\n",
    "        mdp[\"terminal\"],\n",
    "    )\n",
    "\n",
    "    return theta_hat, evd\n",
    "\n",
    "def random_preference_baseline_evd(\n",
    "    full_preferences,\n",
    "    num_select,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    "    beta=5.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly sample `num_select` preferences multiple times.\n",
    "    Returns mean/std EVD.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    evds = []\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        idx = rng.choice(\n",
    "            len(full_preferences),\n",
    "            size=num_select,\n",
    "            replace=False,\n",
    "        )\n",
    "        prefs_rand = [full_preferences[i] for i in idx]\n",
    "\n",
    "        _, evd = evd_from_preferences(\n",
    "            prefs_rand,\n",
    "            mdp,\n",
    "            theta_true,\n",
    "            pi_true,\n",
    "            beta=beta,\n",
    "        )\n",
    "        evds.append(evd)\n",
    "\n",
    "    evds = np.asarray(evds)\n",
    "    return {\n",
    "        \"mean\": float(evds.mean()),\n",
    "        \"std\": float(evds.std()),\n",
    "        \"all\": evds,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PAIRWISE PREFERENCES\n",
    "# =========================\n",
    "\n",
    "\n",
    "# SCOT selection\n",
    "pairwise_scot = scot_on_preferences(pairwise_prefs, mdp)\n",
    "\n",
    "theta_pw_scot, evd_pw_scot = evd_from_preferences(\n",
    "    pairwise_scot,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    ")\n",
    "\n",
    "# RANDOM baseline (same budget, repeated)\n",
    "rand_pw_stats = random_preference_baseline_evd(\n",
    "    full_preferences=pairwise_prefs,\n",
    "    num_select=len(pairwise_scot),\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    pi_true=pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "print(\"\\n===== PAIRWISE: SCOT vs RANDOM =====\")\n",
    "print(f\"Total pairwise prefs : {len(pairwise_prefs)}\")\n",
    "print(f\"SCOT prefs           : {len(pairwise_scot)}\")\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"SCOT EVD             : {evd_pw_scot:.6f}\")\n",
    "print(f\"Random mean EVD      : {rand_pw_stats['mean']:.6f}\")\n",
    "print(f\"Random std EVD       : {rand_pw_stats['std']:.6f}\")\n",
    "\n",
    "# =========================\n",
    "# CORRECTION FEEDBACK\n",
    "# =========================\n",
    "\n",
    "# SCOT selection\n",
    "correction_scot = scot_on_preferences(correction_prefs, mdp)\n",
    "\n",
    "theta_corr_scot, evd_corr_scot = evd_from_preferences(\n",
    "    correction_scot,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    ")\n",
    "\n",
    "# RANDOM baseline (same budget, repeated)\n",
    "rand_corr_stats = random_preference_baseline_evd(\n",
    "    full_preferences=correction_prefs,\n",
    "    num_select=len(correction_scot),\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    pi_true=pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "print(\"\\n===== CORRECTION: SCOT vs RANDOM =====\")\n",
    "print(f\"Total correction feedback : {len(correction_prefs)}\")\n",
    "print(f\"SCOT feedback             : {len(correction_scot)}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"SCOT EVD                  : {evd_corr_scot:.6f}\")\n",
    "print(f\"Random mean EVD           : {rand_corr_stats['mean']:.6f}\")\n",
    "print(f\"Random std EVD            : {rand_corr_stats['std']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c703f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_grouped_regret_from_values(\n",
    "    feedback_to_values,\n",
    "    *,\n",
    "    n_envs=None,\n",
    "    mdp_size=None,\n",
    "    random_trials=None,\n",
    "    sort_by_random=False,\n",
    "    show_values=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    feedback_to_values : dict\n",
    "        {\n",
    "            \"feedback_name\": (two_stage_mean, random_mean)\n",
    "        }\n",
    "\n",
    "    n_envs : int or None\n",
    "    mdp_size : int or None\n",
    "    random_trials : int or None\n",
    "\n",
    "    sort_by_random : bool\n",
    "        If True, sort feedback types by random mean regret (ascending).\n",
    "\n",
    "    show_values : bool\n",
    "        If True, print numeric values on top of bars.\n",
    "    \"\"\"\n",
    "\n",
    "    feedback_names = []\n",
    "    two_stage_means = []\n",
    "    random_means = []\n",
    "\n",
    "    # ---------------- Collect values ----------------\n",
    "    for feedback, (two_stage, random_val) in feedback_to_values.items():\n",
    "        feedback_names.append(feedback)\n",
    "        two_stage_means.append(float(two_stage))\n",
    "        random_means.append(float(random_val))\n",
    "\n",
    "    feedback_names = np.array(feedback_names)\n",
    "    two_stage_means = np.array(two_stage_means)\n",
    "    random_means = np.array(random_means)\n",
    "\n",
    "    # ---------------- Optional sorting ----------------\n",
    "    if sort_by_random:\n",
    "        order = np.argsort(random_means)\n",
    "        feedback_names = feedback_names[order]\n",
    "        two_stage_means = two_stage_means[order]\n",
    "        random_means = random_means[order]\n",
    "\n",
    "    # ---------------- Plot ----------------\n",
    "    x = np.arange(len(feedback_names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    bars_scot = plt.bar(\n",
    "        x - width / 2,\n",
    "        two_stage_means,\n",
    "        width,\n",
    "        label=\"Two-Stage SCOT\",\n",
    "    )\n",
    "\n",
    "    bars_random = plt.bar(\n",
    "        x + width / 2,\n",
    "        random_means,\n",
    "        width,\n",
    "        label=\"Random Selection\",\n",
    "    )\n",
    "\n",
    "    plt.xticks(x, feedback_names)\n",
    "    plt.xlabel(\"Feedback Type\")\n",
    "    plt.ylabel(\"Mean Regret\")\n",
    "\n",
    "    title_parts = [\"Mean Regret Comparison\"]\n",
    "    if n_envs is not None:\n",
    "        title_parts.append(f\"{n_envs} MDPs\")\n",
    "    if mdp_size is not None:\n",
    "        title_parts.append(f\"Grid {mdp_size}×{mdp_size}\")\n",
    "    if random_trials is not None:\n",
    "        title_parts.append(f\"Random Trials = {random_trials}\")\n",
    "\n",
    "    plt.title(\" | \".join(title_parts))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ---------------- Value labels ----------------\n",
    "    if show_values:\n",
    "        def annotate(bars):\n",
    "            for bar in bars:\n",
    "                h = bar.get_height()\n",
    "                plt.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,\n",
    "                    h,\n",
    "                    f\"{h:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=9,\n",
    "                )\n",
    "\n",
    "        annotate(bars_scot)\n",
    "        annotate(bars_random)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "feedback_to_values = {\n",
    "    \"demo\":      (0, 0.6),\n",
    "    \"pairwise\":  (0.001, 0.001),\n",
    "    \"correction\":(0.02, 0.04),\n",
    "}\n",
    "\n",
    "plot_grouped_regret_from_values(\n",
    "    feedback_to_values,\n",
    "    n_envs=1,\n",
    "    mdp_size=3,\n",
    "    #random_trials=1000,\n",
    "    sort_by_random=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b8c1d",
   "metadata": {},
   "source": [
    "### Working with env API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=\"human\")\n",
    "env.reset(seed=0)\n",
    "\n",
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(env)\n",
    "\n",
    "print(\"=== ENV INFO ===\")\n",
    "print(\"Grid size:\", size)\n",
    "print(\"Goal (y,x):\", goal_yx)\n",
    "print(\"FEATURE_SET:\", FEATURE_SET)\n",
    "print(\"Reward weights (normalized):\", l2_normalize(W_MAP[FEATURE_SET]))\n",
    "print(\"================\\n\")\n",
    "\n",
    "manual = ManualControlStateReward(\n",
    "    env=env,\n",
    "    wall_mask=wall_mask,\n",
    "    lava_mask=lava_mask,\n",
    "    lava_cells=lava_cells,\n",
    "    goal_yx=goal_yx,\n",
    "    size=size,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Controls:\")\n",
    "print(\"  LEFT  = turn left\")\n",
    "print(\"  RIGHT = turn right\")\n",
    "print(\"  UP    = move forward\")\n",
    "print(\"  ESC   = quit\\n\")\n",
    "manual.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938c2e6",
   "metadata": {},
   "source": [
    "### Generating random trajectories and pairwise preferneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c5cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d5d624a",
   "metadata": {},
   "source": [
    "### testing pairwise preference generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659b466",
   "metadata": {},
   "source": [
    "### Correction feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6042cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c772f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
