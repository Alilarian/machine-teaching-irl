{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6ccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1717256/542500855.py:676: RuntimeWarning: overflow encountered in exp\n",
      "  ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n",
      "/tmp/ipykernel_1717256/542500855.py:691: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
      "/tmp/ipykernel_1717256/542500855.py:676: RuntimeWarning: divide by zero encountered in log\n",
      "  ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BIRL RESULTS =====\n",
      "True θ     : [-0.05 -0.5  -2.   -0.01]\n",
      "Learned θ  : [-0.31953372 -0.69286049  0.43314024 -0.75588681]\n",
      "EVD        : 13.872706296171962\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.wrappers import SymbolicObsWrapper\n",
    "from minigrid.manual_control import ManualControl\n",
    "\n",
    "\n",
    "def l2_normalize(w, eps=1e-8):\n",
    "    n = np.linalg.norm(w)\n",
    "    if n < eps:\n",
    "        return w\n",
    "    return w / n\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Environment\n",
    "# ======================================================\n",
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=MissionSpace(lambda: \"grand mission\"),\n",
    "            grid_size=size,\n",
    "            see_through_walls=True,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # \"vertical wall\" (actually lava column at x=2, y=1..height-3)\n",
    "        for i in range(height - 2):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            else:\n",
    "                self.grid.set(2, i, Lava())\n",
    "\n",
    "        # goal\n",
    "        self.put_obj(Goal(), width - 2, height - 2)\n",
    "\n",
    "        # agent\n",
    "        self.agent_pos = self.agent_start_pos\n",
    "        self.agent_dir = self.agent_start_dir\n",
    "        self.mission = \"grand mission\"\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Feature extraction (state-based, NumPy-only)\n",
    "# ======================================================\n",
    "\n",
    "# direction → (dx, dy)\n",
    "DIR_TO_VEC = {\n",
    "    0: (1, 0),   # right\n",
    "    1: (0, 1),   # down\n",
    "    2: (-1, 0),  # left\n",
    "    3: (0, -1),  # up\n",
    "}\n",
    "\n",
    "W_MAP = {\n",
    "    \"L1.2\": np.array([   # [dist, on_lava, step]\n",
    "        -0.05,\n",
    "        -2.0,\n",
    "        -0.01\n",
    "    ]),\n",
    "\n",
    "    \"L1.3\": np.array([   # [dist, lava_ahead, on_lava, step]\n",
    "        -0.05,\n",
    "        -0.5,\n",
    "        -2.0,\n",
    "        -0.01\n",
    "    ]),\n",
    "\n",
    "    \"L2.1\": np.array([   # [dist, num_lava4, on_lava, step]\n",
    "        -0.05,\n",
    "        -0.3,\n",
    "        -2.0,\n",
    "        -0.01\n",
    "    ]),\n",
    "\n",
    "    \"L2.3\": np.array([   # [dist, lava_nearby8, on_lava, step]\n",
    "        -0.05,\n",
    "        -0.4,\n",
    "        -2.0,\n",
    "        -0.01\n",
    "    ]),\n",
    "\n",
    "    \"L3.1\": np.array([   # [dx, dy, lava_ahead, on_lava, step]\n",
    "        -0.02,\n",
    "        -0.02,\n",
    "        -0.5,\n",
    "        -2.0,\n",
    "        -0.01\n",
    "    ]),\n",
    "\n",
    "    \"L4.1\": np.array([   # [dist, min_dist_lava, on_lava, step]\n",
    "        -0.05,\n",
    "         0.10,   # farther from lava = good\n",
    "        -2.0,\n",
    "        -0.01\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# ======================================================\n",
    "# Feature set selector\n",
    "# ======================================================\n",
    "\n",
    "FEATURE_SET = \"L1.3\"   # <<< CHANGE HERE\n",
    "\n",
    "# ======================================================\n",
    "# Utilities for reward/features from (x,y,dir)\n",
    "# ======================================================\n",
    "\n",
    "def manhattan(p, q):\n",
    "    return abs(p[0] - q[0]) + abs(p[1] - q[1])\n",
    "\n",
    "def lava_ahead_state(lava_mask: np.ndarray, y: int, x: int, direction: int) -> int:\n",
    "    dx, dy = DIR_TO_VEC[direction]\n",
    "    ny, nx = y + dy, x + dx\n",
    "    if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "        return int(lava_mask[ny, nx])\n",
    "    return 0\n",
    "\n",
    "def on_lava_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    return int(lava_mask[y, x])\n",
    "\n",
    "def num_lava_4_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    cnt = 0\n",
    "    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "        ny, nx = y+dy, x+dx\n",
    "        if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "            cnt += int(lava_mask[ny, nx])\n",
    "    return cnt\n",
    "\n",
    "def lava_nearby_8_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    for dy in [-1,0,1]:\n",
    "        for dx in [-1,0,1]:\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            ny, nx = y+dy, x+dx\n",
    "            if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "                if lava_mask[ny, nx]:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "def min_dist_to_lava_state(lava_cells: np.ndarray, y: int, x: int, size: int) -> float:\n",
    "    if lava_cells.size == 0:\n",
    "        return float(2 * size)\n",
    "    # lava_cells is array of (y,x)\n",
    "    return float(np.min(np.abs(lava_cells[:, 0] - y) + np.abs(lava_cells[:, 1] - x)))\n",
    "\n",
    "def phi_from_state(state, goal_yx, lava_mask, lava_cells, size):\n",
    "    \"\"\"\n",
    "    state = (y, x, dir)\n",
    "    goal_yx = (gy, gx)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "    gy, gx = goal_yx\n",
    "\n",
    "    dist = manhattan((y, x), (gy, gx))\n",
    "    step = 1.0\n",
    "\n",
    "    if FEATURE_SET == \"L1.2\":\n",
    "        return np.array([dist, on_lava_state(lava_mask, y, x), step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L1.3\":\n",
    "        return np.array([\n",
    "            dist,\n",
    "            lava_ahead_state(lava_mask, y, x, direction),\n",
    "            on_lava_state(lava_mask, y, x),\n",
    "            step\n",
    "        ], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.1\":\n",
    "        return np.array([\n",
    "            dist,\n",
    "            num_lava_4_state(lava_mask, y, x),\n",
    "            on_lava_state(lava_mask, y, x),\n",
    "            step\n",
    "        ], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.3\":\n",
    "        return np.array([\n",
    "            dist,\n",
    "            lava_nearby_8_state(lava_mask, y, x),\n",
    "            on_lava_state(lava_mask, y, x),\n",
    "            step\n",
    "        ], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L3.1\":\n",
    "        dx = gx - x\n",
    "        dy = gy - y\n",
    "        return np.array([\n",
    "            dx,\n",
    "            dy,\n",
    "            lava_ahead_state(lava_mask, y, x, direction),\n",
    "            on_lava_state(lava_mask, y, x),\n",
    "            step\n",
    "        ], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L4.1\":\n",
    "        return np.array([\n",
    "            dist,\n",
    "            min_dist_to_lava_state(lava_cells, y, x, size),\n",
    "            on_lava_state(lava_mask, y, x),\n",
    "            step\n",
    "        ], dtype=float)\n",
    "\n",
    "    raise ValueError(f\"Unknown FEATURE_SET: {FEATURE_SET}\")\n",
    "\n",
    "\n",
    "def reward_from_state(state, goal_yx, lava_mask, lava_cells, size) -> float:\n",
    "    phi = phi_from_state(state, goal_yx, lava_mask, lava_cells, size)\n",
    "    W = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    return float(W @ phi)\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# MiniGrid dynamics model for planning (left/right/forward)\n",
    "# ======================================================\n",
    "\n",
    "# MiniGrid actions (common): left=0, right=1, forward=2\n",
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "\n",
    "\n",
    "def build_static_maps(env: SimpleEnv):\n",
    "    \"\"\"\n",
    "    Extract:\n",
    "      - size\n",
    "      - wall mask\n",
    "      - lava mask\n",
    "      - goal position\n",
    "    \"\"\"\n",
    "    size = env.width  # square\n",
    "    wall_mask = np.zeros((size, size), dtype=bool)\n",
    "    lava_mask = np.zeros((size, size), dtype=bool)\n",
    "    goal_yx = None\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            obj = env.grid.get(x, y)\n",
    "            if obj is None:\n",
    "                continue\n",
    "            if isinstance(obj, Wall):\n",
    "                wall_mask[y, x] = True\n",
    "            elif isinstance(obj, Lava):\n",
    "                lava_mask[y, x] = True\n",
    "            elif isinstance(obj, Goal):\n",
    "                goal_yx = (y, x)\n",
    "\n",
    "    if goal_yx is None:\n",
    "        raise RuntimeError(\"Goal not found in grid\")\n",
    "\n",
    "    lava_cells = np.argwhere(lava_mask)\n",
    "    return size, wall_mask, lava_mask, lava_cells, goal_yx\n",
    "\n",
    "def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "    y, x, _ = state\n",
    "    if (y, x) == goal_yx:\n",
    "        return True\n",
    "    if lava_mask[y, x]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def step_model(state, action, wall_mask, goal_yx, lava_mask):\n",
    "    \"\"\"\n",
    "    Deterministic transition for planning.\n",
    "    state = (y, x, dir)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "\n",
    "    # If already terminal, stay terminal (absorbing)\n",
    "    if is_terminal_state(state, goal_yx, lava_mask):\n",
    "        return state, True\n",
    "\n",
    "    if action == ACT_LEFT:\n",
    "        ndir = (direction - 1) % 4\n",
    "        nstate = (y, x, ndir)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_RIGHT:\n",
    "        ndir = (direction + 1) % 4\n",
    "        nstate = (y, x, ndir)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_FORWARD:\n",
    "        dx, dy = DIR_TO_VEC[direction]\n",
    "        ny, nx = y + dy, x + dx\n",
    "\n",
    "        # wall/bounds -> no move\n",
    "        if ny < 0 or ny >= wall_mask.shape[0] or nx < 0 or nx >= wall_mask.shape[1]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        if wall_mask[ny, nx]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        nstate = (ny, nx, direction)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "def enumerate_states(size, wall_mask):\n",
    "    \"\"\"\n",
    "    Include all non-wall cells for all 4 directions.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if wall_mask[y, x]:\n",
    "                continue\n",
    "            for d in range(4):\n",
    "                states.append((y, x, d))\n",
    "    return states\n",
    "\n",
    "def build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Converts your planning model into a tabular MDP for BIRL.\n",
    "    \"\"\"\n",
    "    S = len(states)\n",
    "    A = len(ACTIONS)\n",
    "\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "    T = np.zeros((S, A, S))\n",
    "    terminal_mask = np.zeros(S, dtype=bool)\n",
    "    Phi = np.zeros((S, len(W_MAP[FEATURE_SET])))\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        terminal_mask[i] = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        Phi[i] = phi_from_state(s, goal_yx, lava_mask, lava_cells, size)\n",
    "\n",
    "        for a_idx, a in enumerate(ACTIONS):\n",
    "            sp, _ = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            j = idx_of[sp]\n",
    "            T[i, a_idx, j] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"idx_of\": idx_of,\n",
    "        \"T\": T,\n",
    "        \"Phi\": Phi,\n",
    "        \"terminal\": terminal_mask,\n",
    "        \"gamma\": gamma,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# Value Iteration\n",
    "# ======================================================\n",
    "\n",
    "def value_iteration(\n",
    "    states,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    lava_cells,\n",
    "    size,\n",
    "    gamma=0.99,\n",
    "    theta=1e-8,\n",
    "    max_iters=20000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reward is computed from next state (s') like your ManualControl prints after env.step().\n",
    "    That matches: R = reward_numpy(obs_next).\n",
    "    \"\"\"\n",
    "    # index states for arrays\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "    V = np.zeros(len(states), dtype=float)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for i, s in enumerate(states):\n",
    "            if is_terminal_state(s, goal_yx, lava_mask):\n",
    "                # terminal value = 0 (absorbing, no future)\n",
    "                continue\n",
    "\n",
    "            best = -1e18\n",
    "            for a in ACTIONS:\n",
    "                sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "                r = reward_from_state(sp, goal_yx, lava_mask, lava_cells, size)\n",
    "\n",
    "                j = idx_of[sp]\n",
    "                q = r + (0.0 if done else gamma * V[j])\n",
    "                if q > best:\n",
    "                    best = q\n",
    "\n",
    "            new_v = best\n",
    "            delta = max(delta, abs(new_v - V[i]))\n",
    "            V[i] = new_v\n",
    "\n",
    "        if delta < theta:\n",
    "            # converged\n",
    "            # print(f\"Value iteration converged at iter={it}, delta={delta}\")\n",
    "            break\n",
    "\n",
    "    # greedy policy\n",
    "    pi = np.zeros(len(states), dtype=int)\n",
    "    for i, s in enumerate(states):\n",
    "        if is_terminal_state(s, goal_yx, lava_mask):\n",
    "            pi[i] = ACT_FORWARD  # arbitrary\n",
    "            continue\n",
    "\n",
    "        best_a = ACTIONS[0]\n",
    "        best_q = -1e18\n",
    "        for a in ACTIONS:\n",
    "            sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            r = reward_from_state(sp, goal_yx, lava_mask, lava_cells, size)\n",
    "            j = idx_of[sp]\n",
    "            q = r + (0.0 if done else gamma * V[j])\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "        pi[i] = best_a\n",
    "\n",
    "    return V, pi, idx_of\n",
    "\n",
    "def generate_state_action_demos(states, pi, terminal_mask):\n",
    "    \"\"\"\n",
    "    Demo = (state_id, optimal_action)\n",
    "    No trajectories.\n",
    "    \"\"\"\n",
    "    demos = []\n",
    "    for i, s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "        demos.append((i, int(pi[i])))\n",
    "    return demos\n",
    "\n",
    "# ======================================================\n",
    "# Manual control with live feature + reward printing (unchanged)\n",
    "# ======================================================\n",
    "\n",
    "class ManualControlWithReward(ManualControl):\n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # If you still want obs-based features/reward prints, keep your original pipeline:\n",
    "        # (SymbolicObsWrapper obs contains \"image\" and \"direction\".)\n",
    "        phi = phi_state_numpy(obs)\n",
    "        R = reward_numpy(obs)\n",
    "\n",
    "        print(\"φ(s) =\", phi, \"R(s) =\", R)\n",
    "\n",
    "        return obs, 0.0, terminated, truncated, info\n",
    "\n",
    "# ======================================================\n",
    "# Your original obs-based feature pipeline (kept for printing)\n",
    "# ======================================================\n",
    "\n",
    "LAVA_IDX = 9\n",
    "AGENT_IDX = 10\n",
    "GOAL_IDX = 8\n",
    "\n",
    "def get_agent_goal(idx):\n",
    "    agent = np.argwhere(idx == AGENT_IDX)\n",
    "    goal  = np.argwhere(idx == GOAL_IDX)\n",
    "\n",
    "    ay, ax = agent[0]\n",
    "\n",
    "    if goal.size == 0:\n",
    "        return (ay, ax), None\n",
    "    else:\n",
    "        gy, gx = goal[0]\n",
    "        return (ay, ax), (gy, gx)\n",
    "\n",
    "def lava_ahead(idx, ay, ax, direction):\n",
    "    dx, dy = DIR_TO_VEC[direction]\n",
    "    fy, fx = ay + dy, ax + dx\n",
    "    if 0 <= fy < idx.shape[0] and 0 <= fx < idx.shape[1]:\n",
    "        return int(idx[fy, fx] == LAVA_IDX)\n",
    "    return 0\n",
    "\n",
    "def on_lava(idx, ay, ax):\n",
    "    return int(idx[ay, ax] == LAVA_IDX)\n",
    "\n",
    "def num_lava_4(idx, ay, ax):\n",
    "    cnt = 0\n",
    "    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "        y, x = ay+dy, ax+dx\n",
    "        if 0 <= y < idx.shape[0] and 0 <= x < idx.shape[1]:\n",
    "            cnt += int(idx[y, x] == LAVA_IDX)\n",
    "    return cnt\n",
    "\n",
    "def lava_nearby_8(idx, ay, ax):\n",
    "    for dy in [-1,0,1]:\n",
    "        for dx in [-1,0,1]:\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            y, x = ay+dy, ax+dx\n",
    "            if 0 <= y < idx.shape[0] and 0 <= x < idx.shape[1]:\n",
    "                if idx[y, x] == LAVA_IDX:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "def min_dist_to_lava(idx, ay, ax):\n",
    "    lava_cells = np.argwhere(idx == LAVA_IDX)\n",
    "    if lava_cells.size == 0:\n",
    "        return float(idx.shape[0] + idx.shape[1])\n",
    "    return min(abs(ay-y) + abs(ax-x) for y, x in lava_cells)\n",
    "\n",
    "def phi_L12(obs):\n",
    "    idx = obs[\"image\"][:, :, 2]\n",
    "    (ay, ax), goal = get_agent_goal(idx)\n",
    "    dist = 0 if goal is None else manhattan((ay, ax), goal)\n",
    "    return np.array([dist, on_lava(idx, ay, ax), 1.0])\n",
    "\n",
    "def phi_L13(obs):\n",
    "    idx = obs[\"image\"][:, :, 2]\n",
    "    (ay, ax), goal = get_agent_goal(idx)\n",
    "    dist = 0 if goal is None else manhattan((ay, ax), goal)\n",
    "    return np.array([\n",
    "        dist,\n",
    "        lava_ahead(idx, ay, ax, obs[\"direction\"]),\n",
    "        on_lava(idx, ay, ax),\n",
    "        1.0\n",
    "    ])\n",
    "\n",
    "def phi_L21(obs):\n",
    "    idx = obs[\"image\"][:, :, 2]\n",
    "    (ay, ax), goal = get_agent_goal(idx)\n",
    "    dist = 0 if goal is None else manhattan((ay, ax), goal)\n",
    "    return np.array([\n",
    "        dist,\n",
    "        num_lava_4(idx, ay, ax),\n",
    "        on_lava(idx, ay, ax),\n",
    "        1.0\n",
    "    ])\n",
    "\n",
    "def phi_L23(obs):\n",
    "    idx = obs[\"image\"][:, :, 2]\n",
    "    (ay, ax), goal = get_agent_goal(idx)\n",
    "    dist = 0 if goal is None else manhattan((ay, ax), goal)\n",
    "    return np.array([\n",
    "        dist,\n",
    "        lava_nearby_8(idx, ay, ax),\n",
    "        on_lava(idx, ay, ax),\n",
    "        1.0\n",
    "    ])\n",
    "\n",
    "def phi_L31(obs):\n",
    "    idx = obs[\"image\"][:, :, 2]\n",
    "    (ay, ax), goal = get_agent_goal(idx)\n",
    "\n",
    "    if goal is None:\n",
    "        dx = dy = 0\n",
    "    else:\n",
    "        gy, gx = goal\n",
    "        dy, dx = gy - ay, gx - ax\n",
    "\n",
    "    return np.array([\n",
    "        dx,\n",
    "        dy,\n",
    "        lava_ahead(idx, ay, ax, obs[\"direction\"]),\n",
    "        on_lava(idx, ay, ax),\n",
    "        1.0\n",
    "    ])\n",
    "\n",
    "def phi_L41(obs):\n",
    "    idx = obs[\"image\"][:, :, 2]\n",
    "    (ay, ax), goal = get_agent_goal(idx)\n",
    "    dist = 0 if goal is None else manhattan((ay, ax), goal)\n",
    "    return np.array([\n",
    "        dist,\n",
    "        min_dist_to_lava(idx, ay, ax),\n",
    "        on_lava(idx, ay, ax),\n",
    "        1.0\n",
    "    ])\n",
    "\n",
    "PHI_MAP = {\n",
    "    \"L1.2\": phi_L12,\n",
    "    \"L1.3\": phi_L13,\n",
    "    \"L2.1\": phi_L21,\n",
    "    \"L2.3\": phi_L23,\n",
    "    \"L3.1\": phi_L31,\n",
    "    \"L4.1\": phi_L41,\n",
    "}\n",
    "\n",
    "def phi_state_numpy(obs):\n",
    "    return PHI_MAP[FEATURE_SET](obs)\n",
    "\n",
    "def reward_numpy(obs):\n",
    "    phi = phi_state_numpy(obs)\n",
    "    W = W_MAP[FEATURE_SET]\n",
    "    assert len(W) == len(phi), \"Weight/feature size mismatch\"\n",
    "    return float(W @ phi)\n",
    "\n",
    "# ======================================================\n",
    "# Run optimal policy on the env (rendered)\n",
    "# ======================================================\n",
    "\n",
    "def run_optimal_policy(env_wrapped, pi, idx_of, wall_mask, goal_yx, lava_mask, lava_cells, size, max_steps=1000):\n",
    "    \"\"\"\n",
    "    env_wrapped: SymbolicObsWrapper(SimpleEnv(render_mode=\"human\"))\n",
    "    \"\"\"\n",
    "    obs, info = env_wrapped.reset(seed=42)\n",
    "    terminated = truncated = False\n",
    "\n",
    "    # the wrapper keeps direction in obs; but we will read from env.unwrapped for safety\n",
    "    for t in range(max_steps):\n",
    "        uy, ux = env_wrapped.unwrapped.agent_pos[1], env_wrapped.unwrapped.agent_pos[0]  # careful: env uses (x,y)\n",
    "        # Actually MiniGrid stores agent_pos as (x,y). Your features used (y,x).\n",
    "        x, y = env_wrapped.unwrapped.agent_pos\n",
    "        d = int(env_wrapped.unwrapped.agent_dir)\n",
    "\n",
    "        s = (y, x, d)\n",
    "        a = int(pi[idx_of[s]])\n",
    "\n",
    "        obs, _, terminated, truncated, info = env_wrapped.step(a)\n",
    "\n",
    "        # print same style as manual\n",
    "        phi = phi_state_numpy(obs)\n",
    "        R = reward_numpy(obs)\n",
    "        print(f\"t={t:03d} a={a}  φ(s')={phi}  R(s')={R:.4f}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "# ======================================================\n",
    "# Run BIRL to learn reward \n",
    "# ======================================================\n",
    "\n",
    "class DemoOnlyBIRL:\n",
    "    def __init__(self, T, Phi, gamma, demos, beta=10.0):\n",
    "        self.T = T\n",
    "        self.Phi = Phi\n",
    "        self.gamma = gamma\n",
    "        self.demos = demos\n",
    "        self.beta = beta\n",
    "        self.S, self.A, _ = T.shape\n",
    "        self.D = Phi.shape[1]\n",
    "\n",
    "    def _q_values(self, theta):\n",
    "        r = self.Phi @ theta\n",
    "        Q = np.zeros((self.S, self.A))\n",
    "        V = np.zeros(self.S)\n",
    "\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "            for s in range(self.S):\n",
    "                q_vals = []\n",
    "                for a in range(self.A):\n",
    "                    q_vals.append(r[s] + self.gamma * np.sum(self.T[s, a] * V))\n",
    "                v_new = max(q_vals)\n",
    "                delta = max(delta, abs(V[s] - v_new))\n",
    "                V[s] = v_new\n",
    "            if delta < 1e-8:\n",
    "                break\n",
    "\n",
    "        for s in range(self.S):\n",
    "            for a in range(self.A):\n",
    "                Q[s, a] = r[s] + self.gamma * np.sum(self.T[s, a] * V)\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        Q = self._q_values(theta)\n",
    "\n",
    "        ll = 0.0\n",
    "        for s, a in self.demos:\n",
    "            logits = self.beta * Q[s]\n",
    "            ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=400, step_size=0.3):\n",
    "        rng = np.random.default_rng(0)\n",
    "        theta = rng.normal(0, 1, self.D)\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            #prop = theta + rng.normal(0, step_size, self.D)\n",
    "            prop = theta + rng.normal(0, step_size, self.D)\n",
    "            prop = l2_normalize(prop)\n",
    "\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "def expected_value_difference(T, gamma, r_true, pi_true, pi_eval):\n",
    "    def policy_eval(policy):\n",
    "        V = np.zeros(len(r_true))\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "            for s in range(len(V)):\n",
    "                v_new = r_true[s] + gamma * np.sum(T[s, policy[s]] * V)\n",
    "                delta = max(delta, abs(V[s] - v_new))\n",
    "                V[s] = v_new\n",
    "            if delta < 1e-8:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    V_opt = policy_eval(pi_true)\n",
    "    V_eval = policy_eval(pi_eval)\n",
    "    return np.mean(V_opt) - np.mean(V_eval)\n",
    "\n",
    "def birl_main():\n",
    "    planning_env = SimpleEnv(render_mode=None)\n",
    "    planning_env.reset(seed=0)\n",
    "\n",
    "    size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(planning_env)\n",
    "    states = enumerate_states(size, wall_mask)\n",
    "\n",
    "    # Ground-truth planning\n",
    "    V_true, pi_true, idx_of = value_iteration(\n",
    "        states, wall_mask, goal_yx, lava_mask, lava_cells, size\n",
    "    )\n",
    "\n",
    "    mdp = build_tabular_mdp(\n",
    "        states, wall_mask, goal_yx, lava_mask, lava_cells, size\n",
    "    )\n",
    "\n",
    "    demos = generate_state_action_demos(\n",
    "        states, pi_true, mdp[\"terminal\"]\n",
    "    )\n",
    "\n",
    "    # Run BIRL\n",
    "    birl = DemoOnlyBIRL(\n",
    "        mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], demos\n",
    "    )\n",
    "    theta_hat = birl.run()\n",
    "\n",
    "    # Learned policy\n",
    "    r_hat = mdp[\"Phi\"] @ theta_hat\n",
    "    V_hat = np.zeros(len(states))\n",
    "    pi_hat = np.zeros(len(states), dtype=int)\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        best_q = -1e18\n",
    "        for a in ACTIONS:\n",
    "            sp, _ = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            j = idx_of[sp]\n",
    "            q = r_hat[i] + mdp[\"gamma\"] * V_hat[j]\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                pi_hat[i] = a\n",
    "\n",
    "    # Evaluation\n",
    "    #r_true = mdp[\"Phi\"] @ W_MAP[FEATURE_SET]\n",
    "    r_true = mdp[\"Phi\"] @ l2_normalize(W_MAP[FEATURE_SET])\n",
    "\n",
    "    evd = expected_value_difference(\n",
    "        mdp[\"T\"], mdp[\"gamma\"], r_true, pi_true, pi_hat\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== BIRL RESULTS =====\")\n",
    "    print(\"True θ     :\", W_MAP[FEATURE_SET])\n",
    "    print(\"Learned θ  :\", theta_hat)\n",
    "    print(\"EVD        :\", evd)\n",
    "\n",
    "# ======================================================\n",
    "# Main\n",
    "# ======================================================\n",
    "\n",
    "def main():\n",
    "    # Create env once for planning map extraction (no render needed)\n",
    "    planning_env = SimpleEnv(render_mode=None)\n",
    "    planning_env.reset(seed=42)\n",
    "\n",
    "    size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(planning_env)\n",
    "\n",
    "    states = enumerate_states(size, wall_mask)\n",
    "    V, pi, idx_of = value_iteration(\n",
    "        states,\n",
    "        wall_mask,\n",
    "        goal_yx,\n",
    "        lava_mask,\n",
    "        lava_cells,\n",
    "        size,\n",
    "        gamma=0.99,\n",
    "        theta=1e-8,\n",
    "    )\n",
    "\n",
    "    # Pretty-print a tiny summary\n",
    "    start_state = (planning_env.agent_start_pos[1], planning_env.agent_start_pos[0], planning_env.agent_start_dir)  # (y,x,dir)\n",
    "    print(\"FEATURE_SET =\", FEATURE_SET)\n",
    "    print(\"Start state =\", start_state, \"V(start) =\", V[idx_of[start_state]])\n",
    "\n",
    "    # Now run in rendered env using the computed policy\n",
    "    env = SimpleEnv(render_mode=\"human\")\n",
    "    env = SymbolicObsWrapper(env)\n",
    "\n",
    "    run_optimal_policy(\n",
    "        env_wrapped=env,\n",
    "        pi=pi,\n",
    "        idx_of=idx_of,\n",
    "        wall_mask=wall_mask,\n",
    "        goal_yx=goal_yx,\n",
    "        lava_mask=lava_mask,\n",
    "        lava_cells=lava_cells,\n",
    "        size=size,\n",
    "        max_steps=4 * size * size,\n",
    "    )\n",
    "\n",
    "    # If you want manual mode instead, comment out run_optimal_policy and use:\n",
    "    # manual_control = ManualControlWithReward(env, seed=42)\n",
    "    # manual_control.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    birl_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636874cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
