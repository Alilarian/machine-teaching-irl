{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ae0d00",
   "metadata": {},
   "source": [
    "## Manually control env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6d67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils import remove_redundant_constraints\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.mission import MissionSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14332a3f",
   "metadata": {},
   "source": [
    "### Set cover algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17db8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_key(v, *, normalize=True, round_decimals=12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = np.linalg.norm(v)\n",
    "    if n == 0.0 or not np.isfinite(n):\n",
    "        return (\"ZERO\",)\n",
    "    if normalize:\n",
    "        v = v / n\n",
    "    return tuple(np.round(v, round_decimals))\n",
    "\n",
    "\n",
    "def scot_greedy_single_env(\n",
    "    *,\n",
    "    atoms,               # list of atoms (e.g., demo indices or (s,a) pairs)\n",
    "    constraints_per_atom,# list[list[np.ndarray]]\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal SCOT greedy set cover for ONE environment.\n",
    "\n",
    "    Universe:\n",
    "        unique constraint directions across all atoms\n",
    "\n",
    "    Atom coverage:\n",
    "        atom covers a constraint if it generates that direction\n",
    "\n",
    "    Returns:\n",
    "        chosen_atoms\n",
    "        chosen_constraints (stacked)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Build universe of unique constraints\n",
    "    # --------------------------------------------------\n",
    "    key_to_uid = {}\n",
    "    uid_to_vec = {}\n",
    "    uid = 0\n",
    "\n",
    "    atom_constraint_keys = []\n",
    "\n",
    "    for atom_constraints in constraints_per_atom:\n",
    "        keys = set()\n",
    "        for v in atom_constraints:\n",
    "            k = constraint_key(v,\n",
    "                               normalize=normalize,\n",
    "                               round_decimals=round_decimals)\n",
    "            if k not in key_to_uid:\n",
    "                key_to_uid[k] = uid\n",
    "                uid_to_vec[uid] = v\n",
    "                uid += 1\n",
    "            keys.add(key_to_uid[k])\n",
    "        atom_constraint_keys.append(keys)\n",
    "\n",
    "    universe = set(range(uid))\n",
    "    covered = set()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Greedy set cover\n",
    "    # --------------------------------------------------\n",
    "    chosen_atoms = []\n",
    "    chosen_constraints = []\n",
    "\n",
    "    while covered != universe:\n",
    "        best_gain = 0\n",
    "        best_idx = None\n",
    "        best_new = None\n",
    "\n",
    "        for i, cov in enumerate(atom_constraint_keys):\n",
    "            gain_set = cov - covered\n",
    "            if len(gain_set) > best_gain:\n",
    "                best_gain = len(gain_set)\n",
    "                best_idx = i\n",
    "                best_new = gain_set\n",
    "\n",
    "        if best_idx is None:\n",
    "            break  # cannot cover remaining constraints\n",
    "\n",
    "        chosen_atoms.append(atoms[best_idx])\n",
    "        for uid in best_new:\n",
    "            chosen_constraints.append(uid_to_vec[uid])\n",
    "\n",
    "        covered |= best_new\n",
    "\n",
    "    if chosen_constraints:\n",
    "        chosen_constraints = np.vstack(chosen_constraints)\n",
    "    else:\n",
    "        chosen_constraints = np.zeros((0, atoms[0][0].shape[-1]))\n",
    "\n",
    "    return chosen_atoms, chosen_constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a63f1",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96202c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUCCESSOR FEATURES DEBUG =====\n",
      "State 000 (1, 1, 0)\n",
      "  ψ(s)        = [3. 0. 1. 1.]\n",
      "  ψ(s)·θ      = -1.470928\n",
      "  V_true(s)   = -1.470928\n",
      "State 001 (1, 1, 1)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 002 (1, 1, 2)\n",
      "  ψ(s)        = [11.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.926563\n",
      "  V_true(s)   = -1.926563\n",
      "State 003 (1, 1, 3)\n",
      "  ψ(s)        = [6.97 1.   0.99 1.99]\n",
      "  ψ(s)·θ      = -2.117643\n",
      "  V_true(s)   = -2.117643\n",
      "State 008 (1, 3, 0)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 009 (1, 3, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 010 (1, 3, 2)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 011 (1, 3, 3)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 012 (2, 1, 0)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 013 (2, 1, 1)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 014 (2, 1, 2)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 015 (2, 1, 3)\n",
      "  ψ(s)        = [10.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.768611\n",
      "  V_true(s)   = -1.768611\n",
      "State 020 (2, 3, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 021 (2, 3, 1)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 022 (2, 3, 2)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 023 (2, 3, 3)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 024 (3, 1, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 025 (3, 1, 1)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 026 (3, 1, 2)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 027 (3, 1, 3)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 028 (3, 2, 0)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 029 (3, 2, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 030 (3, 2, 2)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 031 (3, 2, 3)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "\n",
      "===== CONSTRAINTS DEBUG =====\n",
      "Num constraints: 47\n",
      "Dim: (4,)\n",
      "c[0] = [-0.96318961 -0.12069893  0.00242617 -0.24019086]\n",
      "c[1] = [-0.87215285  0.          0.09875597 -0.47916247]\n",
      "c[2] = [ 0.2793058  -0.29697618 -0.29400642  0.86449618]\n",
      "c[3] = [-0.97182342  0.          0.         -0.23571006]\n",
      "c[4] = [ 0.2793058  -0.29697618 -0.29400642  0.86449618]\n",
      "\n",
      "===== CONSTRAINT PRUNING =====\n",
      "Original constraints : 47\n",
      "After pruning        : 5\n",
      "\n",
      "===== BIRL RESULTS (SYMBOLIC-FREE) =====\n",
      "FEATURE_SET : L1.3\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "Learned θ   : [-0.54382213 -0.62544988 -0.52946151  0.18094322]\n",
      "EVD         : 0.0013839260312477064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1224328/3773075852.py:548: RuntimeWarning: overflow encountered in exp\n",
      "  ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SCOT + BIRL RESULTS =====\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "SCOT θ̂     : [-0.2145758  -0.45881618 -0.85507385  0.11087675]\n",
      "Num demos   : 24\n",
      "SCOT demos  : 13\n",
      "SCOT EVD    : 0.0\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Utils\n",
    "# ======================================================\n",
    "def l2_normalize(w, eps=1e-8):\n",
    "    n = np.linalg.norm(w)\n",
    "    return w if n < eps else w / n\n",
    "\n",
    "# ======================================================\n",
    "# Feature extraction (state-based, canonical)\n",
    "# ======================================================\n",
    "\n",
    "DIR_TO_VEC = {\n",
    "    0: (1, 0),   # right\n",
    "    1: (0, 1),   # down\n",
    "    2: (-1, 0),  # left\n",
    "    3: (0, -1),  # up\n",
    "}\n",
    "\n",
    "W_MAP = {\n",
    "    \"L1.2\": np.array([-0.05, -2.0, -0.01]),                       # [dist, on_lava, step]\n",
    "    \"L1.3\": np.array([-0.8, -0.1, -5.0, -0.05]),                  # [dist, lava_ahead, on_lava, step]\n",
    "    \"L2.1\": np.array([-0.05, -0.3, -2.0, -0.01]),                 # [dist, num_lava4, on_lava, step]\n",
    "    \"L2.3\": np.array([-0.05, -0.4, -2.0, -0.01]),                 # [dist, lava_nearby8, on_lava, step]\n",
    "    \"L3.1\": np.array([-0.02, -0.02, -0.5, -2.0, -0.01]),          # [dx, dy, lava_ahead, on_lava, step]\n",
    "    \"L4.1\": np.array([-0.05,  0.10, -2.0, -0.01]),                # [dist, min_dist_lava, on_lava, step]\n",
    "}\n",
    "\n",
    "FEATURE_SET = \"L1.3\"\n",
    "\n",
    "def manhattan(p, q):\n",
    "    return abs(p[0] - q[0]) + abs(p[1] - q[1])\n",
    "\n",
    "def lava_ahead_state(lava_mask: np.ndarray, y: int, x: int, direction: int) -> int:\n",
    "    dx, dy = DIR_TO_VEC[direction]\n",
    "    ny, nx = y + dy, x + dx\n",
    "    if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "        return int(lava_mask[ny, nx])\n",
    "    return 0\n",
    "\n",
    "def on_lava_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    return int(lava_mask[y, x])\n",
    "\n",
    "def num_lava_4_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    cnt = 0\n",
    "    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "        ny, nx = y+dy, x+dx\n",
    "        if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "            cnt += int(lava_mask[ny, nx])\n",
    "    return cnt\n",
    "\n",
    "def lava_nearby_8_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    for dy in [-1,0,1]:\n",
    "        for dx in [-1,0,1]:\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            ny, nx = y+dy, x+dx\n",
    "            if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "                if lava_mask[ny, nx]:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "def min_dist_to_lava_state(lava_cells: np.ndarray, y: int, x: int, size: int) -> float:\n",
    "    if lava_cells.size == 0:\n",
    "        return float(2 * size)\n",
    "    return float(np.min(np.abs(lava_cells[:, 0] - y) + np.abs(lava_cells[:, 1] - x)))\n",
    "\n",
    "def phi_from_state(state, goal_yx, lava_mask, lava_cells, size):\n",
    "    \"\"\"\n",
    "    state = (y, x, dir)\n",
    "    goal_yx = (gy, gx)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "    gy, gx = goal_yx\n",
    "\n",
    "    dist = manhattan((y, x), (gy, gx))\n",
    "    step = 1.0\n",
    "\n",
    "    if FEATURE_SET == \"L1.2\":\n",
    "        return np.array([dist, on_lava_state(lava_mask, y, x), step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L1.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.1\":\n",
    "        return np.array([dist,\n",
    "                         num_lava_4_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_nearby_8_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L3.1\":\n",
    "        dx = gx - x\n",
    "        dy = gy - y\n",
    "        return np.array([dx, dy,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L4.1\":\n",
    "        return np.array([dist,\n",
    "                         min_dist_to_lava_state(lava_cells, y, x, size),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    raise ValueError(f\"Unknown FEATURE_SET: {FEATURE_SET}\")\n",
    "\n",
    "def reward_from_state(state, goal_yx, lava_mask, lava_cells, size, w=None) -> float:\n",
    "    \"\"\"\n",
    "    State-based reward for manual debugging/printing.\n",
    "    If w is None, uses normalized W_MAP[FEATURE_SET].\n",
    "    \"\"\"\n",
    "    phi = phi_from_state(state, goal_yx, lava_mask, lava_cells, size)\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    else:\n",
    "        w = l2_normalize(w)\n",
    "    return float(w @ phi)\n",
    "\n",
    "# ======================================================\n",
    "# Planning dynamics model (left/right/forward)\n",
    "# ======================================================\n",
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "\n",
    "# ======================================================\n",
    "# SimpleEnv (same as your previous; minimal here)\n",
    "# ======================================================\n",
    "mission_space = MissionSpace(mission_func=lambda: \"reach the goal\")\n",
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            width=size,\n",
    "            height=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        from minigrid.core.grid import Grid\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Example layout: you can replace with yours\n",
    "        # Put goal at bottom-right inside the walls\n",
    "        self.put_obj(Goal(), width-2, height-2)\n",
    "\n",
    "        # Optional: lava example\n",
    "        \n",
    "        self.put_obj(Lava(), 2, 2)\n",
    "        self.put_obj(Lava(), 2, 1)\n",
    "        \n",
    "        self.agent_pos = self.agent_start_pos\n",
    "        self.agent_dir = self.agent_start_dir\n",
    "        self.mission = \"reach the goal\"\n",
    "\n",
    "# ======================================================\n",
    "# Static map extraction\n",
    "# ======================================================\n",
    "def build_static_maps(env: SimpleEnv):\n",
    "    \"\"\"\n",
    "    Returns everything in NumPy (y,x) convention.\n",
    "    \"\"\"\n",
    "    size = env.width\n",
    "    wall_mask = np.zeros((size, size), dtype=bool)\n",
    "    lava_mask = np.zeros((size, size), dtype=bool)\n",
    "    goal_yx = None\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            obj = env.grid.get(x, y)  # MiniGrid uses (x,y)\n",
    "            if obj is None:\n",
    "                continue\n",
    "            if isinstance(obj, Wall):\n",
    "                wall_mask[y, x] = True\n",
    "            elif isinstance(obj, Lava):\n",
    "                lava_mask[y, x] = True\n",
    "            elif isinstance(obj, Goal):\n",
    "                goal_yx = (y, x)\n",
    "\n",
    "    if goal_yx is None:\n",
    "        raise RuntimeError(\"Goal not found in grid\")\n",
    "\n",
    "    lava_cells = np.argwhere(lava_mask)\n",
    "    return size, wall_mask, lava_mask, lava_cells, goal_yx\n",
    "\n",
    "# def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "#     y, x, _ = state\n",
    "#     return (y, x) == goal_yx\n",
    "\n",
    "def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "    y, x, _ = state\n",
    "    return (y, x) == goal_yx or lava_mask[y, x]\n",
    "\n",
    "def step_model(state, action, wall_mask, goal_yx, lava_mask):\n",
    "    \"\"\"\n",
    "    Deterministic transition for planning.\n",
    "    state = (y, x, dir)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "\n",
    "    if is_terminal_state(state, goal_yx, lava_mask):\n",
    "        return state, True\n",
    "\n",
    "    if action == ACT_LEFT:\n",
    "        nstate = (y, x, (direction - 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_RIGHT:\n",
    "        nstate = (y, x, (direction + 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_FORWARD:\n",
    "        dx, dy = DIR_TO_VEC[direction]\n",
    "        ny, nx = y + dy, x + dx\n",
    "\n",
    "        # bounds/wall => no move\n",
    "        if ny < 0 or ny >= wall_mask.shape[0] or nx < 0 or nx >= wall_mask.shape[1]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        if wall_mask[ny, nx]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        nstate = (ny, nx, direction)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "def enumerate_states(size, wall_mask):\n",
    "    \"\"\"\n",
    "    All non-wall cells for all 4 directions.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if wall_mask[y, x]:\n",
    "                continue\n",
    "            for d in range(4):\n",
    "                states.append((y, x, d))\n",
    "    return states\n",
    "\n",
    "def build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Tabular MDP: deterministic T, features Phi(s).\n",
    "    \"\"\"\n",
    "    S = len(states)\n",
    "    A = len(ACTIONS)\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "    terminal_mask = np.zeros(S, dtype=bool)\n",
    "    Phi = np.zeros((S, len(W_MAP[FEATURE_SET])), dtype=float)\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        terminal_mask[i] = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        Phi[i] = phi_from_state(s, goal_yx, lava_mask, lava_cells, size)\n",
    "        for a_idx, a in enumerate(ACTIONS):\n",
    "            sp, _done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            T[i, a_idx, idx_of[sp]] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"idx_of\": idx_of,\n",
    "        \"T\": T,\n",
    "        \"Phi\": Phi,\n",
    "        \"terminal\": terminal_mask,\n",
    "        \"gamma\": gamma,\n",
    "        \"goal_yx\": goal_yx,\n",
    "        \"lava_mask\": lava_mask,\n",
    "        \"wall_mask\": wall_mask,\n",
    "        \"lava_cells\": lava_cells,\n",
    "        \"size\": size,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# ONE CONSISTENT next-state Policy Eval + Value Iteration\n",
    "# ======================================================\n",
    "\n",
    "def policy_evaluation_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a fixed policy with NEXT-state reward:\n",
    "      V(s) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "    Terminal states are kept at V=0 (consistent with your VI done-cutoff).\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)  # 1 if nonterminal, 0 if terminal\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "            a = int(policy[s])\n",
    "            v_new = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def value_iteration_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    "):\n",
    "    \"\"\"\n",
    "    NEXT-state reward value iteration:\n",
    "      Q(s,a) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "      V(s) = max_a Q(s,a)\n",
    "    Terminal states fixed at V=0.\n",
    "    Returns: V, Q, pi\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "    Q = np.zeros((S, A), dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            # compute Q(s,a) for all a\n",
    "            for a in range(A):\n",
    "                Q[s, a] = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "\n",
    "            v_new = float(np.max(Q[s]))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # greedy policy\n",
    "    pi = np.zeros(S, dtype=int)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            pi[s] = ACT_FORWARD\n",
    "        else:\n",
    "            pi[s] = int(np.argmax(Q[s]))\n",
    "\n",
    "    return V, Q, pi\n",
    "\n",
    "def compute_successor_features_from_q_next_state(\n",
    "    T: np.ndarray,\n",
    "    Phi: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    tol: float = 1e-10,\n",
    "    max_iters: int = 100000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Successor Features with NEXT-STATE (entering) convention, consistent with your code.\n",
    "\n",
    "    Definitions:\n",
    "      π(s)      = argmax_a Q(s,a)\n",
    "      ψ(s)      = E_π [ sum_t γ^t φ(s_{t+1}) | s0 = s ]\n",
    "      ψ(s,a)    = E [ φ(s1) + γ ψ(s1) | s0=s, a0=a ]\n",
    "\n",
    "    Bellman equation:\n",
    "      ψ(s) = Σ_{s'} P_π(s,s') [ φ(s') + γ * 1[~terminal(s')] * ψ(s') ]\n",
    "\n",
    "    Inputs:\n",
    "      T             : (S,A,S) transition matrix\n",
    "      Phi           : (S,D) state feature matrix (φ(s))\n",
    "      Q             : (S,A) Q-values (used to extract greedy policy)\n",
    "      terminal_mask : (S,) boolean\n",
    "      gamma         : discount factor\n",
    "\n",
    "    Returns:\n",
    "      Psi_sa : (S,A,D) successor features for state-action\n",
    "      Psi_s  : (S,D)   successor features for state\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    D = Phi.shape[1]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Greedy policy from Q\n",
    "    # -----------------------------\n",
    "    Pi = np.zeros((S, A), dtype=float)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            continue\n",
    "        Pi[s, np.argmax(Q[s])] = 1.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Policy transition matrix\n",
    "    # P_pi[s,s'] = Σ_a π(a|s) T[s,a,s']\n",
    "    # -----------------------------\n",
    "    P_pi = np.zeros((S, S), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            if Pi[s, a] > 0:\n",
    "                P_pi[s] += Pi[s, a] * T[s, a]\n",
    "\n",
    "        # absorbing fallback (safety)\n",
    "        if P_pi[s].sum() == 0:\n",
    "            P_pi[s, s] = 1.0\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Iterative policy SFs ψ(s)\n",
    "    # -----------------------------\n",
    "    Psi_s = np.zeros((S, D), dtype=float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        Psi_old = Psi_s.copy()\n",
    "\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            exp_phi_next = P_pi[s] @ Phi\n",
    "            exp_psi_next = P_pi[s] @ Psi_old\n",
    "\n",
    "            Psi_s[s] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "        if np.max(np.abs(Psi_s - Psi_old)) < tol:\n",
    "            break\n",
    "\n",
    "    # -----------------------------\n",
    "    # State–action successor features ψ(s,a)\n",
    "    # -----------------------------\n",
    "    Psi_sa = np.zeros((S, A, D), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            p_next = T[s, a]\n",
    "            exp_phi_next = p_next @ Phi\n",
    "            exp_psi_next = p_next @ Psi_s\n",
    "            Psi_sa[s, a] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "    return Psi_sa, Psi_s\n",
    "\n",
    "\n",
    "# Convenience wrapper if you still want \"state-based VI\" signature:\n",
    "def value_iteration_from_env_next_state(\n",
    "    states,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    lava_cells,\n",
    "    size,\n",
    "    gamma=0.99,\n",
    "    theta=1e-8,\n",
    "    max_iters=20000,\n",
    "    w=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds T and r_next from the env step model, then calls the unified tabular VI.\n",
    "    Keeps your original external interface for minimal disruption.\n",
    "    \"\"\"\n",
    "    mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=gamma)\n",
    "    # reward on NEXT state: r_next[s'] = R(s') (state reward evaluated at s')\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    r_next = mdp[\"Phi\"] @ l2_normalize(w)\n",
    "    V, Q, pi = value_iteration_next_state(mdp[\"T\"], r_next, mdp[\"terminal\"], gamma, theta=theta, max_iters=max_iters)\n",
    "    return V, Q, pi, mdp[\"idx_of\"], mdp\n",
    "\n",
    "# ======================================================\n",
    "# Demos\n",
    "# ======================================================\n",
    "def generate_state_action_demos(states, pi, terminal_mask):\n",
    "    demos = []\n",
    "    for i, _s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "        demos.append((i, int(pi[i])))\n",
    "    return demos\n",
    "\n",
    "# ======================================================\n",
    "# BIRL (demo-only, MCMC over theta)\n",
    "# ======================================================\n",
    "class DemoOnlyBIRL:\n",
    "    def __init__(self, T, Phi, gamma, terminal_mask, demos, beta=10.0):\n",
    "        self.T = T\n",
    "        self.Phi = Phi\n",
    "        self.gamma = gamma\n",
    "        self.terminal_mask = terminal_mask\n",
    "        self.demos = demos\n",
    "        self.beta = beta\n",
    "        self.S, self.A, _ = T.shape\n",
    "        self.D = Phi.shape[1]\n",
    "\n",
    "    def _q_values(self, theta):\n",
    "        \"\"\"\n",
    "        Uses the ONE unified value iteration with NEXT-state reward.\n",
    "        Interprets r_next[s'] = Phi[s'] @ theta  (reward of next state).\n",
    "        \"\"\"\n",
    "        theta = l2_normalize(theta)\n",
    "        r_next = self.Phi @ theta\n",
    "        V, Q, _pi = value_iteration_next_state(\n",
    "            self.T, r_next, self.terminal_mask, self.gamma, theta=1e-8, max_iters=200000\n",
    "        )\n",
    "        return Q\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        Q = self._q_values(theta)\n",
    "\n",
    "        ll = 0.0\n",
    "        for s, a in self.demos:\n",
    "            logits = self.beta * Q[s]\n",
    "            ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=200, step_size=0.3, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        theta = l2_normalize(rng.normal(0, 1, self.D))\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            prop = l2_normalize(theta + rng.normal(0, step_size, self.D))\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "# ======================================================\n",
    "# Evaluation (EVD) using unified policy eval\n",
    "# ======================================================\n",
    "def expected_value_difference(T, gamma, r_next_true, pi_true, pi_eval, terminal_mask):\n",
    "    \"\"\"\n",
    "    EVD with NEXT-state reward, consistent everywhere:\n",
    "      EVD = mean(V^pi_true) - mean(V^pi_eval)\n",
    "    \"\"\"\n",
    "    V_opt = policy_evaluation_next_state(T, r_next_true, pi_true, terminal_mask, gamma)\n",
    "    V_ev  = policy_evaluation_next_state(T, r_next_true, pi_eval, terminal_mask, gamma)\n",
    "    return float(np.mean(V_opt) - np.mean(V_ev))\n",
    "\n",
    "# ======================================================\n",
    "# Debug printing (state-based)\n",
    "# ======================================================\n",
    "def debug_print_values_and_demos_from_tabular(\n",
    "    states,\n",
    "    V,\n",
    "    Q,\n",
    "    pi,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "):\n",
    "    print(\"\\n===== DEBUG: V(s), Q(s,a), DEMOS =====\")\n",
    "    for i, s in enumerate(states):\n",
    "        y, x, d = s\n",
    "\n",
    "        if wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        term = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        term_str = \"TERMINAL\" if term else \"\"\n",
    "\n",
    "        print(f\"\\nState {i:03d} (y={y}, x={x}, d={d}) [{cell_type}] {term_str}\")\n",
    "        print(f\"  V(s) = {V[i]: .6f}\")\n",
    "\n",
    "        if term:\n",
    "            print(\"  (terminal state, no actions)\")\n",
    "            continue\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            print(f\"  Q(s,a={a}) = {Q[i, a]: .6f}\")\n",
    "\n",
    "        print(f\"  OPTIMAL ACTION (demo) = {pi[i]}\")\n",
    "\n",
    "# ======================================================\n",
    "# Optional manual control (no SymbolicObsWrapper)\n",
    "# ======================================================\n",
    "class ManualControlStateReward(ManualControl):\n",
    "    def __init__(self, env, wall_mask, lava_mask, lava_cells, goal_yx, size):\n",
    "        super().__init__(env)\n",
    "        self.wall_mask = wall_mask\n",
    "        self.lava_mask = lava_mask\n",
    "        self.lava_cells = lava_cells\n",
    "        self.goal_yx = goal_yx\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, action):\n",
    "        _obs, _rew, terminated, truncated, _info = self.env.step(action)\n",
    "\n",
    "        x, y = self.env.unwrapped.agent_pos  # env uses (x,y)\n",
    "        d = int(self.env.unwrapped.agent_dir)\n",
    "        s = (y, x, d)\n",
    "\n",
    "        if self.wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == self.goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif self.lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        phi = phi_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "        R = reward_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "\n",
    "        print(f\"STATE {s} [{cell_type}]  φ(s)={phi}  R(s)={R:.4f}\")\n",
    "\n",
    "        return _obs, 0.0, terminated, truncated, _info\n",
    "\n",
    "def constraints_from_demos_next_state(\n",
    "    demos,\n",
    "    Psi_sa,\n",
    "    terminal_mask=None,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds linear reward constraints from demos using successor features.\n",
    "\n",
    "    Each constraint is:\n",
    "        (ψ(s,a*) - ψ(s,a)) · θ >= 0     for all a != a*\n",
    "\n",
    "    Inputs:\n",
    "      demos         : list of (s, a_star) pairs (state index, optimal action)\n",
    "      Psi_sa        : (S, A, D) successor features (NEXT-state convention)\n",
    "      terminal_mask : optional (S,) boolean mask\n",
    "      normalize     : L2-normalize constraint vectors\n",
    "      tol           : skip near-zero constraints\n",
    "\n",
    "    Returns:\n",
    "      constraints : list of constraint vectors v ∈ R^D\n",
    "    \"\"\"\n",
    "    Psi_sa = np.asarray(Psi_sa)\n",
    "    S, A, D = Psi_sa.shape\n",
    "    constraints = []\n",
    "\n",
    "    if demos is None:\n",
    "        return constraints\n",
    "\n",
    "    for s, a_star in demos:\n",
    "        if s is None or a_star is None:\n",
    "            continue\n",
    "\n",
    "        s = int(s)\n",
    "        a_star = int(a_star)\n",
    "\n",
    "        if not (0 <= s < S) or not (0 <= a_star < A):\n",
    "            continue\n",
    "\n",
    "        if terminal_mask is not None and terminal_mask[s]:\n",
    "            continue\n",
    "\n",
    "        psi_star = Psi_sa[s, a_star]\n",
    "\n",
    "        for a in range(A):\n",
    "            if a == a_star:\n",
    "                continue\n",
    "\n",
    "            diff = psi_star - Psi_sa[s, a]\n",
    "            norm = np.linalg.norm(diff)\n",
    "\n",
    "            if norm <= tol:\n",
    "                continue\n",
    "\n",
    "            constraints.append(diff / norm if normalize else diff)\n",
    "\n",
    "    return constraints\n",
    "\n",
    "# ======================================================\n",
    "# Main (BIRL experiment)\n",
    "# ======================================================\n",
    "def birl_main():\n",
    "    planning_env = SimpleEnv(render_mode=None)\n",
    "    planning_env.reset(seed=0)\n",
    "\n",
    "    size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(planning_env)\n",
    "    states = enumerate_states(size, wall_mask)\n",
    "\n",
    "    # Build tabular MDP once\n",
    "    mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99)\n",
    "\n",
    "    # TRUE reward as NEXT-state reward vector over state indices\n",
    "    theta_true = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    r_next_true = mdp[\"Phi\"] @ theta_true\n",
    "\n",
    "    # TRUE optimal policy via unified VI\n",
    "    V_true, Q_true, pi_true = value_iteration_next_state(\n",
    "        mdp[\"T\"], r_next_true, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    "    )\n",
    "\n",
    "    Psi_sa, Psi_s = compute_successor_features_from_q_next_state(\n",
    "        mdp[\"T\"],\n",
    "        mdp[\"Phi\"],\n",
    "        Q_true,\n",
    "        mdp[\"terminal\"],\n",
    "        mdp[\"gamma\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== SUCCESSOR FEATURES DEBUG =====\")\n",
    "    for i, s in enumerate(states):\n",
    "        if mdp[\"terminal\"][i]:\n",
    "            continue\n",
    "        print(f\"State {i:03d} {s}\")\n",
    "        print(f\"  ψ(s)        = {Psi_s[i]}\")\n",
    "        print(f\"  ψ(s)·θ      = {Psi_s[i] @ theta_true:.6f}\")\n",
    "        print(f\"  V_true(s)   = {V_true[i]:.6f}\")\n",
    "\n",
    "    # demos\n",
    "    demos = generate_state_action_demos(states, pi_true, mdp[\"terminal\"])\n",
    "\n",
    "    constraints = constraints_from_demos_next_state(\n",
    "    demos=demos,\n",
    "    Psi_sa=Psi_sa,\n",
    "    terminal_mask=mdp[\"terminal\"],\n",
    "    normalize=True,\n",
    ")\n",
    "    print(\"\\n===== CONSTRAINTS DEBUG =====\")\n",
    "    print(\"Num constraints:\", len(constraints))\n",
    "    print(\"Dim:\", constraints[0].shape if constraints else None)\n",
    "    constraints_pruned = remove_redundant_constraints(\n",
    "    constraints,\n",
    "    epsilon=1e-4,\n",
    ")\n",
    "\n",
    "    for i, c in enumerate(constraints[:5]):\n",
    "        print(f\"c[{i}] = {c}\")\n",
    "\n",
    "    print(\"\\n===== CONSTRAINT PRUNING =====\")\n",
    "    print(\"Original constraints :\", len(constraints))\n",
    "    print(\"After pruning        :\", len(constraints_pruned))\n",
    "\n",
    "\n",
    "    # BIRL\n",
    "    birl = DemoOnlyBIRL(mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], demos, beta=1.0)\n",
    "    theta_hat = birl.run(num_samples=1000, step_size=0.25, seed=0)\n",
    "\n",
    "    # LEARNED policy via the SAME unified VI\n",
    "    r_next_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "    V_hat, Q_hat, pi_hat = value_iteration_next_state(\n",
    "        mdp[\"T\"], r_next_hat, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    "    )\n",
    "\n",
    "    # EVD (policy eval uses unified next-state policy eval)\n",
    "    evd = expected_value_difference(mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_hat, mdp[\"terminal\"])\n",
    "\n",
    "    print(\"\\n===== BIRL RESULTS (SYMBOLIC-FREE) =====\")\n",
    "    print(\"FEATURE_SET :\", FEATURE_SET)\n",
    "    print(\"True θ      :\", theta_true)\n",
    "    print(\"Learned θ   :\", l2_normalize(theta_hat))\n",
    "    print(\"EVD         :\", evd)\n",
    "\n",
    "    # Debug: print V/Q/demos from the SAME VI output\n",
    "    # debug_print_values_and_demos_from_tabular(\n",
    "    #     states=states,\n",
    "    #     V=V_true,\n",
    "    #     Q=Q_true,\n",
    "    #     pi=pi_true,\n",
    "    #     wall_mask=wall_mask,\n",
    "    #     goal_yx=goal_yx,\n",
    "    #     lava_mask=lava_mask,\n",
    "    # )\n",
    "\n",
    "    demos = [(i, pi_true[i]) for i in range(len(states)) if not mdp[\"terminal\"][i]]\n",
    "\n",
    "    constraints_per_atom = []\n",
    "    for s, a in demos:\n",
    "        psi_star = Psi_sa[s, a]\n",
    "        atom_constraints = []\n",
    "        for a2 in range(Psi_sa.shape[1]):\n",
    "            if a2 != a:\n",
    "                diff = psi_star - Psi_sa[s, a2]\n",
    "                if np.linalg.norm(diff) > 1e-12:\n",
    "                    atom_constraints.append(diff)\n",
    "        constraints_per_atom.append(atom_constraints)\n",
    "\n",
    "    scot_demos, _ = scot_greedy_single_env(atoms=demos, constraints_per_atom=constraints_per_atom)\n",
    "\n",
    "    birl_scot = DemoOnlyBIRL(\n",
    "        mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], scot_demos\n",
    "    )\n",
    "    theta_scot = birl_scot.run()\n",
    "\n",
    "    r_scot = mdp[\"Phi\"] @ l2_normalize(theta_scot)\n",
    "    _, _, pi_scot = value_iteration_next_state(\n",
    "        mdp[\"T\"], r_scot, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    "    )\n",
    "\n",
    "    evd_scot = expected_value_difference(\n",
    "        mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_scot, mdp[\"terminal\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== SCOT + BIRL RESULTS =====\")\n",
    "    print(\"True θ      :\", theta_true)\n",
    "    print(\"SCOT θ̂     :\", l2_normalize(theta_scot))\n",
    "    print(\"Num demos   :\", len(demos))\n",
    "    print(\"SCOT demos  :\", len(scot_demos))\n",
    "    print(\"SCOT EVD    :\", evd_scot)\n",
    "\n",
    "\n",
    "    # Optional manual control\n",
    "    # env = SimpleEnv(render_mode=\"human\")\n",
    "    # env.reset(seed=0)\n",
    "    # manual = ManualControlStateReward(env, wall_mask, lava_mask, lava_cells, goal_yx, size)\n",
    "    # manual.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    birl_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=\"human\")\n",
    "env.reset(seed=0)\n",
    "\n",
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(env)\n",
    "\n",
    "print(\"=== ENV INFO ===\")\n",
    "print(\"Grid size:\", size)\n",
    "print(\"Goal (y,x):\", goal_yx)\n",
    "print(\"FEATURE_SET:\", FEATURE_SET)\n",
    "print(\"Reward weights (normalized):\", l2_normalize(W_MAP[FEATURE_SET]))\n",
    "print(\"================\\n\")\n",
    "\n",
    "manual = ManualControlStateReward(\n",
    "    env=env,\n",
    "    wall_mask=wall_mask,\n",
    "    lava_mask=lava_mask,\n",
    "    lava_cells=lava_cells,\n",
    "    goal_yx=goal_yx,\n",
    "    size=size,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Controls:\")\n",
    "print(\"  LEFT  = turn left\")\n",
    "print(\"  RIGHT = turn right\")\n",
    "print(\"  UP    = move forward\")\n",
    "print(\"  ESC   = quit\\n\")\n",
    "manual.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c5cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf5a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
