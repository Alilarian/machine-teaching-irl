{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ae0d00",
   "metadata": {},
   "source": [
    "## Manually control env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4c6e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pressed up\n",
      "step=1, reward=0.00\n",
      "pressed up\n",
      "step=2, reward=0.00\n",
      "pressed right\n",
      "step=3, reward=0.00\n",
      "pressed up\n",
      "step=4, reward=0.00\n",
      "pressed up\n",
      "step=5, reward=0.00\n",
      "pressed right\n",
      "step=6, reward=0.00\n",
      "pressed right\n",
      "step=7, reward=0.00\n",
      "pressed right\n",
      "step=8, reward=0.00\n",
      "pressed up\n",
      "step=9, reward=0.00\n",
      "pressed up\n",
      "step=10, reward=0.00\n",
      "pressed up\n",
      "step=11, reward=0.00\n",
      "pressed up\n",
      "step=12, reward=0.00\n",
      "pressed up\n",
      "step=13, reward=0.00\n",
      "pressed up\n",
      "step=14, reward=0.00\n",
      "pressed up\n",
      "step=15, reward=0.00\n",
      "pressed right\n",
      "step=16, reward=0.00\n",
      "pressed up\n",
      "step=17, reward=0.00\n",
      "pressed up\n",
      "step=18, reward=0.00\n",
      "pressed up\n",
      "step=19, reward=0.00\n",
      "pressed up\n",
      "step=20, reward=0.00\n",
      "pressed up\n",
      "step=21, reward=0.00\n",
      "pressed up\n",
      "step=22, reward=0.00\n",
      "pressed right\n",
      "step=23, reward=0.00\n",
      "pressed right\n",
      "step=24, reward=0.00\n",
      "pressed right\n",
      "step=25, reward=0.00\n",
      "pressed up\n",
      "step=26, reward=0.00\n",
      "pressed up\n",
      "step=27, reward=0.00\n",
      "pressed up\n",
      "step=28, reward=0.00\n",
      "pressed up\n",
      "step=29, reward=0.00\n",
      "pressed up\n",
      "step=30, reward=0.00\n",
      "pressed up\n",
      "step=31, reward=0.00\n",
      "pressed up\n",
      "step=32, reward=0.00\n",
      "pressed up\n",
      "step=33, reward=0.00\n",
      "pressed up\n",
      "step=34, reward=0.00\n",
      "pressed up\n",
      "step=35, reward=0.00\n",
      "pressed up\n",
      "step=36, reward=0.00\n",
      "pressed up\n",
      "step=37, reward=0.00\n",
      "pressed up\n",
      "step=38, reward=0.00\n",
      "pressed left\n",
      "step=39, reward=0.00\n",
      "pressed up\n",
      "step=40, reward=0.00\n",
      "pressed up\n",
      "step=41, reward=0.00\n",
      "pressed right\n",
      "step=42, reward=0.00\n",
      "pressed up\n",
      "step=43, reward=0.00\n",
      "pressed up\n",
      "step=44, reward=0.00\n",
      "pressed right\n",
      "step=45, reward=0.00\n",
      "pressed up\n",
      "step=46, reward=0.00\n",
      "pressed up\n",
      "step=47, reward=0.91\n",
      "terminated!\n",
      "pressed up\n",
      "step=1, reward=0.00\n",
      "pressed up\n",
      "step=2, reward=0.00\n",
      "pressed up\n",
      "step=3, reward=0.00\n",
      "pressed right\n",
      "step=4, reward=0.00\n",
      "pressed right\n",
      "step=5, reward=0.00\n",
      "pressed up\n",
      "step=6, reward=0.00\n",
      "pressed up\n",
      "step=7, reward=0.00\n",
      "pressed up\n",
      "step=8, reward=0.00\n",
      "pressed up\n",
      "step=9, reward=0.00\n",
      "pressed left\n",
      "step=10, reward=0.00\n",
      "pressed up\n",
      "step=11, reward=0.00\n",
      "pressed up\n",
      "step=12, reward=0.00\n",
      "pressed up\n",
      "step=13, reward=0.00\n",
      "pressed up\n",
      "step=14, reward=0.00\n",
      "pressed up\n",
      "step=15, reward=0.00\n",
      "pressed up\n",
      "step=16, reward=0.00\n",
      "pressed up\n",
      "step=17, reward=0.00\n",
      "pressed right\n",
      "step=18, reward=0.00\n",
      "pressed up\n",
      "step=19, reward=0.00\n",
      "pressed up\n",
      "step=20, reward=0.00\n",
      "pressed up\n",
      "step=21, reward=0.00\n",
      "pressed right\n",
      "step=22, reward=0.00\n",
      "pressed right\n",
      "step=23, reward=0.00\n",
      "pressed up\n",
      "step=24, reward=0.00\n",
      "pressed up\n",
      "step=25, reward=0.00\n",
      "pressed up\n",
      "step=26, reward=0.00\n",
      "pressed up\n",
      "step=27, reward=0.00\n",
      "pressed up\n",
      "step=28, reward=0.00\n",
      "pressed up\n",
      "step=29, reward=0.00\n",
      "pressed up\n",
      "step=30, reward=0.00\n",
      "pressed up\n",
      "step=31, reward=0.00\n",
      "pressed right\n",
      "step=32, reward=0.00\n",
      "pressed up\n",
      "step=33, reward=0.00\n",
      "pressed up\n",
      "step=34, reward=0.00\n",
      "pressed up\n",
      "step=35, reward=0.00\n",
      "pressed right\n",
      "step=36, reward=0.00\n",
      "pressed right\n",
      "step=37, reward=0.00\n",
      "pressed right\n",
      "step=38, reward=0.00\n",
      "pressed up\n",
      "step=39, reward=0.00\n",
      "pressed left\n",
      "step=40, reward=0.00\n",
      "pressed up\n",
      "step=41, reward=0.00\n",
      "pressed right\n",
      "step=42, reward=0.00\n",
      "pressed up\n",
      "step=43, reward=0.92\n",
      "terminated!\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "video system not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     manual\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mrun_manual_crossing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mrun_manual_crossing\u001b[0;34m(size, crossings)\u001b[0m\n\u001b[1;32m     14\u001b[0m env \u001b[38;5;241m=\u001b[39m FullyObsWrapper(env)\n\u001b[1;32m     16\u001b[0m manual \u001b[38;5;241m=\u001b[39m ManualControl(env)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmanual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/Projects/machine-teaching-irl/venv/lib/python3.10/site-packages/minigrid/manual_control.py:29\u001b[0m, in \u001b[0;36mManualControl.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mQUIT:\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31merror\u001b[0m: video system not initialized"
     ]
    }
   ],
   "source": [
    "#import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.wrappers import FullyObsWrapper\n",
    "\n",
    "def run_manual_crossing(size=9, crossings=1):\n",
    "    env = gym.make(\n",
    "        #f\"MiniGrid-CrossingS{size}N{crossings}-v0\",\n",
    "        \"MiniGrid-SimpleCrossingS11N5-v0\",\n",
    "        render_mode=\"human\"\n",
    "    )\n",
    "\n",
    "    # Optional: full observability (recommended for debugging)\n",
    "    env = FullyObsWrapper(env)\n",
    "\n",
    "    manual = ManualControl(env)\n",
    "    manual.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_manual_crossing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6d67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils import remove_redundant_constraints\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.mission import MissionSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14332a3f",
   "metadata": {},
   "source": [
    "### Set cover algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17db8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_key(v, *, normalize=True, round_decimals=12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = np.linalg.norm(v)\n",
    "    if n == 0.0 or not np.isfinite(n):\n",
    "        return (\"ZERO\",)\n",
    "    if normalize:\n",
    "        v = v / n\n",
    "    return tuple(np.round(v, round_decimals))\n",
    "\n",
    "\n",
    "def scot_greedy_single_env(\n",
    "    *,\n",
    "    atoms,               # list of atoms (e.g., demo indices or (s,a) pairs)\n",
    "    constraints_per_atom,# list[list[np.ndarray]]\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal SCOT greedy set cover for ONE environment.\n",
    "\n",
    "    Universe:\n",
    "        unique constraint directions across all atoms\n",
    "\n",
    "    Atom coverage:\n",
    "        atom covers a constraint if it generates that direction\n",
    "\n",
    "    Returns:\n",
    "        chosen_atoms\n",
    "        chosen_constraints (stacked)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Build universe of unique constraints\n",
    "    # --------------------------------------------------\n",
    "    key_to_uid = {}\n",
    "    uid_to_vec = {}\n",
    "    uid = 0\n",
    "\n",
    "    atom_constraint_keys = []\n",
    "\n",
    "    for atom_constraints in constraints_per_atom:\n",
    "        keys = set()\n",
    "        for v in atom_constraints:\n",
    "            k = constraint_key(v,\n",
    "                               normalize=normalize,\n",
    "                               round_decimals=round_decimals)\n",
    "            if k not in key_to_uid:\n",
    "                key_to_uid[k] = uid\n",
    "                uid_to_vec[uid] = v\n",
    "                uid += 1\n",
    "            keys.add(key_to_uid[k])\n",
    "        atom_constraint_keys.append(keys)\n",
    "\n",
    "    universe = set(range(uid))\n",
    "    covered = set()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Greedy set cover\n",
    "    # --------------------------------------------------\n",
    "    chosen_atoms = []\n",
    "    chosen_constraints = []\n",
    "\n",
    "    while covered != universe:\n",
    "        best_gain = 0\n",
    "        best_idx = None\n",
    "        best_new = None\n",
    "\n",
    "        for i, cov in enumerate(atom_constraint_keys):\n",
    "            gain_set = cov - covered\n",
    "            if len(gain_set) > best_gain:\n",
    "                best_gain = len(gain_set)\n",
    "                best_idx = i\n",
    "                best_new = gain_set\n",
    "\n",
    "        if best_idx is None:\n",
    "            break  # cannot cover remaining constraints\n",
    "\n",
    "        chosen_atoms.append(atoms[best_idx])\n",
    "        for uid in best_new:\n",
    "            chosen_constraints.append(uid_to_vec[uid])\n",
    "\n",
    "        covered |= best_new\n",
    "\n",
    "    if chosen_constraints:\n",
    "        chosen_constraints = np.vstack(chosen_constraints)\n",
    "    else:\n",
    "        chosen_constraints = np.zeros((0, atoms[0][0].shape[-1]))\n",
    "\n",
    "    return chosen_atoms, chosen_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29c4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f5a63f1",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96202c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUCCESSOR FEATURES DEBUG =====\n",
      "State 000 (1, 1, 0)\n",
      "  ψ(s)        = [3. 0. 1. 1.]\n",
      "  ψ(s)·θ      = -1.470928\n",
      "  V_true(s)   = -1.470928\n",
      "State 001 (1, 1, 1)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 002 (1, 1, 2)\n",
      "  ψ(s)        = [11.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.926563\n",
      "  V_true(s)   = -1.926563\n",
      "State 003 (1, 1, 3)\n",
      "  ψ(s)        = [6.97 1.   0.99 1.99]\n",
      "  ψ(s)·θ      = -2.117643\n",
      "  V_true(s)   = -2.117643\n",
      "State 008 (1, 3, 0)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 009 (1, 3, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 010 (1, 3, 2)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 011 (1, 3, 3)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 012 (2, 1, 0)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 013 (2, 1, 1)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 014 (2, 1, 2)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 015 (2, 1, 3)\n",
      "  ψ(s)        = [10.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.768611\n",
      "  V_true(s)   = -1.768611\n",
      "State 020 (2, 3, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 021 (2, 3, 1)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 022 (2, 3, 2)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 023 (2, 3, 3)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 024 (3, 1, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 025 (3, 1, 1)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 026 (3, 1, 2)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 027 (3, 1, 3)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 028 (3, 2, 0)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 029 (3, 2, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 030 (3, 2, 2)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 031 (3, 2, 3)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "\n",
      "===== CONSTRAINTS DEBUG =====\n",
      "Num constraints: 47\n",
      "Dim: (4,)\n",
      "\n",
      "===== CONSTRAINT PRUNING =====\n",
      "Original constraints : 47\n",
      "After pruning        : 5\n",
      "\n",
      "===== BIRL RESULTS (SYMBOLIC-FREE) =====\n",
      "FEATURE_SET : L1.3\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "Learned θ   : [-0.54382213 -0.62544988 -0.52946151  0.18094322]\n",
      "EVD         : 0.0013839260312477064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1689414/1598151985.py:548: RuntimeWarning: overflow encountered in exp\n",
      "  ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SCOT + BIRL RESULTS =====\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "SCOT θ̂     : [-0.2145758  -0.45881618 -0.85507385  0.11087675]\n",
      "Num demos   : 24\n",
      "SCOT demos  : 13\n",
      "SCOT EVD    : 0.0\n",
      "\n",
      "===== RANDOM BASELINE RESULTS =====\n",
      "Random EVD mean : 0.6524653538893673\n",
      "Random EVD std  : 2.54457487978724\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Utils\n",
    "# ======================================================\n",
    "def l2_normalize(w, eps=1e-8):\n",
    "    n = np.linalg.norm(w)\n",
    "    return w if n < eps else w / n\n",
    "\n",
    "# ======================================================\n",
    "# Feature extraction (state-based, canonical)\n",
    "# ======================================================\n",
    "\n",
    "DIR_TO_VEC = {\n",
    "    0: (1, 0),   # right\n",
    "    1: (0, 1),   # down\n",
    "    2: (-1, 0),  # left\n",
    "    3: (0, -1),  # up\n",
    "}\n",
    "\n",
    "W_MAP = {\n",
    "    \"L1.2\": np.array([-0.05, -2.0, -0.01]),                       # [dist, on_lava, step]\n",
    "    \"L1.3\": np.array([-0.8, -0.1, -5.0, -0.05]),                  # [dist, lava_ahead, on_lava, step]\n",
    "    \"L2.1\": np.array([-0.05, -0.3, -2.0, -0.01]),                 # [dist, num_lava4, on_lava, step]\n",
    "    \"L2.3\": np.array([-0.05, -0.4, -2.0, -0.01]),                 # [dist, lava_nearby8, on_lava, step]\n",
    "    \"L3.1\": np.array([-0.02, -0.02, -0.5, -2.0, -0.01]),          # [dx, dy, lava_ahead, on_lava, step]\n",
    "    \"L4.1\": np.array([-0.05,  0.10, -2.0, -0.01]),                # [dist, min_dist_lava, on_lava, step]\n",
    "}\n",
    "\n",
    "FEATURE_SET = \"L1.3\"\n",
    "\n",
    "def manhattan(p, q):\n",
    "    return abs(p[0] - q[0]) + abs(p[1] - q[1])\n",
    "\n",
    "def lava_ahead_state(lava_mask: np.ndarray, y: int, x: int, direction: int) -> int:\n",
    "    dx, dy = DIR_TO_VEC[direction]\n",
    "    ny, nx = y + dy, x + dx\n",
    "    if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "        return int(lava_mask[ny, nx])\n",
    "    return 0\n",
    "\n",
    "def on_lava_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    return int(lava_mask[y, x])\n",
    "\n",
    "def num_lava_4_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    cnt = 0\n",
    "    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "        ny, nx = y+dy, x+dx\n",
    "        if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "            cnt += int(lava_mask[ny, nx])\n",
    "    return cnt\n",
    "\n",
    "def lava_nearby_8_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    for dy in [-1,0,1]:\n",
    "        for dx in [-1,0,1]:\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            ny, nx = y+dy, x+dx\n",
    "            if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "                if lava_mask[ny, nx]:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "def min_dist_to_lava_state(lava_cells: np.ndarray, y: int, x: int, size: int) -> float:\n",
    "    if lava_cells.size == 0:\n",
    "        return float(2 * size)\n",
    "    return float(np.min(np.abs(lava_cells[:, 0] - y) + np.abs(lava_cells[:, 1] - x)))\n",
    "\n",
    "def phi_from_state(state, goal_yx, lava_mask, lava_cells, size):\n",
    "    \"\"\"\n",
    "    state = (y, x, dir)\n",
    "    goal_yx = (gy, gx)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "    gy, gx = goal_yx\n",
    "\n",
    "    dist = manhattan((y, x), (gy, gx))\n",
    "    step = 1.0\n",
    "\n",
    "    if FEATURE_SET == \"L1.2\":\n",
    "        return np.array([dist, on_lava_state(lava_mask, y, x), step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L1.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.1\":\n",
    "        return np.array([dist,\n",
    "                         num_lava_4_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_nearby_8_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L3.1\":\n",
    "        dx = gx - x\n",
    "        dy = gy - y\n",
    "        return np.array([dx, dy,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L4.1\":\n",
    "        return np.array([dist,\n",
    "                         min_dist_to_lava_state(lava_cells, y, x, size),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    raise ValueError(f\"Unknown FEATURE_SET: {FEATURE_SET}\")\n",
    "\n",
    "def reward_from_state(state, goal_yx, lava_mask, lava_cells, size, w=None) -> float:\n",
    "    \"\"\"\n",
    "    State-based reward for manual debugging/printing.\n",
    "    If w is None, uses normalized W_MAP[FEATURE_SET].\n",
    "    \"\"\"\n",
    "    phi = phi_from_state(state, goal_yx, lava_mask, lava_cells, size)\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    else:\n",
    "        w = l2_normalize(w)\n",
    "    return float(w @ phi)\n",
    "\n",
    "# ======================================================\n",
    "# Planning dynamics model (left/right/forward)\n",
    "# ======================================================\n",
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "\n",
    "# ======================================================\n",
    "# SimpleEnv (same as your previous; minimal here)\n",
    "# ======================================================\n",
    "mission_space = MissionSpace(mission_func=lambda: \"reach the goal\")\n",
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            width=size,\n",
    "            height=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        from minigrid.core.grid import Grid\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Example layout: you can replace with yours\n",
    "        # Put goal at bottom-right inside the walls\n",
    "        self.put_obj(Goal(), width-2, height-2)\n",
    "\n",
    "        # Optional: lava example\n",
    "        \n",
    "        self.put_obj(Lava(), 2, 2)\n",
    "        self.put_obj(Lava(), 2, 1)\n",
    "        \n",
    "        self.agent_pos = self.agent_start_pos\n",
    "        self.agent_dir = self.agent_start_dir\n",
    "        self.mission = \"reach the goal\"\n",
    "\n",
    "# ======================================================\n",
    "# Static map extraction\n",
    "# ======================================================\n",
    "def build_static_maps(env: SimpleEnv):\n",
    "    \"\"\"\n",
    "    Returns everything in NumPy (y,x) convention.\n",
    "    \"\"\"\n",
    "    size = env.width\n",
    "    wall_mask = np.zeros((size, size), dtype=bool)\n",
    "    lava_mask = np.zeros((size, size), dtype=bool)\n",
    "    goal_yx = None\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            obj = env.grid.get(x, y)  # MiniGrid uses (x,y)\n",
    "            if obj is None:\n",
    "                continue\n",
    "            if isinstance(obj, Wall):\n",
    "                wall_mask[y, x] = True\n",
    "            elif isinstance(obj, Lava):\n",
    "                lava_mask[y, x] = True\n",
    "            elif isinstance(obj, Goal):\n",
    "                goal_yx = (y, x)\n",
    "\n",
    "    if goal_yx is None:\n",
    "        raise RuntimeError(\"Goal not found in grid\")\n",
    "\n",
    "    lava_cells = np.argwhere(lava_mask)\n",
    "    return size, wall_mask, lava_mask, lava_cells, goal_yx\n",
    "\n",
    "# def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "#     y, x, _ = state\n",
    "#     return (y, x) == goal_yx\n",
    "\n",
    "def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "    y, x, _ = state\n",
    "    return (y, x) == goal_yx or lava_mask[y, x]\n",
    "\n",
    "def step_model(state, action, wall_mask, goal_yx, lava_mask):\n",
    "    \"\"\"\n",
    "    Deterministic transition for planning.\n",
    "    state = (y, x, dir)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "\n",
    "    if is_terminal_state(state, goal_yx, lava_mask):\n",
    "        return state, True\n",
    "\n",
    "    if action == ACT_LEFT:\n",
    "        nstate = (y, x, (direction - 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_RIGHT:\n",
    "        nstate = (y, x, (direction + 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_FORWARD:\n",
    "        dx, dy = DIR_TO_VEC[direction]\n",
    "        ny, nx = y + dy, x + dx\n",
    "\n",
    "        # bounds/wall => no move\n",
    "        if ny < 0 or ny >= wall_mask.shape[0] or nx < 0 or nx >= wall_mask.shape[1]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        if wall_mask[ny, nx]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        nstate = (ny, nx, direction)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "def enumerate_states(size, wall_mask):\n",
    "    \"\"\"\n",
    "    All non-wall cells for all 4 directions.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if wall_mask[y, x]:\n",
    "                continue\n",
    "            for d in range(4):\n",
    "                states.append((y, x, d))\n",
    "    return states\n",
    "\n",
    "def build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Tabular MDP: deterministic T, features Phi(s).\n",
    "    \"\"\"\n",
    "    S = len(states)\n",
    "    A = len(ACTIONS)\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "    terminal_mask = np.zeros(S, dtype=bool)\n",
    "    Phi = np.zeros((S, len(W_MAP[FEATURE_SET])), dtype=float)\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        terminal_mask[i] = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        Phi[i] = phi_from_state(s, goal_yx, lava_mask, lava_cells, size)\n",
    "        for a_idx, a in enumerate(ACTIONS):\n",
    "            sp, _done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            T[i, a_idx, idx_of[sp]] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"idx_of\": idx_of,\n",
    "        \"T\": T,\n",
    "        \"Phi\": Phi,\n",
    "        \"terminal\": terminal_mask,\n",
    "        \"gamma\": gamma,\n",
    "        \"goal_yx\": goal_yx,\n",
    "        \"lava_mask\": lava_mask,\n",
    "        \"wall_mask\": wall_mask,\n",
    "        \"lava_cells\": lava_cells,\n",
    "        \"size\": size,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# ONE CONSISTENT next-state Policy Eval + Value Iteration\n",
    "# ======================================================\n",
    "\n",
    "def policy_evaluation_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a fixed policy with NEXT-state reward:\n",
    "      V(s) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "    Terminal states are kept at V=0 (consistent with your VI done-cutoff).\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)  # 1 if nonterminal, 0 if terminal\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "            a = int(policy[s])\n",
    "            v_new = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def value_iteration_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    "):\n",
    "    \"\"\"\n",
    "    NEXT-state reward value iteration:\n",
    "      Q(s,a) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "      V(s) = max_a Q(s,a)\n",
    "    Terminal states fixed at V=0.\n",
    "    Returns: V, Q, pi\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "    Q = np.zeros((S, A), dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            # compute Q(s,a) for all a\n",
    "            for a in range(A):\n",
    "                Q[s, a] = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "\n",
    "            v_new = float(np.max(Q[s]))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # greedy policy\n",
    "    pi = np.zeros(S, dtype=int)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            pi[s] = ACT_FORWARD\n",
    "        else:\n",
    "            pi[s] = int(np.argmax(Q[s]))\n",
    "\n",
    "    return V, Q, pi\n",
    "\n",
    "def compute_successor_features_from_q_next_state(\n",
    "    T: np.ndarray,\n",
    "    Phi: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    tol: float = 1e-10,\n",
    "    max_iters: int = 100000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Successor Features with NEXT-STATE (entering) convention, consistent with your code.\n",
    "\n",
    "    Definitions:\n",
    "      π(s)      = argmax_a Q(s,a)\n",
    "      ψ(s)      = E_π [ sum_t γ^t φ(s_{t+1}) | s0 = s ]\n",
    "      ψ(s,a)    = E [ φ(s1) + γ ψ(s1) | s0=s, a0=a ]\n",
    "\n",
    "    Bellman equation:\n",
    "      ψ(s) = Σ_{s'} P_π(s,s') [ φ(s') + γ * 1[~terminal(s')] * ψ(s') ]\n",
    "\n",
    "    Inputs:\n",
    "      T             : (S,A,S) transition matrix\n",
    "      Phi           : (S,D) state feature matrix (φ(s))\n",
    "      Q             : (S,A) Q-values (used to extract greedy policy)\n",
    "      terminal_mask : (S,) boolean\n",
    "      gamma         : discount factor\n",
    "\n",
    "    Returns:\n",
    "      Psi_sa : (S,A,D) successor features for state-action\n",
    "      Psi_s  : (S,D)   successor features for state\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    D = Phi.shape[1]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Greedy policy from Q\n",
    "    # -----------------------------\n",
    "    Pi = np.zeros((S, A), dtype=float)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            continue\n",
    "        Pi[s, np.argmax(Q[s])] = 1.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Policy transition matrix\n",
    "    # P_pi[s,s'] = Σ_a π(a|s) T[s,a,s']\n",
    "    # -----------------------------\n",
    "    P_pi = np.zeros((S, S), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            if Pi[s, a] > 0:\n",
    "                P_pi[s] += Pi[s, a] * T[s, a]\n",
    "\n",
    "        # absorbing fallback (safety)\n",
    "        if P_pi[s].sum() == 0:\n",
    "            P_pi[s, s] = 1.0\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Iterative policy SFs ψ(s)\n",
    "    # -----------------------------\n",
    "    Psi_s = np.zeros((S, D), dtype=float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        Psi_old = Psi_s.copy()\n",
    "\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            exp_phi_next = P_pi[s] @ Phi\n",
    "            exp_psi_next = P_pi[s] @ Psi_old\n",
    "\n",
    "            Psi_s[s] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "        if np.max(np.abs(Psi_s - Psi_old)) < tol:\n",
    "            break\n",
    "\n",
    "    # -----------------------------\n",
    "    # State–action successor features ψ(s,a)\n",
    "    # -----------------------------\n",
    "    Psi_sa = np.zeros((S, A, D), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            p_next = T[s, a]\n",
    "            exp_phi_next = p_next @ Phi\n",
    "            exp_psi_next = p_next @ Psi_s\n",
    "            Psi_sa[s, a] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "    return Psi_sa, Psi_s\n",
    "\n",
    "\n",
    "# Convenience wrapper if you still want \"state-based VI\" signature:\n",
    "def value_iteration_from_env_next_state(\n",
    "    states,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    lava_cells,\n",
    "    size,\n",
    "    gamma=0.99,\n",
    "    theta=1e-8,\n",
    "    max_iters=20000,\n",
    "    w=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds T and r_next from the env step model, then calls the unified tabular VI.\n",
    "    Keeps your original external interface for minimal disruption.\n",
    "    \"\"\"\n",
    "    mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=gamma)\n",
    "    # reward on NEXT state: r_next[s'] = R(s') (state reward evaluated at s')\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    r_next = mdp[\"Phi\"] @ l2_normalize(w)\n",
    "    V, Q, pi = value_iteration_next_state(mdp[\"T\"], r_next, mdp[\"terminal\"], gamma, theta=theta, max_iters=max_iters)\n",
    "    return V, Q, pi, mdp[\"idx_of\"], mdp\n",
    "\n",
    "# ======================================================\n",
    "# Demos\n",
    "# ======================================================\n",
    "def generate_state_action_demos(states, pi, terminal_mask):\n",
    "    demos = []\n",
    "    for i, _s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "        demos.append((i, int(pi[i])))\n",
    "    return demos\n",
    "\n",
    "# ======================================================\n",
    "# BIRL (demo-only, MCMC over theta)\n",
    "# ======================================================\n",
    "class DemoOnlyBIRL:\n",
    "    def __init__(self, T, Phi, gamma, terminal_mask, demos, beta=10.0):\n",
    "        self.T = T\n",
    "        self.Phi = Phi\n",
    "        self.gamma = gamma\n",
    "        self.terminal_mask = terminal_mask\n",
    "        self.demos = demos\n",
    "        self.beta = beta\n",
    "        self.S, self.A, _ = T.shape\n",
    "        self.D = Phi.shape[1]\n",
    "\n",
    "    def _q_values(self, theta):\n",
    "        \"\"\"\n",
    "        Uses the ONE unified value iteration with NEXT-state reward.\n",
    "        Interprets r_next[s'] = Phi[s'] @ theta  (reward of next state).\n",
    "        \"\"\"\n",
    "        theta = l2_normalize(theta)\n",
    "        r_next = self.Phi @ theta\n",
    "        V, Q, _pi = value_iteration_next_state(\n",
    "            self.T, r_next, self.terminal_mask, self.gamma, theta=1e-8, max_iters=200000\n",
    "        )\n",
    "        return Q\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        Q = self._q_values(theta)\n",
    "\n",
    "        ll = 0.0\n",
    "        for s, a in self.demos:\n",
    "            logits = self.beta * Q[s]\n",
    "            ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=200, step_size=0.3, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        theta = l2_normalize(rng.normal(0, 1, self.D))\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            prop = l2_normalize(theta + rng.normal(0, step_size, self.D))\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "class TrajectoryPreferenceBIRL:\n",
    "    \"\"\"\n",
    "    BIRL over trajectory preferences:\n",
    "      (tau_pos, tau_neg)\n",
    "    Works for:\n",
    "      - pairwise preferences\n",
    "      - correction feedback (same start state)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mdp, preferences, beta=5.0):\n",
    "        self.mdp = mdp\n",
    "        self.preferences = preferences\n",
    "        self.beta = beta\n",
    "        self.D = mdp[\"Phi\"].shape[1]\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        ll = 0.0\n",
    "\n",
    "        for tau_pos, tau_neg in self.preferences:\n",
    "            r_pos = trajectory_return(tau_pos, self.mdp, theta)\n",
    "            r_neg = trajectory_return(tau_neg, self.mdp, theta)\n",
    "\n",
    "            diff = r_pos - r_neg\n",
    "            ll += self.beta * diff - np.log1p(np.exp(self.beta * diff))\n",
    "\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=1000, step_size=0.25, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        theta = l2_normalize(rng.normal(0, 1, self.D))\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            prop = l2_normalize(theta + rng.normal(0, step_size, self.D))\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "# ======================================================\n",
    "# Evaluation (EVD) using unified policy eval\n",
    "# ======================================================\n",
    "def expected_value_difference(T, gamma, r_next_true, pi_true, pi_eval, terminal_mask):\n",
    "    \"\"\"\n",
    "    EVD with NEXT-state reward, consistent everywhere:\n",
    "      EVD = mean(V^pi_true) - mean(V^pi_eval)\n",
    "    \"\"\"\n",
    "    V_opt = policy_evaluation_next_state(T, r_next_true, pi_true, terminal_mask, gamma)\n",
    "    V_ev  = policy_evaluation_next_state(T, r_next_true, pi_eval, terminal_mask, gamma)\n",
    "    return float(np.mean(V_opt) - np.mean(V_ev))\n",
    "\n",
    "# ======================================================\n",
    "# Debug printing (state-based)\n",
    "# ======================================================\n",
    "def debug_print_values_and_demos_from_tabular(\n",
    "    states,\n",
    "    V,\n",
    "    Q,\n",
    "    pi,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "):\n",
    "    print(\"\\n===== DEBUG: V(s), Q(s,a), DEMOS =====\")\n",
    "    for i, s in enumerate(states):\n",
    "        y, x, d = s\n",
    "\n",
    "        if wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        term = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        term_str = \"TERMINAL\" if term else \"\"\n",
    "\n",
    "        print(f\"\\nState {i:03d} (y={y}, x={x}, d={d}) [{cell_type}] {term_str}\")\n",
    "        print(f\"  V(s) = {V[i]: .6f}\")\n",
    "\n",
    "        if term:\n",
    "            print(\"  (terminal state, no actions)\")\n",
    "            continue\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            print(f\"  Q(s,a={a}) = {Q[i, a]: .6f}\")\n",
    "\n",
    "        print(f\"  OPTIMAL ACTION (demo) = {pi[i]}\")\n",
    "\n",
    "# ======================================================\n",
    "# Optional manual control (no SymbolicObsWrapper)\n",
    "# ======================================================\n",
    "class ManualControlStateReward(ManualControl):\n",
    "    def __init__(self, env, wall_mask, lava_mask, lava_cells, goal_yx, size):\n",
    "        super().__init__(env)\n",
    "        self.wall_mask = wall_mask\n",
    "        self.lava_mask = lava_mask\n",
    "        self.lava_cells = lava_cells\n",
    "        self.goal_yx = goal_yx\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, action):\n",
    "        _obs, _rew, terminated, truncated, _info = self.env.step(action)\n",
    "\n",
    "        x, y = self.env.unwrapped.agent_pos  # env uses (x,y)\n",
    "        d = int(self.env.unwrapped.agent_dir)\n",
    "        s = (y, x, d)\n",
    "\n",
    "        if self.wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == self.goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif self.lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        phi = phi_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "        R = reward_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "\n",
    "        print(f\"STATE {s} [{cell_type}]  φ(s)={phi}  R(s)={R:.4f}\")\n",
    "\n",
    "        return _obs, 0.0, terminated, truncated, _info\n",
    "\n",
    "def constraints_from_demos_next_state(\n",
    "    demos,\n",
    "    Psi_sa,\n",
    "    terminal_mask=None,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds linear reward constraints from demos using successor features.\n",
    "\n",
    "    Each constraint is:\n",
    "        (ψ(s,a*) - ψ(s,a)) · θ >= 0     for all a != a*\n",
    "\n",
    "    Inputs:\n",
    "      demos         : list of (s, a_star) pairs (state index, optimal action)\n",
    "      Psi_sa        : (S, A, D) successor features (NEXT-state convention)\n",
    "      terminal_mask : optional (S,) boolean mask\n",
    "      normalize     : L2-normalize constraint vectors\n",
    "      tol           : skip near-zero constraints\n",
    "\n",
    "    Returns:\n",
    "      constraints : list of constraint vectors v ∈ R^D\n",
    "    \"\"\"\n",
    "    Psi_sa = np.asarray(Psi_sa)\n",
    "    S, A, D = Psi_sa.shape\n",
    "    constraints = []\n",
    "\n",
    "    if demos is None:\n",
    "        return constraints\n",
    "\n",
    "    for s, a_star in demos:\n",
    "        if s is None or a_star is None:\n",
    "            continue\n",
    "\n",
    "        s = int(s)\n",
    "        a_star = int(a_star)\n",
    "\n",
    "        if not (0 <= s < S) or not (0 <= a_star < A):\n",
    "            continue\n",
    "\n",
    "        if terminal_mask is not None and terminal_mask[s]:\n",
    "            continue\n",
    "\n",
    "        psi_star = Psi_sa[s, a_star]\n",
    "\n",
    "        for a in range(A):\n",
    "            if a == a_star:\n",
    "                continue\n",
    "\n",
    "            diff = psi_star - Psi_sa[s, a]\n",
    "            norm = np.linalg.norm(diff)\n",
    "\n",
    "            if norm <= tol:\n",
    "                continue\n",
    "\n",
    "            constraints.append(diff / norm if normalize else diff)\n",
    "\n",
    "    return constraints\n",
    "\n",
    "def trajectory_feature_sum(traj, mdp):\n",
    "    D = mdp[\"Phi\"].shape[1]\n",
    "    out = np.zeros(D)\n",
    "    g = 1.0\n",
    "\n",
    "    for (_s, _a, sp) in traj:\n",
    "        sp_idx = mdp[\"idx_of\"][sp]\n",
    "        out += g * mdp[\"Phi\"][sp_idx]\n",
    "        g *= mdp[\"gamma\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "def constraints_from_trajectory_preferences(\n",
    "    preferences,\n",
    "    mdp,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "):\n",
    "    constraints = []\n",
    "\n",
    "    for tau_pos, tau_neg in preferences:\n",
    "        phi_pos = trajectory_feature_sum(tau_pos, mdp)\n",
    "        phi_neg = trajectory_feature_sum(tau_neg, mdp)\n",
    "\n",
    "        diff = phi_pos - phi_neg\n",
    "        norm = np.linalg.norm(diff)\n",
    "\n",
    "        if norm <= tol:\n",
    "            continue\n",
    "\n",
    "        constraints.append(diff / norm if normalize else diff)\n",
    "\n",
    "    return constraints\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Main (BIRL experiment)\n",
    "# ======================================================\n",
    "#def birl_main():\n",
    "planning_env = SimpleEnv(render_mode=None)\n",
    "planning_env.reset(seed=0)\n",
    "\n",
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(planning_env)\n",
    "states = enumerate_states(size, wall_mask)\n",
    "\n",
    "# Build tabular MDP once\n",
    "mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99)\n",
    "\n",
    "# TRUE reward as NEXT-state reward vector over state indices\n",
    "theta_true = l2_normalize(W_MAP[FEATURE_SET])\n",
    "r_next_true = mdp[\"Phi\"] @ theta_true\n",
    "\n",
    "# TRUE optimal policy via unified VI\n",
    "V_true, Q_true, pi_true = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_next_true, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    ")\n",
    "\n",
    "Psi_sa, Psi_s = compute_successor_features_from_q_next_state(\n",
    "    mdp[\"T\"],\n",
    "    mdp[\"Phi\"],\n",
    "    Q_true,\n",
    "    mdp[\"terminal\"],\n",
    "    mdp[\"gamma\"],\n",
    ")\n",
    "\n",
    "print(\"\\n===== SUCCESSOR FEATURES DEBUG =====\")\n",
    "for i, s in enumerate(states):\n",
    "    if mdp[\"terminal\"][i]:\n",
    "        continue\n",
    "    print(f\"State {i:03d} {s}\")\n",
    "    print(f\"  ψ(s)        = {Psi_s[i]}\")\n",
    "    print(f\"  ψ(s)·θ      = {Psi_s[i] @ theta_true:.6f}\")\n",
    "    print(f\"  V_true(s)   = {V_true[i]:.6f}\")\n",
    "\n",
    "# demos\n",
    "demos = generate_state_action_demos(states, pi_true, mdp[\"terminal\"])\n",
    "\n",
    "constraints = constraints_from_demos_next_state(\n",
    "demos=demos,\n",
    "Psi_sa=Psi_sa,\n",
    "terminal_mask=mdp[\"terminal\"],\n",
    "normalize=True,\n",
    ")\n",
    "print(\"\\n===== CONSTRAINTS DEBUG =====\")\n",
    "print(\"Num constraints:\", len(constraints))\n",
    "print(\"Dim:\", constraints[0].shape if constraints else None)\n",
    "constraints_pruned = remove_redundant_constraints(\n",
    "constraints,\n",
    "epsilon=1e-4,\n",
    ")\n",
    "\n",
    "# for i, c in enumerate(constraints[:5]):\n",
    "#     print(f\"c[{i}] = {c}\")\n",
    "\n",
    "print(\"\\n===== CONSTRAINT PRUNING =====\")\n",
    "print(\"Original constraints :\", len(constraints))\n",
    "print(\"After pruning        :\", len(constraints_pruned))\n",
    "\n",
    "\n",
    "# BIRL\n",
    "birl = DemoOnlyBIRL(mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], demos, beta=1.0)\n",
    "theta_hat = birl.run(num_samples=1000, step_size=0.25, seed=0)\n",
    "\n",
    "# LEARNED policy via the SAME unified VI\n",
    "r_next_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "V_hat, Q_hat, pi_hat = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_next_hat, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    ")\n",
    "\n",
    "# EVD (policy eval uses unified next-state policy eval)\n",
    "evd = expected_value_difference(mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_hat, mdp[\"terminal\"])\n",
    "\n",
    "print(\"\\n===== BIRL RESULTS (SYMBOLIC-FREE) =====\")\n",
    "print(\"FEATURE_SET :\", FEATURE_SET)\n",
    "print(\"True θ      :\", theta_true)\n",
    "print(\"Learned θ   :\", l2_normalize(theta_hat))\n",
    "print(\"EVD         :\", evd)\n",
    "\n",
    "# Debug: print V/Q/demos from the SAME VI output\n",
    "# debug_print_values_and_demos_from_tabular(\n",
    "#     states=states,\n",
    "#     V=V_true,\n",
    "#     Q=Q_true,\n",
    "#     pi=pi_true,\n",
    "#     wall_mask=wall_mask,\n",
    "#     goal_yx=goal_yx,\n",
    "#     lava_mask=lava_mask,\n",
    "# )\n",
    "\n",
    "demos = [(i, pi_true[i]) for i in range(len(states)) if not mdp[\"terminal\"][i]]\n",
    "\n",
    "constraints_per_atom = []\n",
    "for s, a in demos:\n",
    "    psi_star = Psi_sa[s, a]\n",
    "    atom_constraints = []\n",
    "    for a2 in range(Psi_sa.shape[1]):\n",
    "        if a2 != a:\n",
    "            diff = psi_star - Psi_sa[s, a2]\n",
    "            if np.linalg.norm(diff) > 1e-12:\n",
    "                atom_constraints.append(diff)\n",
    "    constraints_per_atom.append(atom_constraints)\n",
    "\n",
    "scot_demos, _ = scot_greedy_single_env(atoms=demos, constraints_per_atom=constraints_per_atom)\n",
    "\n",
    "birl_scot = DemoOnlyBIRL(\n",
    "    mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], scot_demos\n",
    ")\n",
    "theta_scot = birl_scot.run()\n",
    "\n",
    "r_scot = mdp[\"Phi\"] @ l2_normalize(theta_scot)\n",
    "_, _, pi_scot = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_scot, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    ")\n",
    "\n",
    "evd_scot = expected_value_difference(\n",
    "    mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_scot, mdp[\"terminal\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== SCOT + BIRL RESULTS =====\")\n",
    "print(\"True θ      :\", theta_true)\n",
    "print(\"SCOT θ̂     :\", l2_normalize(theta_scot))\n",
    "print(\"Num demos   :\", len(demos))\n",
    "print(\"SCOT demos  :\", len(scot_demos))\n",
    "print(\"SCOT EVD    :\", evd_scot)\n",
    "\n",
    "def evd_from_demos(\n",
    "    demos,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    birl_kwargs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a demo set:\n",
    "      demos = [(s, a), ...]\n",
    "    learn reward via DemoOnlyBIRL, derive policy, compute EVD.\n",
    "    \"\"\"\n",
    "    if birl_kwargs is None:\n",
    "        birl_kwargs = {}\n",
    "\n",
    "    birl = DemoOnlyBIRL(\n",
    "        mdp[\"T\"],\n",
    "        mdp[\"Phi\"],\n",
    "        mdp[\"gamma\"],\n",
    "        mdp[\"terminal\"],\n",
    "        demos,\n",
    "        **birl_kwargs,\n",
    "    )\n",
    "\n",
    "    theta_hat = birl.run()\n",
    "\n",
    "    r_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "    _, _, pi_hat = value_iteration_next_state(\n",
    "        mdp[\"T\"],\n",
    "        r_hat,\n",
    "        mdp[\"terminal\"],\n",
    "        mdp[\"gamma\"],\n",
    "    )\n",
    "\n",
    "    evd = expected_value_difference(\n",
    "        mdp[\"T\"],\n",
    "        mdp[\"gamma\"],\n",
    "        mdp[\"Phi\"] @ theta_true,\n",
    "        pi_true,\n",
    "        pi_hat,\n",
    "        mdp[\"terminal\"],\n",
    "    )\n",
    "\n",
    "    return evd\n",
    "\n",
    "\n",
    "def random_demo_baseline_evd(\n",
    "    full_demos,\n",
    "    num_select,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    "    birl_kwargs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly select `num_select` demos from full_demos,\n",
    "    repeat `num_trials` times, return mean/std EVD.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    evds = []\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        sel = rng.choice(\n",
    "            len(full_demos),\n",
    "            size=num_select,\n",
    "            replace=False,\n",
    "        )\n",
    "        demos_rand = [full_demos[i] for i in sel]\n",
    "\n",
    "        evd = evd_from_demos(\n",
    "            demos_rand,\n",
    "            mdp,\n",
    "            theta_true,\n",
    "            pi_true,\n",
    "            birl_kwargs=birl_kwargs,\n",
    "        )\n",
    "        evds.append(evd)\n",
    "\n",
    "    evds = np.asarray(evds)\n",
    "    return {\n",
    "        \"mean\": float(evds.mean()),\n",
    "        \"std\": float(evds.std()),\n",
    "        \"all\": evds,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# RANDOM BASELINE (same demo budget as SCOT)\n",
    "# ======================================================\n",
    "\n",
    "rand_stats = random_demo_baseline_evd(\n",
    "    full_demos=demos,                 # all possible demos\n",
    "    num_select=len(scot_demos),        # SAME budget\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    pi_true=pi_true,\n",
    "    num_trials=50,                     # repeat\n",
    "    seed=0,\n",
    "    birl_kwargs={\"beta\": 1.0},\n",
    ")\n",
    "\n",
    "print(\"\\n===== RANDOM BASELINE RESULTS =====\")\n",
    "print(\"Random EVD mean :\", rand_stats[\"mean\"])\n",
    "print(\"Random EVD std  :\", rand_stats[\"std\"])\n",
    "\n",
    "\n",
    "# Optional manual control\n",
    "# env = SimpleEnv(render_mode=\"human\")\n",
    "# env.reset(seed=0)\n",
    "# manual = ManualControlStateReward(env, wall_mask, lava_mask, lava_cells, goal_yx, size)\n",
    "# manual.start()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     birl_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e8788",
   "metadata": {},
   "source": [
    "### Generating pairwise preference and correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd0a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_random_trajectory(\n",
    "    start_state,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    max_horizon=30,\n",
    "    rng=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Roll out a random trajectory from a fixed start state.\n",
    "    Returns list of (s, a, s_next).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    traj = []\n",
    "    s = start_state\n",
    "\n",
    "    for _ in range(max_horizon):\n",
    "        if is_terminal_state(s, goal_yx, lava_mask):\n",
    "            break\n",
    "\n",
    "        a = rng.choice(ACTIONS)\n",
    "        sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "\n",
    "        traj.append((s, a, sp))\n",
    "        s = sp\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return traj\n",
    "\n",
    "def generate_random_trajectories_from_state(\n",
    "    start_state,\n",
    "    n_trajs,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    max_horizon=30,\n",
    "    seed=0,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return [\n",
    "        rollout_random_trajectory(\n",
    "            start_state,\n",
    "            wall_mask,\n",
    "            goal_yx,\n",
    "            lava_mask,\n",
    "            max_horizon=max_horizon,\n",
    "            rng=rng,\n",
    "        )\n",
    "        for _ in range(n_trajs)\n",
    "    ]\n",
    "\n",
    "def generate_trajectory_pool(\n",
    "    states,\n",
    "    terminal_mask,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    n_trajs_per_state=5,\n",
    "    max_horizon=30,\n",
    "):\n",
    "    pool = []\n",
    "    for i, s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "\n",
    "        trajs = generate_random_trajectories_from_state(\n",
    "            start_state=s,\n",
    "            n_trajs=n_trajs_per_state,\n",
    "            wall_mask=wall_mask,\n",
    "            goal_yx=goal_yx,\n",
    "            lava_mask=lava_mask,\n",
    "            max_horizon=max_horizon,\n",
    "            seed=i,\n",
    "        )\n",
    "        pool.extend(trajs)\n",
    "\n",
    "    return pool\n",
    "\n",
    "def trajectory_return(\n",
    "    traj,\n",
    "    Phi,\n",
    "    theta,\n",
    "    gamma=0.99,\n",
    "):\n",
    "    \"\"\"\n",
    "    traj: list of (s, a, s_next)\n",
    "    \"\"\"\n",
    "    theta = l2_normalize(theta)\n",
    "    ret = 0.0\n",
    "    g = 1.0\n",
    "\n",
    "    for (_s, _a, sp) in traj:\n",
    "        sp_idx = Phi[\"idx_of\"][sp]\n",
    "        r = Phi[\"Phi\"][sp_idx] @ theta\n",
    "        ret += g * r\n",
    "        g *= gamma\n",
    "\n",
    "    return ret\n",
    "\n",
    "def generate_pairwise_preferences(\n",
    "    trajectories,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    gamma=0.99,\n",
    "    n_pairs=1000,\n",
    "    seed=0,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    prefs = []\n",
    "\n",
    "    returns = [\n",
    "        trajectory_return(traj, mdp, theta_true, gamma)\n",
    "        for traj in trajectories\n",
    "    ]\n",
    "\n",
    "    N = len(trajectories)\n",
    "\n",
    "    for _ in range(n_pairs):\n",
    "        i, j = rng.choice(N, size=2, replace=False)\n",
    "\n",
    "        if returns[i] == returns[j]:\n",
    "            continue\n",
    "\n",
    "        if returns[i] > returns[j]:\n",
    "            prefs.append((trajectories[i], trajectories[j]))\n",
    "        else:\n",
    "            prefs.append((trajectories[j], trajectories[i]))\n",
    "\n",
    "    return prefs\n",
    "\n",
    "def simulate_correction_one(\n",
    "    traj,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    num_random_trajs=10,\n",
    "    max_horizon=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given an existing trajectory τ, attempt to find a better trajectory\n",
    "    starting from the SAME start state.\n",
    "\n",
    "    Returns:\n",
    "        (tau_improved, tau_original)\n",
    "        or None if no improvement found\n",
    "    \"\"\"\n",
    "    start_state = traj[0][0]\n",
    "\n",
    "    # use original length unless overridden\n",
    "    horizon = len(traj) if max_horizon is None else max_horizon\n",
    "\n",
    "    original_return = trajectory_return(traj, mdp, theta_true)\n",
    "    best_traj = traj\n",
    "    best_return = original_return\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for _ in range(num_random_trajs):\n",
    "        new_traj = rollout_random_trajectory(\n",
    "            start_state=start_state,\n",
    "            wall_mask=mdp[\"wall_mask\"],\n",
    "            goal_yx=mdp[\"goal_yx\"],\n",
    "            lava_mask=mdp[\"lava_mask\"],\n",
    "            max_horizon=horizon,\n",
    "            rng=rng,\n",
    "        )\n",
    "\n",
    "        if len(new_traj) == 0:\n",
    "            continue\n",
    "\n",
    "        new_return = trajectory_return(new_traj, mdp, theta_true)\n",
    "\n",
    "        if new_return > best_return:\n",
    "            best_return = new_return\n",
    "            best_traj = new_traj\n",
    "\n",
    "    if best_traj is traj:\n",
    "        return None  # no improvement found\n",
    "\n",
    "    return (best_traj, traj)\n",
    "\n",
    "def generate_correction_feedback(\n",
    "    trajectories,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    num_random_trajs=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate correction (improvement) feedback:\n",
    "    (tau_improved ≻ tau_original), same start state.\n",
    "    \"\"\"\n",
    "    corrections = []\n",
    "\n",
    "    for traj in trajectories:\n",
    "        if len(traj) == 0:\n",
    "            continue\n",
    "\n",
    "        corr = simulate_correction_one(\n",
    "            traj=traj,\n",
    "            mdp=mdp,\n",
    "            theta_true=theta_true,\n",
    "            num_random_trajs=num_random_trajs,\n",
    "        )\n",
    "\n",
    "        if corr is not None:\n",
    "            corrections.append(corr)\n",
    "\n",
    "    return corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94d86c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num pairwise prefs  : 2988\n",
      "Num correction prefs: 1095\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Generate random trajectory pool\n",
    "# --------------------------------------------------\n",
    "traj_pool = generate_trajectory_pool(\n",
    "    states=states,\n",
    "    terminal_mask=mdp[\"terminal\"],\n",
    "    wall_mask=mdp[\"wall_mask\"],\n",
    "    goal_yx=mdp[\"goal_yx\"],\n",
    "    lava_mask=mdp[\"lava_mask\"],\n",
    "    n_trajs_per_state=50,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Pairwise preferences\n",
    "# --------------------------------------------------\n",
    "pairwise_prefs = generate_pairwise_preferences(\n",
    "    trajectories=traj_pool,\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    n_pairs=3000,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Correction feedback\n",
    "# --------------------------------------------------\n",
    "correction_prefs = generate_correction_feedback(\n",
    "    trajectories=traj_pool,\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    num_random_trajs=20,\n",
    ")\n",
    "\n",
    "print(\"Num pairwise prefs  :\", len(pairwise_prefs))\n",
    "print(\"Num correction prefs:\", len(correction_prefs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf15032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FEEDBACK COMPARISON RESULTS =====\n",
      "True θ         : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "Pairwise θ̂    : [-0.22482072 -0.08180079 -0.97085573  0.01426253]\n",
      "Correction θ̂  : [-0.21189942 -0.27807927 -0.83803472 -0.41888944]\n",
      "--------------------------------------\n",
      "EVD Pairwise   : 0.0013839260312477064\n",
      "EVD Correction : 0.0466995967129622\n"
     ]
    }
   ],
   "source": [
    "birl_pairwise = TrajectoryPreferenceBIRL(\n",
    "    mdp=mdp,\n",
    "    preferences=pairwise_prefs,\n",
    "    beta=5.0,\n",
    ")\n",
    "\n",
    "theta_pairwise = birl_pairwise.run()\n",
    "\n",
    "birl_correction = TrajectoryPreferenceBIRL(\n",
    "    mdp=mdp,\n",
    "    preferences=correction_prefs,\n",
    "    beta=5.0,\n",
    ")\n",
    "\n",
    "theta_correction = birl_correction.run()\n",
    "\n",
    "# TRUE optimal policy already computed: pi_true\n",
    "\n",
    "# Pairwise-learned policy\n",
    "r_pairwise = mdp[\"Phi\"] @ l2_normalize(theta_pairwise)\n",
    "_, _, pi_pairwise = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_pairwise, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    ")\n",
    "\n",
    "evd_pairwise = expected_value_difference(\n",
    "    mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_pairwise, mdp[\"terminal\"]\n",
    ")\n",
    "\n",
    "# Correction-learned policy\n",
    "r_correction = mdp[\"Phi\"] @ l2_normalize(theta_correction)\n",
    "_, _, pi_correction = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_correction, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    ")\n",
    "\n",
    "evd_correction = expected_value_difference(\n",
    "    mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_correction, mdp[\"terminal\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== FEEDBACK COMPARISON RESULTS =====\")\n",
    "print(\"True θ         :\", theta_true)\n",
    "print(\"Pairwise θ̂    :\", l2_normalize(theta_pairwise))\n",
    "print(\"Correction θ̂  :\", l2_normalize(theta_correction))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"EVD Pairwise   :\", evd_pairwise)\n",
    "print(\"EVD Correction :\", evd_correction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7561f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PAIRWISE: SCOT vs RANDOM =====\n",
      "Total pairwise prefs : 2988\n",
      "SCOT prefs           : 2708\n",
      "-----------------------------------\n",
      "SCOT EVD             : 0.001384\n",
      "Random mean EVD      : 0.001398\n",
      "Random std EVD       : 0.000269\n",
      "\n",
      "===== CORRECTION: SCOT vs RANDOM =====\n",
      "Total correction feedback : 1095\n",
      "SCOT feedback             : 761\n",
      "------------------------------------\n",
      "SCOT EVD                  : 0.046700\n",
      "Random mean EVD           : 0.020417\n",
      "Random std EVD            : 0.022366\n"
     ]
    }
   ],
   "source": [
    "def preference_atoms_with_constraints(\n",
    "    preferences,\n",
    "    mdp,\n",
    "    normalize=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Each preference is an atom with exactly one constraint:\n",
    "        Φ(τ⁺) − Φ(τ⁻)\n",
    "    \"\"\"\n",
    "    atoms = []\n",
    "    constraints_per_atom = []\n",
    "\n",
    "    for pref in preferences:\n",
    "        cons = constraints_from_trajectory_preferences(\n",
    "            [pref],\n",
    "            mdp,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        if len(cons) == 0:\n",
    "            continue\n",
    "\n",
    "        atoms.append(pref)\n",
    "        constraints_per_atom.append(cons)\n",
    "\n",
    "    return atoms, constraints_per_atom\n",
    "\n",
    "def scot_on_preferences(\n",
    "    preferences,\n",
    "    mdp,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs SCOT selection on trajectory preferences.\n",
    "    Returns selected subset of preferences.\n",
    "    \"\"\"\n",
    "    atoms, constraints_per_atom = preference_atoms_with_constraints(\n",
    "        preferences,\n",
    "        mdp,\n",
    "        normalize=True,\n",
    "    )\n",
    "\n",
    "    scot_atoms, _ = scot_greedy_single_env(\n",
    "        atoms=atoms,\n",
    "        constraints_per_atom=constraints_per_atom,\n",
    "    )\n",
    "\n",
    "    return scot_atoms\n",
    "\n",
    "def evd_from_preferences(\n",
    "    preferences,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    beta=5.0,\n",
    "):\n",
    "    birl = TrajectoryPreferenceBIRL(\n",
    "        mdp=mdp,\n",
    "        preferences=preferences,\n",
    "        beta=beta,\n",
    "    )\n",
    "\n",
    "    theta_hat = birl.run()\n",
    "\n",
    "    r_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "    _, _, pi_hat = value_iteration_next_state(\n",
    "        mdp[\"T\"],\n",
    "        r_hat,\n",
    "        mdp[\"terminal\"],\n",
    "        mdp[\"gamma\"],\n",
    "    )\n",
    "\n",
    "    evd = expected_value_difference(\n",
    "        mdp[\"T\"],\n",
    "        mdp[\"gamma\"],\n",
    "        mdp[\"Phi\"] @ theta_true,\n",
    "        pi_true,\n",
    "        pi_hat,\n",
    "        mdp[\"terminal\"],\n",
    "    )\n",
    "\n",
    "    return theta_hat, evd\n",
    "\n",
    "def random_preference_baseline_evd(\n",
    "    full_preferences,\n",
    "    num_select,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    "    beta=5.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly sample `num_select` preferences multiple times.\n",
    "    Returns mean/std EVD.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    evds = []\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        idx = rng.choice(\n",
    "            len(full_preferences),\n",
    "            size=num_select,\n",
    "            replace=False,\n",
    "        )\n",
    "        prefs_rand = [full_preferences[i] for i in idx]\n",
    "\n",
    "        _, evd = evd_from_preferences(\n",
    "            prefs_rand,\n",
    "            mdp,\n",
    "            theta_true,\n",
    "            pi_true,\n",
    "            beta=beta,\n",
    "        )\n",
    "        evds.append(evd)\n",
    "\n",
    "    evds = np.asarray(evds)\n",
    "    return {\n",
    "        \"mean\": float(evds.mean()),\n",
    "        \"std\": float(evds.std()),\n",
    "        \"all\": evds,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PAIRWISE PREFERENCES\n",
    "# =========================\n",
    "\n",
    "\n",
    "# SCOT selection\n",
    "pairwise_scot = scot_on_preferences(pairwise_prefs, mdp)\n",
    "\n",
    "theta_pw_scot, evd_pw_scot = evd_from_preferences(\n",
    "    pairwise_scot,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    ")\n",
    "\n",
    "# RANDOM baseline (same budget, repeated)\n",
    "rand_pw_stats = random_preference_baseline_evd(\n",
    "    full_preferences=pairwise_prefs,\n",
    "    num_select=len(pairwise_scot),\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    pi_true=pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "print(\"\\n===== PAIRWISE: SCOT vs RANDOM =====\")\n",
    "print(f\"Total pairwise prefs : {len(pairwise_prefs)}\")\n",
    "print(f\"SCOT prefs           : {len(pairwise_scot)}\")\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"SCOT EVD             : {evd_pw_scot:.6f}\")\n",
    "print(f\"Random mean EVD      : {rand_pw_stats['mean']:.6f}\")\n",
    "print(f\"Random std EVD       : {rand_pw_stats['std']:.6f}\")\n",
    "\n",
    "# =========================\n",
    "# CORRECTION FEEDBACK\n",
    "# =========================\n",
    "\n",
    "# SCOT selection\n",
    "correction_scot = scot_on_preferences(correction_prefs, mdp)\n",
    "\n",
    "theta_corr_scot, evd_corr_scot = evd_from_preferences(\n",
    "    correction_scot,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    pi_true,\n",
    ")\n",
    "\n",
    "# RANDOM baseline (same budget, repeated)\n",
    "rand_corr_stats = random_preference_baseline_evd(\n",
    "    full_preferences=correction_prefs,\n",
    "    num_select=len(correction_scot),\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    pi_true=pi_true,\n",
    "    num_trials=50,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "print(\"\\n===== CORRECTION: SCOT vs RANDOM =====\")\n",
    "print(f\"Total correction feedback : {len(correction_prefs)}\")\n",
    "print(f\"SCOT feedback             : {len(correction_scot)}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"SCOT EVD                  : {evd_corr_scot:.6f}\")\n",
    "print(f\"Random mean EVD           : {rand_corr_stats['mean']:.6f}\")\n",
    "print(f\"Random std EVD            : {rand_corr_stats['std']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c703f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_grouped_regret_from_values(\n",
    "    feedback_to_values,\n",
    "    *,\n",
    "    n_envs=None,\n",
    "    mdp_size=None,\n",
    "    random_trials=None,\n",
    "    sort_by_random=False,\n",
    "    show_values=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    feedback_to_values : dict\n",
    "        {\n",
    "            \"feedback_name\": (two_stage_mean, random_mean)\n",
    "        }\n",
    "\n",
    "    n_envs : int or None\n",
    "    mdp_size : int or None\n",
    "    random_trials : int or None\n",
    "\n",
    "    sort_by_random : bool\n",
    "        If True, sort feedback types by random mean regret (ascending).\n",
    "\n",
    "    show_values : bool\n",
    "        If True, print numeric values on top of bars.\n",
    "    \"\"\"\n",
    "\n",
    "    feedback_names = []\n",
    "    two_stage_means = []\n",
    "    random_means = []\n",
    "\n",
    "    # ---------------- Collect values ----------------\n",
    "    for feedback, (two_stage, random_val) in feedback_to_values.items():\n",
    "        feedback_names.append(feedback)\n",
    "        two_stage_means.append(float(two_stage))\n",
    "        random_means.append(float(random_val))\n",
    "\n",
    "    feedback_names = np.array(feedback_names)\n",
    "    two_stage_means = np.array(two_stage_means)\n",
    "    random_means = np.array(random_means)\n",
    "\n",
    "    # ---------------- Optional sorting ----------------\n",
    "    if sort_by_random:\n",
    "        order = np.argsort(random_means)\n",
    "        feedback_names = feedback_names[order]\n",
    "        two_stage_means = two_stage_means[order]\n",
    "        random_means = random_means[order]\n",
    "\n",
    "    # ---------------- Plot ----------------\n",
    "    x = np.arange(len(feedback_names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    bars_scot = plt.bar(\n",
    "        x - width / 2,\n",
    "        two_stage_means,\n",
    "        width,\n",
    "        label=\"Two-Stage SCOT\",\n",
    "    )\n",
    "\n",
    "    bars_random = plt.bar(\n",
    "        x + width / 2,\n",
    "        random_means,\n",
    "        width,\n",
    "        label=\"Random Selection\",\n",
    "    )\n",
    "\n",
    "    plt.xticks(x, feedback_names)\n",
    "    plt.xlabel(\"Feedback Type\")\n",
    "    plt.ylabel(\"Mean Regret\")\n",
    "\n",
    "    title_parts = [\"Mean Regret Comparison\"]\n",
    "    if n_envs is not None:\n",
    "        title_parts.append(f\"{n_envs} MDPs\")\n",
    "    if mdp_size is not None:\n",
    "        title_parts.append(f\"Grid {mdp_size}×{mdp_size}\")\n",
    "    if random_trials is not None:\n",
    "        title_parts.append(f\"Random Trials = {random_trials}\")\n",
    "\n",
    "    plt.title(\" | \".join(title_parts))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ---------------- Value labels ----------------\n",
    "    if show_values:\n",
    "        def annotate(bars):\n",
    "            for bar in bars:\n",
    "                h = bar.get_height()\n",
    "                plt.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,\n",
    "                    h,\n",
    "                    f\"{h:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=9,\n",
    "                )\n",
    "\n",
    "        annotate(bars_scot)\n",
    "        annotate(bars_random)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "feedback_to_values = {\n",
    "    \"demo\":      (0, 0.6),\n",
    "    \"pairwise\":  (0.001, 0.001),\n",
    "    \"correction\":(0.02, 0.04),\n",
    "}\n",
    "\n",
    "plot_grouped_regret_from_values(\n",
    "    feedback_to_values,\n",
    "    n_envs=1,\n",
    "    mdp_size=3,\n",
    "    #random_trials=1000,\n",
    "    sort_by_random=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b8c1d",
   "metadata": {},
   "source": [
    "### Working with env API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=\"human\")\n",
    "env.reset(seed=0)\n",
    "\n",
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(env)\n",
    "\n",
    "print(\"=== ENV INFO ===\")\n",
    "print(\"Grid size:\", size)\n",
    "print(\"Goal (y,x):\", goal_yx)\n",
    "print(\"FEATURE_SET:\", FEATURE_SET)\n",
    "print(\"Reward weights (normalized):\", l2_normalize(W_MAP[FEATURE_SET]))\n",
    "print(\"================\\n\")\n",
    "\n",
    "manual = ManualControlStateReward(\n",
    "    env=env,\n",
    "    wall_mask=wall_mask,\n",
    "    lava_mask=lava_mask,\n",
    "    lava_cells=lava_cells,\n",
    "    goal_yx=goal_yx,\n",
    "    size=size,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Controls:\")\n",
    "print(\"  LEFT  = turn left\")\n",
    "print(\"  RIGHT = turn right\")\n",
    "print(\"  UP    = move forward\")\n",
    "print(\"  ESC   = quit\\n\")\n",
    "manual.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938c2e6",
   "metadata": {},
   "source": [
    "### Generating random trajectories and pairwise preferneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c5cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d5d624a",
   "metadata": {},
   "source": [
    "### testing pairwise preference generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659b466",
   "metadata": {},
   "source": [
    "### Correction feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6042cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c772f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
