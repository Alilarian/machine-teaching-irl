{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ae0d00",
   "metadata": {},
   "source": [
    "## Manually control env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6d67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils import remove_redundant_constraints\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.mission import MissionSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14332a3f",
   "metadata": {},
   "source": [
    "### Set cover algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17db8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_key(v, *, normalize=True, round_decimals=12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = np.linalg.norm(v)\n",
    "    if n == 0.0 or not np.isfinite(n):\n",
    "        return (\"ZERO\",)\n",
    "    if normalize:\n",
    "        v = v / n\n",
    "    return tuple(np.round(v, round_decimals))\n",
    "\n",
    "\n",
    "def scot_greedy_single_env(\n",
    "    *,\n",
    "    atoms,               # list of atoms (e.g., demo indices or (s,a) pairs)\n",
    "    constraints_per_atom,# list[list[np.ndarray]]\n",
    "    normalize=True,\n",
    "    round_decimals=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal SCOT greedy set cover for ONE environment.\n",
    "\n",
    "    Universe:\n",
    "        unique constraint directions across all atoms\n",
    "\n",
    "    Atom coverage:\n",
    "        atom covers a constraint if it generates that direction\n",
    "\n",
    "    Returns:\n",
    "        chosen_atoms\n",
    "        chosen_constraints (stacked)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Build universe of unique constraints\n",
    "    # --------------------------------------------------\n",
    "    key_to_uid = {}\n",
    "    uid_to_vec = {}\n",
    "    uid = 0\n",
    "\n",
    "    atom_constraint_keys = []\n",
    "\n",
    "    for atom_constraints in constraints_per_atom:\n",
    "        keys = set()\n",
    "        for v in atom_constraints:\n",
    "            k = constraint_key(v,\n",
    "                               normalize=normalize,\n",
    "                               round_decimals=round_decimals)\n",
    "            if k not in key_to_uid:\n",
    "                key_to_uid[k] = uid\n",
    "                uid_to_vec[uid] = v\n",
    "                uid += 1\n",
    "            keys.add(key_to_uid[k])\n",
    "        atom_constraint_keys.append(keys)\n",
    "\n",
    "    universe = set(range(uid))\n",
    "    covered = set()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Greedy set cover\n",
    "    # --------------------------------------------------\n",
    "    chosen_atoms = []\n",
    "    chosen_constraints = []\n",
    "\n",
    "    while covered != universe:\n",
    "        best_gain = 0\n",
    "        best_idx = None\n",
    "        best_new = None\n",
    "\n",
    "        for i, cov in enumerate(atom_constraint_keys):\n",
    "            gain_set = cov - covered\n",
    "            if len(gain_set) > best_gain:\n",
    "                best_gain = len(gain_set)\n",
    "                best_idx = i\n",
    "                best_new = gain_set\n",
    "\n",
    "        if best_idx is None:\n",
    "            break  # cannot cover remaining constraints\n",
    "\n",
    "        chosen_atoms.append(atoms[best_idx])\n",
    "        for uid in best_new:\n",
    "            chosen_constraints.append(uid_to_vec[uid])\n",
    "\n",
    "        covered |= best_new\n",
    "\n",
    "    if chosen_constraints:\n",
    "        chosen_constraints = np.vstack(chosen_constraints)\n",
    "    else:\n",
    "        chosen_constraints = np.zeros((0, atoms[0][0].shape[-1]))\n",
    "\n",
    "    return chosen_atoms, chosen_constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a63f1",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96202c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUCCESSOR FEATURES DEBUG =====\n",
      "State 000 (1, 1, 0)\n",
      "  ψ(s)        = [3. 0. 1. 1.]\n",
      "  ψ(s)·θ      = -1.470928\n",
      "  V_true(s)   = -1.470928\n",
      "State 001 (1, 1, 1)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 002 (1, 1, 2)\n",
      "  ψ(s)        = [11.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.926563\n",
      "  V_true(s)   = -1.926563\n",
      "State 003 (1, 1, 3)\n",
      "  ψ(s)        = [6.97 1.   0.99 1.99]\n",
      "  ψ(s)·θ      = -2.117643\n",
      "  V_true(s)   = -2.117643\n",
      "State 008 (1, 3, 0)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 009 (1, 3, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 010 (1, 3, 2)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 011 (1, 3, 3)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 012 (2, 1, 0)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 013 (2, 1, 1)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 014 (2, 1, 2)\n",
      "  ψ(s)        = [7.910499   0.         0.         4.90099501]\n",
      "  ψ(s)·θ      = -1.297862\n",
      "  V_true(s)   = -1.297862\n",
      "State 015 (2, 1, 3)\n",
      "  ψ(s)        = [10.83139401  0.          0.          5.85198506]\n",
      "  ψ(s)·θ      = -1.768611\n",
      "  V_true(s)   = -1.768611\n",
      "State 020 (2, 3, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 021 (2, 3, 1)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 022 (2, 3, 2)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 023 (2, 3, 3)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 024 (3, 1, 0)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 025 (3, 1, 1)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 026 (3, 1, 2)\n",
      "  ψ(s)        = [4.9601   0.       0.       3.940399]\n",
      "  ψ(s)·θ      = -0.822357\n",
      "  V_true(s)   = -0.822357\n",
      "State 027 (3, 1, 3)\n",
      "  ψ(s)        = [2.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.501597\n",
      "  V_true(s)   = -0.501597\n",
      "State 028 (3, 2, 0)\n",
      "  ψ(s)        = [0. 0. 0. 1.]\n",
      "  ψ(s)·θ      = -0.009872\n",
      "  V_true(s)   = -0.009872\n",
      "State 029 (3, 2, 1)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "State 030 (3, 2, 2)\n",
      "  ψ(s)        = [1.99   0.     0.     2.9701]\n",
      "  ψ(s)·θ      = -0.343645\n",
      "  V_true(s)   = -0.343645\n",
      "State 031 (3, 2, 3)\n",
      "  ψ(s)        = [1.   0.   0.   1.99]\n",
      "  ψ(s)·θ      = -0.177597\n",
      "  V_true(s)   = -0.177597\n",
      "\n",
      "===== CONSTRAINTS DEBUG =====\n",
      "Num constraints: 47\n",
      "Dim: (4,)\n",
      "c[0] = [-0.96318961 -0.12069893  0.00242617 -0.24019086]\n",
      "c[1] = [-0.87215285  0.          0.09875597 -0.47916247]\n",
      "c[2] = [ 0.2793058  -0.29697618 -0.29400642  0.86449618]\n",
      "c[3] = [-0.97182342  0.          0.         -0.23571006]\n",
      "c[4] = [ 0.2793058  -0.29697618 -0.29400642  0.86449618]\n",
      "\n",
      "===== CONSTRAINT PRUNING =====\n",
      "Original constraints : 47\n",
      "After pruning        : 5\n",
      "\n",
      "===== BIRL RESULTS (SYMBOLIC-FREE) =====\n",
      "FEATURE_SET : L1.3\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "Learned θ   : [-0.54382213 -0.62544988 -0.52946151  0.18094322]\n",
      "EVD         : 0.0013839260312477064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1502352/3487842515.py:548: RuntimeWarning: overflow encountered in exp\n",
      "  ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SCOT + BIRL RESULTS =====\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "SCOT θ̂     : [-0.2145758  -0.45881618 -0.85507385  0.11087675]\n",
      "Num demos   : 24\n",
      "SCOT demos  : 13\n",
      "SCOT EVD    : 0.0\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Utils\n",
    "# ======================================================\n",
    "def l2_normalize(w, eps=1e-8):\n",
    "    n = np.linalg.norm(w)\n",
    "    return w if n < eps else w / n\n",
    "\n",
    "# ======================================================\n",
    "# Feature extraction (state-based, canonical)\n",
    "# ======================================================\n",
    "\n",
    "DIR_TO_VEC = {\n",
    "    0: (1, 0),   # right\n",
    "    1: (0, 1),   # down\n",
    "    2: (-1, 0),  # left\n",
    "    3: (0, -1),  # up\n",
    "}\n",
    "\n",
    "W_MAP = {\n",
    "    \"L1.2\": np.array([-0.05, -2.0, -0.01]),                       # [dist, on_lava, step]\n",
    "    \"L1.3\": np.array([-0.8, -0.1, -5.0, -0.05]),                  # [dist, lava_ahead, on_lava, step]\n",
    "    \"L2.1\": np.array([-0.05, -0.3, -2.0, -0.01]),                 # [dist, num_lava4, on_lava, step]\n",
    "    \"L2.3\": np.array([-0.05, -0.4, -2.0, -0.01]),                 # [dist, lava_nearby8, on_lava, step]\n",
    "    \"L3.1\": np.array([-0.02, -0.02, -0.5, -2.0, -0.01]),          # [dx, dy, lava_ahead, on_lava, step]\n",
    "    \"L4.1\": np.array([-0.05,  0.10, -2.0, -0.01]),                # [dist, min_dist_lava, on_lava, step]\n",
    "}\n",
    "\n",
    "FEATURE_SET = \"L1.3\"\n",
    "\n",
    "def manhattan(p, q):\n",
    "    return abs(p[0] - q[0]) + abs(p[1] - q[1])\n",
    "\n",
    "def lava_ahead_state(lava_mask: np.ndarray, y: int, x: int, direction: int) -> int:\n",
    "    dx, dy = DIR_TO_VEC[direction]\n",
    "    ny, nx = y + dy, x + dx\n",
    "    if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "        return int(lava_mask[ny, nx])\n",
    "    return 0\n",
    "\n",
    "def on_lava_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    return int(lava_mask[y, x])\n",
    "\n",
    "def num_lava_4_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    cnt = 0\n",
    "    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "        ny, nx = y+dy, x+dx\n",
    "        if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "            cnt += int(lava_mask[ny, nx])\n",
    "    return cnt\n",
    "\n",
    "def lava_nearby_8_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    for dy in [-1,0,1]:\n",
    "        for dx in [-1,0,1]:\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            ny, nx = y+dy, x+dx\n",
    "            if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "                if lava_mask[ny, nx]:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "def min_dist_to_lava_state(lava_cells: np.ndarray, y: int, x: int, size: int) -> float:\n",
    "    if lava_cells.size == 0:\n",
    "        return float(2 * size)\n",
    "    return float(np.min(np.abs(lava_cells[:, 0] - y) + np.abs(lava_cells[:, 1] - x)))\n",
    "\n",
    "def phi_from_state(state, goal_yx, lava_mask, lava_cells, size):\n",
    "    \"\"\"\n",
    "    state = (y, x, dir)\n",
    "    goal_yx = (gy, gx)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "    gy, gx = goal_yx\n",
    "\n",
    "    dist = manhattan((y, x), (gy, gx))\n",
    "    step = 1.0\n",
    "\n",
    "    if FEATURE_SET == \"L1.2\":\n",
    "        return np.array([dist, on_lava_state(lava_mask, y, x), step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L1.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.1\":\n",
    "        return np.array([dist,\n",
    "                         num_lava_4_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_nearby_8_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L3.1\":\n",
    "        dx = gx - x\n",
    "        dy = gy - y\n",
    "        return np.array([dx, dy,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L4.1\":\n",
    "        return np.array([dist,\n",
    "                         min_dist_to_lava_state(lava_cells, y, x, size),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    raise ValueError(f\"Unknown FEATURE_SET: {FEATURE_SET}\")\n",
    "\n",
    "def reward_from_state(state, goal_yx, lava_mask, lava_cells, size, w=None) -> float:\n",
    "    \"\"\"\n",
    "    State-based reward for manual debugging/printing.\n",
    "    If w is None, uses normalized W_MAP[FEATURE_SET].\n",
    "    \"\"\"\n",
    "    phi = phi_from_state(state, goal_yx, lava_mask, lava_cells, size)\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    else:\n",
    "        w = l2_normalize(w)\n",
    "    return float(w @ phi)\n",
    "\n",
    "# ======================================================\n",
    "# Planning dynamics model (left/right/forward)\n",
    "# ======================================================\n",
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "\n",
    "# ======================================================\n",
    "# SimpleEnv (same as your previous; minimal here)\n",
    "# ======================================================\n",
    "mission_space = MissionSpace(mission_func=lambda: \"reach the goal\")\n",
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            width=size,\n",
    "            height=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        from minigrid.core.grid import Grid\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Example layout: you can replace with yours\n",
    "        # Put goal at bottom-right inside the walls\n",
    "        self.put_obj(Goal(), width-2, height-2)\n",
    "\n",
    "        # Optional: lava example\n",
    "        \n",
    "        self.put_obj(Lava(), 2, 2)\n",
    "        self.put_obj(Lava(), 2, 1)\n",
    "        \n",
    "        self.agent_pos = self.agent_start_pos\n",
    "        self.agent_dir = self.agent_start_dir\n",
    "        self.mission = \"reach the goal\"\n",
    "\n",
    "# ======================================================\n",
    "# Static map extraction\n",
    "# ======================================================\n",
    "def build_static_maps(env: SimpleEnv):\n",
    "    \"\"\"\n",
    "    Returns everything in NumPy (y,x) convention.\n",
    "    \"\"\"\n",
    "    size = env.width\n",
    "    wall_mask = np.zeros((size, size), dtype=bool)\n",
    "    lava_mask = np.zeros((size, size), dtype=bool)\n",
    "    goal_yx = None\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            obj = env.grid.get(x, y)  # MiniGrid uses (x,y)\n",
    "            if obj is None:\n",
    "                continue\n",
    "            if isinstance(obj, Wall):\n",
    "                wall_mask[y, x] = True\n",
    "            elif isinstance(obj, Lava):\n",
    "                lava_mask[y, x] = True\n",
    "            elif isinstance(obj, Goal):\n",
    "                goal_yx = (y, x)\n",
    "\n",
    "    if goal_yx is None:\n",
    "        raise RuntimeError(\"Goal not found in grid\")\n",
    "\n",
    "    lava_cells = np.argwhere(lava_mask)\n",
    "    return size, wall_mask, lava_mask, lava_cells, goal_yx\n",
    "\n",
    "# def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "#     y, x, _ = state\n",
    "#     return (y, x) == goal_yx\n",
    "\n",
    "def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "    y, x, _ = state\n",
    "    return (y, x) == goal_yx or lava_mask[y, x]\n",
    "\n",
    "def step_model(state, action, wall_mask, goal_yx, lava_mask):\n",
    "    \"\"\"\n",
    "    Deterministic transition for planning.\n",
    "    state = (y, x, dir)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "\n",
    "    if is_terminal_state(state, goal_yx, lava_mask):\n",
    "        return state, True\n",
    "\n",
    "    if action == ACT_LEFT:\n",
    "        nstate = (y, x, (direction - 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_RIGHT:\n",
    "        nstate = (y, x, (direction + 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_FORWARD:\n",
    "        dx, dy = DIR_TO_VEC[direction]\n",
    "        ny, nx = y + dy, x + dx\n",
    "\n",
    "        # bounds/wall => no move\n",
    "        if ny < 0 or ny >= wall_mask.shape[0] or nx < 0 or nx >= wall_mask.shape[1]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        if wall_mask[ny, nx]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        nstate = (ny, nx, direction)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "def enumerate_states(size, wall_mask):\n",
    "    \"\"\"\n",
    "    All non-wall cells for all 4 directions.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if wall_mask[y, x]:\n",
    "                continue\n",
    "            for d in range(4):\n",
    "                states.append((y, x, d))\n",
    "    return states\n",
    "\n",
    "def build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Tabular MDP: deterministic T, features Phi(s).\n",
    "    \"\"\"\n",
    "    S = len(states)\n",
    "    A = len(ACTIONS)\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "\n",
    "    T = np.zeros((S, A, S), dtype=float)\n",
    "    terminal_mask = np.zeros(S, dtype=bool)\n",
    "    Phi = np.zeros((S, len(W_MAP[FEATURE_SET])), dtype=float)\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        terminal_mask[i] = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        Phi[i] = phi_from_state(s, goal_yx, lava_mask, lava_cells, size)\n",
    "        for a_idx, a in enumerate(ACTIONS):\n",
    "            sp, _done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            T[i, a_idx, idx_of[sp]] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"idx_of\": idx_of,\n",
    "        \"T\": T,\n",
    "        \"Phi\": Phi,\n",
    "        \"terminal\": terminal_mask,\n",
    "        \"gamma\": gamma,\n",
    "        \"goal_yx\": goal_yx,\n",
    "        \"lava_mask\": lava_mask,\n",
    "        \"wall_mask\": wall_mask,\n",
    "        \"lava_cells\": lava_cells,\n",
    "        \"size\": size,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# ONE CONSISTENT next-state Policy Eval + Value Iteration\n",
    "# ======================================================\n",
    "\n",
    "def policy_evaluation_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a fixed policy with NEXT-state reward:\n",
    "      V(s) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "    Terminal states are kept at V=0 (consistent with your VI done-cutoff).\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)  # 1 if nonterminal, 0 if terminal\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "            a = int(policy[s])\n",
    "            v_new = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def value_iteration_next_state(\n",
    "    T: np.ndarray,\n",
    "    r_next: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    theta: float = 1e-8,\n",
    "    max_iters: int = 200000,\n",
    "):\n",
    "    \"\"\"\n",
    "    NEXT-state reward value iteration:\n",
    "      Q(s,a) = Σ_{s'} T[s,a,s'] * ( r_next[s'] + gamma * 1[~terminal(s')] * V(s') )\n",
    "      V(s) = max_a Q(s,a)\n",
    "    Terminal states fixed at V=0.\n",
    "    Returns: V, Q, pi\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    V = np.zeros(S, dtype=float)\n",
    "    Q = np.zeros((S, A), dtype=float)\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            # compute Q(s,a) for all a\n",
    "            for a in range(A):\n",
    "                Q[s, a] = float(np.sum(T[s, a] * (r_next + gamma * (cont * V))))\n",
    "\n",
    "            v_new = float(np.max(Q[s]))\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # greedy policy\n",
    "    pi = np.zeros(S, dtype=int)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            pi[s] = ACT_FORWARD\n",
    "        else:\n",
    "            pi[s] = int(np.argmax(Q[s]))\n",
    "\n",
    "    return V, Q, pi\n",
    "\n",
    "def compute_successor_features_from_q_next_state(\n",
    "    T: np.ndarray,\n",
    "    Phi: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    terminal_mask: np.ndarray,\n",
    "    gamma: float,\n",
    "    tol: float = 1e-10,\n",
    "    max_iters: int = 100000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Successor Features with NEXT-STATE (entering) convention, consistent with your code.\n",
    "\n",
    "    Definitions:\n",
    "      π(s)      = argmax_a Q(s,a)\n",
    "      ψ(s)      = E_π [ sum_t γ^t φ(s_{t+1}) | s0 = s ]\n",
    "      ψ(s,a)    = E [ φ(s1) + γ ψ(s1) | s0=s, a0=a ]\n",
    "\n",
    "    Bellman equation:\n",
    "      ψ(s) = Σ_{s'} P_π(s,s') [ φ(s') + γ * 1[~terminal(s')] * ψ(s') ]\n",
    "\n",
    "    Inputs:\n",
    "      T             : (S,A,S) transition matrix\n",
    "      Phi           : (S,D) state feature matrix (φ(s))\n",
    "      Q             : (S,A) Q-values (used to extract greedy policy)\n",
    "      terminal_mask : (S,) boolean\n",
    "      gamma         : discount factor\n",
    "\n",
    "    Returns:\n",
    "      Psi_sa : (S,A,D) successor features for state-action\n",
    "      Psi_s  : (S,D)   successor features for state\n",
    "    \"\"\"\n",
    "    S, A, S2 = T.shape\n",
    "    assert S == S2\n",
    "    D = Phi.shape[1]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Greedy policy from Q\n",
    "    # -----------------------------\n",
    "    Pi = np.zeros((S, A), dtype=float)\n",
    "    for s in range(S):\n",
    "        if terminal_mask[s]:\n",
    "            continue\n",
    "        Pi[s, np.argmax(Q[s])] = 1.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Policy transition matrix\n",
    "    # P_pi[s,s'] = Σ_a π(a|s) T[s,a,s']\n",
    "    # -----------------------------\n",
    "    P_pi = np.zeros((S, S), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            if Pi[s, a] > 0:\n",
    "                P_pi[s] += Pi[s, a] * T[s, a]\n",
    "\n",
    "        # absorbing fallback (safety)\n",
    "        if P_pi[s].sum() == 0:\n",
    "            P_pi[s, s] = 1.0\n",
    "\n",
    "    cont = (~terminal_mask).astype(float)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Iterative policy SFs ψ(s)\n",
    "    # -----------------------------\n",
    "    Psi_s = np.zeros((S, D), dtype=float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        Psi_old = Psi_s.copy()\n",
    "\n",
    "        for s in range(S):\n",
    "            if terminal_mask[s]:\n",
    "                continue\n",
    "\n",
    "            exp_phi_next = P_pi[s] @ Phi\n",
    "            exp_psi_next = P_pi[s] @ Psi_old\n",
    "\n",
    "            Psi_s[s] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "        if np.max(np.abs(Psi_s - Psi_old)) < tol:\n",
    "            break\n",
    "\n",
    "    # -----------------------------\n",
    "    # State–action successor features ψ(s,a)\n",
    "    # -----------------------------\n",
    "    Psi_sa = np.zeros((S, A, D), dtype=float)\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            p_next = T[s, a]\n",
    "            exp_phi_next = p_next @ Phi\n",
    "            exp_psi_next = p_next @ Psi_s\n",
    "            Psi_sa[s, a] = exp_phi_next + gamma * cont[s] * exp_psi_next\n",
    "\n",
    "    return Psi_sa, Psi_s\n",
    "\n",
    "\n",
    "# Convenience wrapper if you still want \"state-based VI\" signature:\n",
    "def value_iteration_from_env_next_state(\n",
    "    states,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    lava_cells,\n",
    "    size,\n",
    "    gamma=0.99,\n",
    "    theta=1e-8,\n",
    "    max_iters=20000,\n",
    "    w=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds T and r_next from the env step model, then calls the unified tabular VI.\n",
    "    Keeps your original external interface for minimal disruption.\n",
    "    \"\"\"\n",
    "    mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=gamma)\n",
    "    # reward on NEXT state: r_next[s'] = R(s') (state reward evaluated at s')\n",
    "    if w is None:\n",
    "        w = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    r_next = mdp[\"Phi\"] @ l2_normalize(w)\n",
    "    V, Q, pi = value_iteration_next_state(mdp[\"T\"], r_next, mdp[\"terminal\"], gamma, theta=theta, max_iters=max_iters)\n",
    "    return V, Q, pi, mdp[\"idx_of\"], mdp\n",
    "\n",
    "# ======================================================\n",
    "# Demos\n",
    "# ======================================================\n",
    "def generate_state_action_demos(states, pi, terminal_mask):\n",
    "    demos = []\n",
    "    for i, _s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "        demos.append((i, int(pi[i])))\n",
    "    return demos\n",
    "\n",
    "# ======================================================\n",
    "# BIRL (demo-only, MCMC over theta)\n",
    "# ======================================================\n",
    "class DemoOnlyBIRL:\n",
    "    def __init__(self, T, Phi, gamma, terminal_mask, demos, beta=10.0):\n",
    "        self.T = T\n",
    "        self.Phi = Phi\n",
    "        self.gamma = gamma\n",
    "        self.terminal_mask = terminal_mask\n",
    "        self.demos = demos\n",
    "        self.beta = beta\n",
    "        self.S, self.A, _ = T.shape\n",
    "        self.D = Phi.shape[1]\n",
    "\n",
    "    def _q_values(self, theta):\n",
    "        \"\"\"\n",
    "        Uses the ONE unified value iteration with NEXT-state reward.\n",
    "        Interprets r_next[s'] = Phi[s'] @ theta  (reward of next state).\n",
    "        \"\"\"\n",
    "        theta = l2_normalize(theta)\n",
    "        r_next = self.Phi @ theta\n",
    "        V, Q, _pi = value_iteration_next_state(\n",
    "            self.T, r_next, self.terminal_mask, self.gamma, theta=1e-8, max_iters=200000\n",
    "        )\n",
    "        return Q\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        Q = self._q_values(theta)\n",
    "\n",
    "        ll = 0.0\n",
    "        for s, a in self.demos:\n",
    "            logits = self.beta * Q[s]\n",
    "            ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=200, step_size=0.3, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        theta = l2_normalize(rng.normal(0, 1, self.D))\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            prop = l2_normalize(theta + rng.normal(0, step_size, self.D))\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "# ======================================================\n",
    "# Evaluation (EVD) using unified policy eval\n",
    "# ======================================================\n",
    "def expected_value_difference(T, gamma, r_next_true, pi_true, pi_eval, terminal_mask):\n",
    "    \"\"\"\n",
    "    EVD with NEXT-state reward, consistent everywhere:\n",
    "      EVD = mean(V^pi_true) - mean(V^pi_eval)\n",
    "    \"\"\"\n",
    "    V_opt = policy_evaluation_next_state(T, r_next_true, pi_true, terminal_mask, gamma)\n",
    "    V_ev  = policy_evaluation_next_state(T, r_next_true, pi_eval, terminal_mask, gamma)\n",
    "    return float(np.mean(V_opt) - np.mean(V_ev))\n",
    "\n",
    "# ======================================================\n",
    "# Debug printing (state-based)\n",
    "# ======================================================\n",
    "def debug_print_values_and_demos_from_tabular(\n",
    "    states,\n",
    "    V,\n",
    "    Q,\n",
    "    pi,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "):\n",
    "    print(\"\\n===== DEBUG: V(s), Q(s,a), DEMOS =====\")\n",
    "    for i, s in enumerate(states):\n",
    "        y, x, d = s\n",
    "\n",
    "        if wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        term = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        term_str = \"TERMINAL\" if term else \"\"\n",
    "\n",
    "        print(f\"\\nState {i:03d} (y={y}, x={x}, d={d}) [{cell_type}] {term_str}\")\n",
    "        print(f\"  V(s) = {V[i]: .6f}\")\n",
    "\n",
    "        if term:\n",
    "            print(\"  (terminal state, no actions)\")\n",
    "            continue\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            print(f\"  Q(s,a={a}) = {Q[i, a]: .6f}\")\n",
    "\n",
    "        print(f\"  OPTIMAL ACTION (demo) = {pi[i]}\")\n",
    "\n",
    "# ======================================================\n",
    "# Optional manual control (no SymbolicObsWrapper)\n",
    "# ======================================================\n",
    "class ManualControlStateReward(ManualControl):\n",
    "    def __init__(self, env, wall_mask, lava_mask, lava_cells, goal_yx, size):\n",
    "        super().__init__(env)\n",
    "        self.wall_mask = wall_mask\n",
    "        self.lava_mask = lava_mask\n",
    "        self.lava_cells = lava_cells\n",
    "        self.goal_yx = goal_yx\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, action):\n",
    "        _obs, _rew, terminated, truncated, _info = self.env.step(action)\n",
    "\n",
    "        x, y = self.env.unwrapped.agent_pos  # env uses (x,y)\n",
    "        d = int(self.env.unwrapped.agent_dir)\n",
    "        s = (y, x, d)\n",
    "\n",
    "        if self.wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == self.goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif self.lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        phi = phi_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "        R = reward_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "\n",
    "        print(f\"STATE {s} [{cell_type}]  φ(s)={phi}  R(s)={R:.4f}\")\n",
    "\n",
    "        return _obs, 0.0, terminated, truncated, _info\n",
    "\n",
    "def constraints_from_demos_next_state(\n",
    "    demos,\n",
    "    Psi_sa,\n",
    "    terminal_mask=None,\n",
    "    normalize=True,\n",
    "    tol=1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds linear reward constraints from demos using successor features.\n",
    "\n",
    "    Each constraint is:\n",
    "        (ψ(s,a*) - ψ(s,a)) · θ >= 0     for all a != a*\n",
    "\n",
    "    Inputs:\n",
    "      demos         : list of (s, a_star) pairs (state index, optimal action)\n",
    "      Psi_sa        : (S, A, D) successor features (NEXT-state convention)\n",
    "      terminal_mask : optional (S,) boolean mask\n",
    "      normalize     : L2-normalize constraint vectors\n",
    "      tol           : skip near-zero constraints\n",
    "\n",
    "    Returns:\n",
    "      constraints : list of constraint vectors v ∈ R^D\n",
    "    \"\"\"\n",
    "    Psi_sa = np.asarray(Psi_sa)\n",
    "    S, A, D = Psi_sa.shape\n",
    "    constraints = []\n",
    "\n",
    "    if demos is None:\n",
    "        return constraints\n",
    "\n",
    "    for s, a_star in demos:\n",
    "        if s is None or a_star is None:\n",
    "            continue\n",
    "\n",
    "        s = int(s)\n",
    "        a_star = int(a_star)\n",
    "\n",
    "        if not (0 <= s < S) or not (0 <= a_star < A):\n",
    "            continue\n",
    "\n",
    "        if terminal_mask is not None and terminal_mask[s]:\n",
    "            continue\n",
    "\n",
    "        psi_star = Psi_sa[s, a_star]\n",
    "\n",
    "        for a in range(A):\n",
    "            if a == a_star:\n",
    "                continue\n",
    "\n",
    "            diff = psi_star - Psi_sa[s, a]\n",
    "            norm = np.linalg.norm(diff)\n",
    "\n",
    "            if norm <= tol:\n",
    "                continue\n",
    "\n",
    "            constraints.append(diff / norm if normalize else diff)\n",
    "\n",
    "    return constraints\n",
    "\n",
    "# ======================================================\n",
    "# Main (BIRL experiment)\n",
    "# ======================================================\n",
    "#def birl_main():\n",
    "planning_env = SimpleEnv(render_mode=None)\n",
    "planning_env.reset(seed=0)\n",
    "\n",
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(planning_env)\n",
    "states = enumerate_states(size, wall_mask)\n",
    "\n",
    "# Build tabular MDP once\n",
    "mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99)\n",
    "\n",
    "# TRUE reward as NEXT-state reward vector over state indices\n",
    "theta_true = l2_normalize(W_MAP[FEATURE_SET])\n",
    "r_next_true = mdp[\"Phi\"] @ theta_true\n",
    "\n",
    "# TRUE optimal policy via unified VI\n",
    "V_true, Q_true, pi_true = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_next_true, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    ")\n",
    "\n",
    "Psi_sa, Psi_s = compute_successor_features_from_q_next_state(\n",
    "    mdp[\"T\"],\n",
    "    mdp[\"Phi\"],\n",
    "    Q_true,\n",
    "    mdp[\"terminal\"],\n",
    "    mdp[\"gamma\"],\n",
    ")\n",
    "\n",
    "print(\"\\n===== SUCCESSOR FEATURES DEBUG =====\")\n",
    "for i, s in enumerate(states):\n",
    "    if mdp[\"terminal\"][i]:\n",
    "        continue\n",
    "    print(f\"State {i:03d} {s}\")\n",
    "    print(f\"  ψ(s)        = {Psi_s[i]}\")\n",
    "    print(f\"  ψ(s)·θ      = {Psi_s[i] @ theta_true:.6f}\")\n",
    "    print(f\"  V_true(s)   = {V_true[i]:.6f}\")\n",
    "\n",
    "# demos\n",
    "demos = generate_state_action_demos(states, pi_true, mdp[\"terminal\"])\n",
    "\n",
    "constraints = constraints_from_demos_next_state(\n",
    "demos=demos,\n",
    "Psi_sa=Psi_sa,\n",
    "terminal_mask=mdp[\"terminal\"],\n",
    "normalize=True,\n",
    ")\n",
    "print(\"\\n===== CONSTRAINTS DEBUG =====\")\n",
    "print(\"Num constraints:\", len(constraints))\n",
    "print(\"Dim:\", constraints[0].shape if constraints else None)\n",
    "constraints_pruned = remove_redundant_constraints(\n",
    "constraints,\n",
    "epsilon=1e-4,\n",
    ")\n",
    "\n",
    "for i, c in enumerate(constraints[:5]):\n",
    "    print(f\"c[{i}] = {c}\")\n",
    "\n",
    "print(\"\\n===== CONSTRAINT PRUNING =====\")\n",
    "print(\"Original constraints :\", len(constraints))\n",
    "print(\"After pruning        :\", len(constraints_pruned))\n",
    "\n",
    "\n",
    "# BIRL\n",
    "birl = DemoOnlyBIRL(mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], demos, beta=1.0)\n",
    "theta_hat = birl.run(num_samples=1000, step_size=0.25, seed=0)\n",
    "\n",
    "# LEARNED policy via the SAME unified VI\n",
    "r_next_hat = mdp[\"Phi\"] @ l2_normalize(theta_hat)\n",
    "V_hat, Q_hat, pi_hat = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_next_hat, mdp[\"terminal\"], mdp[\"gamma\"], theta=1e-8, max_iters=200000\n",
    ")\n",
    "\n",
    "# EVD (policy eval uses unified next-state policy eval)\n",
    "evd = expected_value_difference(mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_hat, mdp[\"terminal\"])\n",
    "\n",
    "print(\"\\n===== BIRL RESULTS (SYMBOLIC-FREE) =====\")\n",
    "print(\"FEATURE_SET :\", FEATURE_SET)\n",
    "print(\"True θ      :\", theta_true)\n",
    "print(\"Learned θ   :\", l2_normalize(theta_hat))\n",
    "print(\"EVD         :\", evd)\n",
    "\n",
    "# Debug: print V/Q/demos from the SAME VI output\n",
    "# debug_print_values_and_demos_from_tabular(\n",
    "#     states=states,\n",
    "#     V=V_true,\n",
    "#     Q=Q_true,\n",
    "#     pi=pi_true,\n",
    "#     wall_mask=wall_mask,\n",
    "#     goal_yx=goal_yx,\n",
    "#     lava_mask=lava_mask,\n",
    "# )\n",
    "\n",
    "demos = [(i, pi_true[i]) for i in range(len(states)) if not mdp[\"terminal\"][i]]\n",
    "\n",
    "constraints_per_atom = []\n",
    "for s, a in demos:\n",
    "    psi_star = Psi_sa[s, a]\n",
    "    atom_constraints = []\n",
    "    for a2 in range(Psi_sa.shape[1]):\n",
    "        if a2 != a:\n",
    "            diff = psi_star - Psi_sa[s, a2]\n",
    "            if np.linalg.norm(diff) > 1e-12:\n",
    "                atom_constraints.append(diff)\n",
    "    constraints_per_atom.append(atom_constraints)\n",
    "\n",
    "scot_demos, _ = scot_greedy_single_env(atoms=demos, constraints_per_atom=constraints_per_atom)\n",
    "\n",
    "birl_scot = DemoOnlyBIRL(\n",
    "    mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], mdp[\"terminal\"], scot_demos\n",
    ")\n",
    "theta_scot = birl_scot.run()\n",
    "\n",
    "r_scot = mdp[\"Phi\"] @ l2_normalize(theta_scot)\n",
    "_, _, pi_scot = value_iteration_next_state(\n",
    "    mdp[\"T\"], r_scot, mdp[\"terminal\"], mdp[\"gamma\"]\n",
    ")\n",
    "\n",
    "evd_scot = expected_value_difference(\n",
    "    mdp[\"T\"], mdp[\"gamma\"], r_next_true, pi_true, pi_scot, mdp[\"terminal\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== SCOT + BIRL RESULTS =====\")\n",
    "print(\"True θ      :\", theta_true)\n",
    "print(\"SCOT θ̂     :\", l2_normalize(theta_scot))\n",
    "print(\"Num demos   :\", len(demos))\n",
    "print(\"SCOT demos  :\", len(scot_demos))\n",
    "print(\"SCOT EVD    :\", evd_scot)\n",
    "\n",
    "\n",
    "# Optional manual control\n",
    "# env = SimpleEnv(render_mode=\"human\")\n",
    "# env.reset(seed=0)\n",
    "# manual = ManualControlStateReward(env, wall_mask, lava_mask, lava_cells, goal_yx, size)\n",
    "# manual.start()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     birl_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b8c1d",
   "metadata": {},
   "source": [
    "### Working with env API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=\"human\")\n",
    "env.reset(seed=0)\n",
    "\n",
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(env)\n",
    "\n",
    "print(\"=== ENV INFO ===\")\n",
    "print(\"Grid size:\", size)\n",
    "print(\"Goal (y,x):\", goal_yx)\n",
    "print(\"FEATURE_SET:\", FEATURE_SET)\n",
    "print(\"Reward weights (normalized):\", l2_normalize(W_MAP[FEATURE_SET]))\n",
    "print(\"================\\n\")\n",
    "\n",
    "manual = ManualControlStateReward(\n",
    "    env=env,\n",
    "    wall_mask=wall_mask,\n",
    "    lava_mask=lava_mask,\n",
    "    lava_cells=lava_cells,\n",
    "    goal_yx=goal_yx,\n",
    "    size=size,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Controls:\")\n",
    "print(\"  LEFT  = turn left\")\n",
    "print(\"  RIGHT = turn right\")\n",
    "print(\"  UP    = move forward\")\n",
    "print(\"  ESC   = quit\\n\")\n",
    "manual.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938c2e6",
   "metadata": {},
   "source": [
    "### Generating random trajectories and pairwise preferneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33c5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_random_trajectory(\n",
    "    start_state,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    max_horizon=30,\n",
    "    rng=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Roll out a random trajectory from a fixed start state.\n",
    "    Returns list of (s, a, s_next).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    traj = []\n",
    "    s = start_state\n",
    "\n",
    "    for _ in range(max_horizon):\n",
    "        if is_terminal_state(s, goal_yx, lava_mask):\n",
    "            break\n",
    "\n",
    "        a = rng.choice(ACTIONS)\n",
    "        sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "\n",
    "        traj.append((s, a, sp))\n",
    "        s = sp\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return traj\n",
    "\n",
    "def generate_random_trajectories_from_state(\n",
    "    start_state,\n",
    "    n_trajs,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    max_horizon=30,\n",
    "    seed=0,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return [\n",
    "        rollout_random_trajectory(\n",
    "            start_state,\n",
    "            wall_mask,\n",
    "            goal_yx,\n",
    "            lava_mask,\n",
    "            max_horizon=max_horizon,\n",
    "            rng=rng,\n",
    "        )\n",
    "        for _ in range(n_trajs)\n",
    "    ]\n",
    "\n",
    "def generate_trajectory_pool(\n",
    "    states,\n",
    "    terminal_mask,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    n_trajs_per_state=5,\n",
    "    max_horizon=30,\n",
    "):\n",
    "    pool = []\n",
    "    for i, s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "\n",
    "        trajs = generate_random_trajectories_from_state(\n",
    "            start_state=s,\n",
    "            n_trajs=n_trajs_per_state,\n",
    "            wall_mask=wall_mask,\n",
    "            goal_yx=goal_yx,\n",
    "            lava_mask=lava_mask,\n",
    "            max_horizon=max_horizon,\n",
    "            seed=i,\n",
    "        )\n",
    "        pool.extend(trajs)\n",
    "\n",
    "    return pool\n",
    "\n",
    "def trajectory_return(\n",
    "    traj,\n",
    "    Phi,\n",
    "    theta,\n",
    "    gamma=0.99,\n",
    "):\n",
    "    \"\"\"\n",
    "    traj: list of (s, a, s_next)\n",
    "    \"\"\"\n",
    "    theta = l2_normalize(theta)\n",
    "    ret = 0.0\n",
    "    g = 1.0\n",
    "\n",
    "    for (_s, _a, sp) in traj:\n",
    "        sp_idx = Phi[\"idx_of\"][sp]\n",
    "        r = Phi[\"Phi\"][sp_idx] @ theta\n",
    "        ret += g * r\n",
    "        g *= gamma\n",
    "\n",
    "    return ret\n",
    "\n",
    "def generate_pairwise_preferences(\n",
    "    trajectories,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    gamma=0.99,\n",
    "    n_pairs=1000,\n",
    "    seed=0,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    prefs = []\n",
    "\n",
    "    returns = [\n",
    "        trajectory_return(traj, mdp, theta_true, gamma)\n",
    "        for traj in trajectories\n",
    "    ]\n",
    "\n",
    "    N = len(trajectories)\n",
    "\n",
    "    for _ in range(n_pairs):\n",
    "        i, j = rng.choice(N, size=2, replace=False)\n",
    "\n",
    "        if returns[i] == returns[j]:\n",
    "            continue\n",
    "\n",
    "        if returns[i] > returns[j]:\n",
    "            prefs.append((trajectories[i], trajectories[j]))\n",
    "        else:\n",
    "            prefs.append((trajectories[j], trajectories[i]))\n",
    "\n",
    "    return prefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d624a",
   "metadata": {},
   "source": [
    "### testing pairwise preference generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4daf5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 1: ONE RANDOM TRAJECTORY PER START STATE\n",
      "================================================================================\n",
      "\n",
      "START STATE 000: (1, 1, 0)\n",
      "  t=00  s=(1, 1, 0)  a=2  s'=(1, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 2, 0)  r=-1.4709  γ^t r=-1.4709\n",
      "    TOTAL RETURN = -1.470928\n",
      "\n",
      "START STATE 001: (1, 1, 1)\n",
      "  t=00  s=(1, 1, 1)  a=1  s'=(1, 1, 2)\n",
      "  t=01  s=(1, 1, 2)  a=1  s'=(1, 1, 3)\n",
      "  t=02  s=(1, 1, 3)  a=0  s'=(1, 1, 2)\n",
      "  t=03  s=(1, 1, 2)  a=0  s'=(1, 1, 1)\n",
      "  t=04  s=(1, 1, 1)  a=0  s'=(1, 1, 0)\n",
      "  t=05  s=(1, 1, 0)  a=0  s'=(1, 1, 3)\n",
      "  t=06  s=(1, 1, 3)  a=0  s'=(1, 1, 2)\n",
      "  t=07  s=(1, 1, 2)  a=0  s'=(1, 1, 1)\n",
      "  t=08  s=(1, 1, 1)  a=2  s'=(2, 1, 1)\n",
      "  t=09  s=(2, 1, 1)  a=1  s'=(2, 1, 2)\n",
      "  t=10  s=(2, 1, 2)  a=2  s'=(2, 1, 2)\n",
      "  t=11  s=(2, 1, 2)  a=1  s'=(2, 1, 3)\n",
      "  t=12  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=13  s=(2, 1, 0)  a=2  s'=(2, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6417\n",
      "      t=01  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6289\n",
      "      t=03  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6354\n",
      "      t=05  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6102\n",
      "      t=06  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6041\n",
      "      t=07  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5981\n",
      "      t=08  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4464\n",
      "      t=09  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4419\n",
      "      t=10  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4375\n",
      "      t=11  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4331\n",
      "      t=12  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4463\n",
      "      t=13  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.1522\n",
      "    TOTAL RETURN = -8.333545\n",
      "\n",
      "START STATE 002: (1, 1, 2)\n",
      "  t=00  s=(1, 1, 2)  a=2  s'=(1, 1, 2)\n",
      "  t=01  s=(1, 1, 2)  a=1  s'=(1, 1, 3)\n",
      "  t=02  s=(1, 1, 3)  a=1  s'=(1, 1, 0)\n",
      "  t=03  s=(1, 1, 0)  a=1  s'=(1, 1, 1)\n",
      "  t=04  s=(1, 1, 1)  a=2  s'=(2, 1, 1)\n",
      "  t=05  s=(2, 1, 1)  a=0  s'=(2, 1, 0)\n",
      "  t=06  s=(2, 1, 0)  a=2  s'=(2, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6417\n",
      "      t=01  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6483\n",
      "      t=03  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4647\n",
      "      t=05  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4788\n",
      "      t=06  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.2361\n",
      "    TOTAL RETURN = -4.727431\n",
      "\n",
      "START STATE 003: (1, 1, 3)\n",
      "  t=00  s=(1, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=01  s=(1, 1, 3)  a=0  s'=(1, 1, 2)\n",
      "  t=02  s=(1, 1, 2)  a=1  s'=(1, 1, 3)\n",
      "  t=03  s=(1, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=04  s=(1, 1, 3)  a=1  s'=(1, 1, 0)\n",
      "  t=05  s=(1, 1, 0)  a=0  s'=(1, 1, 3)\n",
      "  t=06  s=(1, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=07  s=(1, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=08  s=(1, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=09  s=(1, 1, 3)  a=0  s'=(1, 1, 2)\n",
      "  t=10  s=(1, 1, 2)  a=0  s'=(1, 1, 1)\n",
      "  t=11  s=(1, 1, 1)  a=2  s'=(2, 1, 1)\n",
      "  t=12  s=(2, 1, 1)  a=0  s'=(2, 1, 0)\n",
      "  t=13  s=(2, 1, 0)  a=1  s'=(2, 1, 1)\n",
      "  t=14  s=(2, 1, 1)  a=0  s'=(2, 1, 0)\n",
      "  t=15  s=(2, 1, 0)  a=0  s'=(2, 1, 3)\n",
      "  t=16  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=17  s=(2, 1, 0)  a=1  s'=(2, 1, 1)\n",
      "  t=18  s=(2, 1, 1)  a=1  s'=(2, 1, 2)\n",
      "  t=19  s=(2, 1, 2)  a=0  s'=(2, 1, 1)\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6417\n",
      "      t=01  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6289\n",
      "      t=03  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6354\n",
      "      t=05  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6102\n",
      "      t=06  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6041\n",
      "      t=07  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5981\n",
      "      t=08  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5921\n",
      "      t=09  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5862\n",
      "      t=10  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5803\n",
      "      t=11  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4331\n",
      "      t=12  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4463\n",
      "      t=13  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4245\n",
      "      t=14  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4374\n",
      "      t=15  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4160\n",
      "      t=16  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4287\n",
      "      t=17  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4078\n",
      "      t=18  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4037\n",
      "      t=19  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.3996\n",
      "    TOTAL RETURN = -10.531935\n",
      "\n",
      "START STATE 008: (1, 3, 0)\n",
      "  t=00  s=(1, 3, 0)  a=0  s'=(1, 3, 3)\n",
      "  t=01  s=(1, 3, 3)  a=0  s'=(1, 3, 2)\n",
      "  t=02  s=(1, 3, 2)  a=0  s'=(1, 3, 1)\n",
      "  t=03  s=(1, 3, 1)  a=2  s'=(2, 3, 1)\n",
      "  t=04  s=(2, 3, 1)  a=1  s'=(2, 3, 2)\n",
      "  t=05  s=(2, 3, 2)  a=1  s'=(2, 3, 3)\n",
      "  t=06  s=(2, 3, 3)  a=0  s'=(2, 3, 2)\n",
      "  t=07  s=(2, 3, 2)  a=1  s'=(2, 3, 3)\n",
      "  t=08  s=(2, 3, 3)  a=2  s'=(1, 3, 3)\n",
      "  t=09  s=(1, 3, 3)  a=1  s'=(1, 3, 0)\n",
      "  t=10  s=(1, 3, 0)  a=1  s'=(1, 3, 1)\n",
      "  t=11  s=(1, 3, 1)  a=2  s'=(2, 3, 1)\n",
      "  t=12  s=(2, 3, 1)  a=2  s'=(3, 3, 1)\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3421\n",
      "      t=02  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1802\n",
      "      t=05  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1766\n",
      "      t=07  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1564\n",
      "      t=08  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3006\n",
      "      t=09  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2976\n",
      "      t=10  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1503\n",
      "      t=12  s'=(3, 3, 1)  r=-0.0099  γ^t r=-0.0088\n",
      "    TOTAL RETURN = -2.874610\n",
      "\n",
      "START STATE 009: (1, 3, 1)\n",
      "  t=00  s=(1, 3, 1)  a=2  s'=(2, 3, 1)\n",
      "  t=01  s=(2, 3, 1)  a=1  s'=(2, 3, 2)\n",
      "  t=02  s=(2, 3, 2)  a=2  s'=(2, 2, 2)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1857\n",
      "      t=02  s'=(2, 2, 2)  r=-1.3130  γ^t r=-1.2868\n",
      "    TOTAL RETURN = -1.640364\n",
      "\n",
      "START STATE 010: (1, 3, 2)\n",
      "  t=00  s=(1, 3, 2)  a=2  s'=(1, 2, 2)\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 2, 2)  r=-1.4709  γ^t r=-1.4709\n",
      "    TOTAL RETURN = -1.470928\n",
      "\n",
      "START STATE 011: (1, 3, 3)\n",
      "  t=00  s=(1, 3, 3)  a=1  s'=(1, 3, 0)\n",
      "  t=01  s=(1, 3, 0)  a=2  s'=(1, 3, 0)\n",
      "  t=02  s=(1, 3, 0)  a=2  s'=(1, 3, 0)\n",
      "  t=03  s=(1, 3, 0)  a=2  s'=(1, 3, 0)\n",
      "  t=04  s=(1, 3, 0)  a=1  s'=(1, 3, 1)\n",
      "  t=05  s=(1, 3, 1)  a=2  s'=(2, 3, 1)\n",
      "  t=06  s=(2, 3, 1)  a=0  s'=(2, 3, 0)\n",
      "  t=07  s=(2, 3, 0)  a=1  s'=(2, 3, 1)\n",
      "  t=08  s=(2, 3, 1)  a=2  s'=(3, 3, 1)\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1580\n",
      "      t=07  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1564\n",
      "      t=08  s'=(3, 3, 1)  r=-0.0099  γ^t r=-0.0091\n",
      "    TOTAL RETURN = -2.079761\n",
      "\n",
      "START STATE 012: (2, 1, 0)\n",
      "  t=00  s=(2, 1, 0)  a=2  s'=(2, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.3130\n",
      "    TOTAL RETURN = -1.312976\n",
      "\n",
      "START STATE 013: (2, 1, 1)\n",
      "  t=00  s=(2, 1, 1)  a=1  s'=(2, 1, 2)\n",
      "  t=01  s=(2, 1, 2)  a=1  s'=(2, 1, 3)\n",
      "  t=02  s=(2, 1, 3)  a=0  s'=(2, 1, 2)\n",
      "  t=03  s=(2, 1, 2)  a=1  s'=(2, 1, 3)\n",
      "  t=04  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=05  s=(2, 1, 0)  a=2  s'=(2, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4837\n",
      "      t=01  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4789\n",
      "      t=02  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4741\n",
      "      t=03  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4694\n",
      "      t=04  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4836\n",
      "      t=05  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.2486\n",
      "    TOTAL RETURN = -3.638342\n",
      "\n",
      "START STATE 014: (2, 1, 2)\n",
      "  t=00  s=(2, 1, 2)  a=2  s'=(2, 1, 2)\n",
      "  t=01  s=(2, 1, 2)  a=0  s'=(2, 1, 1)\n",
      "  t=02  s=(2, 1, 1)  a=2  s'=(3, 1, 1)\n",
      "  t=03  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=04  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=05  s=(3, 1, 3)  a=2  s'=(2, 1, 3)\n",
      "  t=06  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=07  s=(2, 1, 0)  a=0  s'=(2, 1, 3)\n",
      "  t=08  s=(2, 1, 3)  a=0  s'=(2, 1, 2)\n",
      "  t=09  s=(2, 1, 2)  a=2  s'=(2, 1, 2)\n",
      "  t=10  s=(2, 1, 2)  a=1  s'=(2, 1, 3)\n",
      "  t=11  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=12  s=(2, 1, 0)  a=1  s'=(2, 1, 1)\n",
      "  t=13  s=(2, 1, 1)  a=2  s'=(3, 1, 1)\n",
      "  t=14  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=15  s=(3, 1, 2)  a=0  s'=(3, 1, 1)\n",
      "  t=16  s=(3, 1, 1)  a=2  s'=(3, 1, 1)\n",
      "  t=17  s=(3, 1, 1)  a=0  s'=(3, 1, 0)\n",
      "  t=18  s=(3, 1, 0)  a=0  s'=(3, 1, 3)\n",
      "  t=19  s=(3, 1, 3)  a=2  s'=(2, 1, 3)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4837\n",
      "      t=01  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4789\n",
      "      t=02  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4600\n",
      "      t=06  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4740\n",
      "      t=07  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4509\n",
      "      t=08  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4464\n",
      "      t=09  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4419\n",
      "      t=10  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4375\n",
      "      t=11  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4508\n",
      "      t=12  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4288\n",
      "      t=13  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2859\n",
      "      t=14  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2830\n",
      "      t=15  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2802\n",
      "      t=16  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2774\n",
      "      t=17  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2746\n",
      "      t=18  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2719\n",
      "      t=19  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.3996\n",
      "    TOTAL RETURN = -7.573699\n",
      "\n",
      "START STATE 015: (2, 1, 3)\n",
      "  t=00  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=01  s=(2, 1, 0)  a=0  s'=(2, 1, 3)\n",
      "  t=02  s=(2, 1, 3)  a=0  s'=(2, 1, 2)\n",
      "  t=03  s=(2, 1, 2)  a=1  s'=(2, 1, 3)\n",
      "  t=04  s=(2, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=05  s=(1, 1, 3)  a=1  s'=(1, 1, 0)\n",
      "  t=06  s=(1, 1, 0)  a=2  s'=(1, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.5035\n",
      "      t=01  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4789\n",
      "      t=02  s'=(2, 1, 2)  r=-0.4837  γ^t r=-0.4741\n",
      "      t=03  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4694\n",
      "      t=04  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6164\n",
      "      t=05  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6290\n",
      "      t=06  s'=(1, 2, 0)  r=-1.4709  γ^t r=-1.3848\n",
      "    TOTAL RETURN = -4.556078\n",
      "\n",
      "START STATE 020: (2, 3, 0)\n",
      "  t=00  s=(2, 3, 0)  a=0  s'=(2, 3, 3)\n",
      "  t=01  s=(2, 3, 3)  a=0  s'=(2, 3, 2)\n",
      "  t=02  s=(2, 3, 2)  a=2  s'=(2, 2, 2)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1857\n",
      "      t=02  s'=(2, 2, 2)  r=-1.3130  γ^t r=-1.2868\n",
      "    TOTAL RETURN = -1.640364\n",
      "\n",
      "START STATE 021: (2, 3, 1)\n",
      "  t=00  s=(2, 3, 1)  a=2  s'=(3, 3, 1)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 3, 1)  r=-0.0099  γ^t r=-0.0099\n",
      "    TOTAL RETURN = -0.009872\n",
      "\n",
      "START STATE 022: (2, 3, 2)\n",
      "  t=00  s=(2, 3, 2)  a=0  s'=(2, 3, 1)\n",
      "  t=01  s=(2, 3, 1)  a=0  s'=(2, 3, 0)\n",
      "  t=02  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=03  s=(2, 3, 0)  a=1  s'=(2, 3, 1)\n",
      "  t=04  s=(2, 3, 1)  a=1  s'=(2, 3, 2)\n",
      "  t=05  s=(2, 3, 2)  a=0  s'=(2, 3, 1)\n",
      "  t=06  s=(2, 3, 1)  a=2  s'=(3, 3, 1)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1802\n",
      "      t=05  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(3, 3, 1)  r=-0.0099  γ^t r=-0.0093\n",
      "    TOTAL RETURN = -1.010364\n",
      "\n",
      "START STATE 023: (2, 3, 3)\n",
      "  t=00  s=(2, 3, 3)  a=1  s'=(2, 3, 0)\n",
      "  t=01  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=02  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=03  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=04  s=(2, 3, 0)  a=0  s'=(2, 3, 3)\n",
      "  t=05  s=(2, 3, 3)  a=2  s'=(1, 3, 3)\n",
      "  t=06  s=(1, 3, 3)  a=0  s'=(1, 3, 2)\n",
      "  t=07  s=(1, 3, 2)  a=1  s'=(1, 3, 3)\n",
      "  t=08  s=(1, 3, 3)  a=1  s'=(1, 3, 0)\n",
      "  t=09  s=(1, 3, 0)  a=2  s'=(1, 3, 0)\n",
      "  t=10  s=(1, 3, 0)  a=0  s'=(1, 3, 3)\n",
      "  t=11  s=(1, 3, 3)  a=2  s'=(1, 3, 3)\n",
      "  t=12  s=(1, 3, 3)  a=0  s'=(1, 3, 2)\n",
      "  t=13  s=(1, 3, 2)  a=1  s'=(1, 3, 3)\n",
      "  t=14  s=(1, 3, 3)  a=1  s'=(1, 3, 0)\n",
      "  t=15  s=(1, 3, 0)  a=2  s'=(1, 3, 0)\n",
      "  t=16  s=(1, 3, 0)  a=0  s'=(1, 3, 3)\n",
      "  t=17  s=(1, 3, 3)  a=2  s'=(1, 3, 3)\n",
      "  t=18  s=(1, 3, 3)  a=2  s'=(1, 3, 3)\n",
      "  t=19  s=(1, 3, 3)  a=2  s'=(1, 3, 3)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3098\n",
      "      t=06  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3253\n",
      "      t=07  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3036\n",
      "      t=08  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.3006\n",
      "      t=09  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2976\n",
      "      t=10  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2917\n",
      "      t=12  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3063\n",
      "      t=13  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2859\n",
      "      t=14  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2830\n",
      "      t=15  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2802\n",
      "      t=16  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2774\n",
      "      t=17  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2746\n",
      "      t=18  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2719\n",
      "      t=19  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2691\n",
      "    TOTAL RETURN = -5.194122\n",
      "\n",
      "START STATE 024: (3, 1, 0)\n",
      "  t=00  s=(3, 1, 0)  a=0  s'=(3, 1, 3)\n",
      "  t=01  s=(3, 1, 3)  a=2  s'=(2, 1, 3)\n",
      "  t=02  s=(2, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=03  s=(1, 1, 3)  a=0  s'=(1, 1, 2)\n",
      "  t=04  s=(1, 1, 2)  a=1  s'=(1, 1, 3)\n",
      "  t=05  s=(1, 1, 3)  a=1  s'=(1, 1, 0)\n",
      "  t=06  s=(1, 1, 0)  a=0  s'=(1, 1, 3)\n",
      "  t=07  s=(1, 1, 3)  a=1  s'=(1, 1, 0)\n",
      "  t=08  s=(1, 1, 0)  a=1  s'=(1, 1, 1)\n",
      "  t=09  s=(1, 1, 1)  a=2  s'=(2, 1, 1)\n",
      "  t=10  s=(2, 1, 1)  a=2  s'=(3, 1, 1)\n",
      "  t=11  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=12  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=13  s=(3, 1, 3)  a=1  s'=(3, 1, 0)\n",
      "  t=14  s=(3, 1, 0)  a=2  s'=(3, 2, 0)\n",
      "  t=15  s=(3, 2, 0)  a=0  s'=(3, 2, 3)\n",
      "  t=16  s=(3, 2, 3)  a=1  s'=(3, 2, 0)\n",
      "  t=17  s=(3, 2, 0)  a=0  s'=(3, 2, 3)\n",
      "  t=18  s=(3, 2, 3)  a=1  s'=(3, 2, 0)\n",
      "  t=19  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4789\n",
      "      t=02  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6289\n",
      "      t=03  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6164\n",
      "      t=05  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6290\n",
      "      t=06  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6041\n",
      "      t=07  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6165\n",
      "      t=08  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5921\n",
      "      t=09  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4419\n",
      "      t=10  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2917\n",
      "      t=12  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2888\n",
      "      t=13  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2859\n",
      "      t=14  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1458\n",
      "      t=15  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1613\n",
      "      t=16  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1429\n",
      "      t=17  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1581\n",
      "      t=18  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1401\n",
      "      t=19  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0082\n",
      "    TOTAL RETURN = -7.473493\n",
      "\n",
      "START STATE 025: (3, 1, 1)\n",
      "  t=00  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=01  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=02  s=(3, 1, 3)  a=2  s'=(2, 1, 3)\n",
      "  t=03  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=04  s=(2, 1, 0)  a=2  s'=(2, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4741\n",
      "      t=03  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4885\n",
      "      t=04  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.2612\n",
      "    TOTAL RETURN = -2.872154\n",
      "\n",
      "START STATE 026: (3, 1, 2)\n",
      "  t=00  s=(3, 1, 2)  a=0  s'=(3, 1, 1)\n",
      "  t=01  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=02  s=(3, 1, 2)  a=2  s'=(3, 1, 2)\n",
      "  t=03  s=(3, 1, 2)  a=2  s'=(3, 1, 2)\n",
      "  t=04  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=05  s=(3, 1, 3)  a=1  s'=(3, 1, 0)\n",
      "  t=06  s=(3, 1, 0)  a=1  s'=(3, 1, 1)\n",
      "  t=07  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=08  s=(3, 1, 2)  a=0  s'=(3, 1, 1)\n",
      "  t=09  s=(3, 1, 1)  a=2  s'=(3, 1, 1)\n",
      "  t=10  s=(3, 1, 1)  a=0  s'=(3, 1, 0)\n",
      "  t=11  s=(3, 1, 0)  a=1  s'=(3, 1, 1)\n",
      "  t=12  s=(3, 1, 1)  a=0  s'=(3, 1, 0)\n",
      "  t=13  s=(3, 1, 0)  a=2  s'=(3, 2, 0)\n",
      "  t=14  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3098\n",
      "      t=06  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3067\n",
      "      t=07  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3036\n",
      "      t=08  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3006\n",
      "      t=09  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2976\n",
      "      t=10  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2917\n",
      "      t=12  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2888\n",
      "      t=13  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1473\n",
      "      t=14  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0086\n",
      "    TOTAL RETURN = -4.145917\n",
      "\n",
      "START STATE 027: (3, 1, 3)\n",
      "  t=00  s=(3, 1, 3)  a=2  s'=(2, 1, 3)\n",
      "  t=01  s=(2, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=02  s=(1, 1, 3)  a=2  s'=(1, 1, 3)\n",
      "  t=03  s=(1, 1, 3)  a=0  s'=(1, 1, 2)\n",
      "  t=04  s=(1, 1, 2)  a=0  s'=(1, 1, 1)\n",
      "  t=05  s=(1, 1, 1)  a=0  s'=(1, 1, 0)\n",
      "  t=06  s=(1, 1, 0)  a=2  s'=(1, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4837\n",
      "      t=01  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6289\n",
      "      t=03  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.6164\n",
      "      t=05  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6290\n",
      "      t=06  s'=(1, 2, 0)  r=-1.4709  γ^t r=-1.3848\n",
      "    TOTAL RETURN = -5.000776\n",
      "\n",
      "START STATE 028: (3, 2, 0)\n",
      "  t=00  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0099\n",
      "    TOTAL RETURN = -0.009872\n",
      "\n",
      "START STATE 029: (3, 2, 1)\n",
      "  t=00  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=01  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=02  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=03  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=04  s=(3, 2, 1)  a=0  s'=(3, 2, 0)\n",
      "  t=05  s=(3, 2, 0)  a=0  s'=(3, 2, 3)\n",
      "  t=06  s=(3, 2, 3)  a=2  s'=(2, 2, 3)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1784\n",
      "      t=06  s'=(2, 2, 3)  r=-1.3327  γ^t r=-1.2547\n",
      "    TOTAL RETURN = -2.255609\n",
      "\n",
      "START STATE 030: (3, 2, 2)\n",
      "  t=00  s=(3, 2, 2)  a=0  s'=(3, 2, 1)\n",
      "  t=01  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=02  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=03  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=04  s=(3, 2, 1)  a=1  s'=(3, 2, 2)\n",
      "  t=05  s=(3, 2, 2)  a=0  s'=(3, 2, 1)\n",
      "  t=06  s=(3, 2, 1)  a=1  s'=(3, 2, 2)\n",
      "  t=07  s=(3, 2, 2)  a=2  s'=(3, 1, 2)\n",
      "  t=08  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=09  s=(3, 1, 3)  a=2  s'=(2, 1, 3)\n",
      "  t=10  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=11  s=(2, 1, 0)  a=2  s'=(2, 2, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1580\n",
      "      t=07  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3036\n",
      "      t=08  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3006\n",
      "      t=09  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4419\n",
      "      t=10  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4553\n",
      "      t=11  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.1756\n",
      "    TOTAL RETURN = -3.817142\n",
      "\n",
      "START STATE 031: (3, 2, 3)\n",
      "  t=00  s=(3, 2, 3)  a=0  s'=(3, 2, 2)\n",
      "  t=01  s=(3, 2, 2)  a=1  s'=(3, 2, 3)\n",
      "  t=02  s=(3, 2, 3)  a=0  s'=(3, 2, 2)\n",
      "  t=03  s=(3, 2, 2)  a=0  s'=(3, 2, 1)\n",
      "  t=04  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=05  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=06  s=(3, 2, 1)  a=0  s'=(3, 2, 0)\n",
      "  t=07  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1857\n",
      "      t=02  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1580\n",
      "      t=07  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0092\n",
      "    TOTAL RETURN = -1.168854\n",
      "\n",
      "✓ Generated 1 trajectory from 24 starting states\n",
      "\n",
      "================================================================================\n",
      "TEST 2: PAIRWISE PREFERENCE DEBUG\n",
      "================================================================================\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 2, 0)  r=-1.4709  γ^t r=-1.4709\n",
      "    TOTAL RETURN = -1.470928\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6417\n",
      "      t=01  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6289\n",
      "      t=03  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6164\n",
      "      t=05  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.6102\n",
      "      t=06  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4554\n",
      "      t=07  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3036\n",
      "      t=08  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3006\n",
      "      t=09  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2976\n",
      "      t=10  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4375\n",
      "      t=11  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4508\n",
      "      t=12  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4288\n",
      "      t=13  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5631\n",
      "      t=14  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5575\n",
      "      t=15  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5519\n",
      "      t=16  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.5632\n",
      "      t=17  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5409\n",
      "      t=18  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.5520\n",
      "      t=19  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5301\n",
      "      t=20  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5248\n",
      "      t=21  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5196\n",
      "      t=22  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5144\n",
      "      t=23  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.5249\n",
      "      t=24  s'=(1, 2, 0)  r=-1.4709  γ^t r=-1.1557\n",
      "    TOTAL RETURN = -13.527406\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6417\n",
      "      t=01  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6483\n",
      "      t=03  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6354\n",
      "      t=05  s'=(1, 2, 0)  r=-1.4709  γ^t r=-1.3988\n",
      "    TOTAL RETURN = -4.582026\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6417\n",
      "      t=01  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.6289\n",
      "      t=03  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6418\n",
      "      t=04  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6164\n",
      "      t=05  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6102\n",
      "      t=06  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6041\n",
      "      t=07  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.6165\n",
      "      t=08  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5921\n",
      "      t=09  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5862\n",
      "      t=10  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5803\n",
      "      t=11  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5745\n",
      "      t=12  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5688\n",
      "      t=13  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.5804\n",
      "      t=14  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5575\n",
      "      t=15  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5519\n",
      "      t=16  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5464\n",
      "      t=17  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5409\n",
      "      t=18  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5355\n",
      "      t=19  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.5464\n",
      "      t=20  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5248\n",
      "      t=21  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5196\n",
      "      t=22  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5144\n",
      "      t=23  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5092\n",
      "      t=24  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.5197\n",
      "      t=25  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.4991\n",
      "      t=26  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.4941\n",
      "      t=27  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.4892\n",
      "      t=28  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.4843\n",
      "      t=29  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.4794\n",
      "    TOTAL RETURN = -16.789617\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3386\n",
      "      t=03  s'=(1, 2, 2)  r=-1.4709  γ^t r=-1.4272\n",
      "    TOTAL RETURN = -2.414178\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3455\n",
      "      t=01  s'=(1, 2, 2)  r=-1.4709  γ^t r=-1.4562\n",
      "    TOTAL RETURN = -1.801739\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 2, 2)  r=-1.4709  γ^t r=-1.4709\n",
      "    TOTAL RETURN = -1.470928\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3455\n",
      "      t=01  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1820\n",
      "      t=04  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1580\n",
      "      t=07  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1564\n",
      "      t=08  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1549\n",
      "      t=09  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1533\n",
      "      t=10  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1518\n",
      "      t=11  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1503\n",
      "      t=12  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1488\n",
      "      t=13  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1473\n",
      "      t=14  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1458\n",
      "      t=15  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1443\n",
      "      t=16  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1429\n",
      "      t=17  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1415\n",
      "      t=18  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1401\n",
      "      t=19  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1387\n",
      "      t=20  s'=(3, 3, 1)  r=-0.0099  γ^t r=-0.0081\n",
      "    TOTAL RETURN = -3.417259\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4837\n",
      "      t=01  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4984\n",
      "      t=02  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.2868\n",
      "    TOTAL RETURN = -2.269013\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0093\n",
      "    TOTAL RETURN = -1.765520\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4837\n",
      "      t=01  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3098\n",
      "      t=06  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3067\n",
      "      t=07  s'=(2, 1, 3)  r=-0.4837  γ^t r=-0.4509\n",
      "      t=08  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4646\n",
      "      t=09  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4419\n",
      "      t=10  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4553\n",
      "      t=11  s'=(2, 1, 1)  r=-0.4837  γ^t r=-0.4331\n",
      "      t=12  s'=(2, 1, 0)  r=-0.5035  γ^t r=-0.4463\n",
      "      t=13  s'=(2, 2, 0)  r=-1.3130  γ^t r=-1.1522\n",
      "    TOTAL RETURN = -6.215301\n",
      "    Step rewards:\n",
      "      t=00  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6417\n",
      "      t=01  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6353\n",
      "      t=02  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6289\n",
      "      t=03  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6226\n",
      "      t=04  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6164\n",
      "      t=05  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.6102\n",
      "      t=06  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.6041\n",
      "      t=07  s'=(1, 1, 1)  r=-0.6417  γ^t r=-0.5981\n",
      "      t=08  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5921\n",
      "      t=09  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5862\n",
      "      t=10  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5803\n",
      "      t=11  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5745\n",
      "      t=12  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5688\n",
      "      t=13  s'=(1, 1, 2)  r=-0.6417  γ^t r=-0.5631\n",
      "      t=14  s'=(1, 1, 3)  r=-0.6417  γ^t r=-0.5575\n",
      "      t=15  s'=(1, 1, 0)  r=-0.6614  γ^t r=-0.5689\n",
      "      t=16  s'=(1, 2, 0)  r=-1.4709  γ^t r=-1.2524\n",
      "    TOTAL RETURN = -10.801072\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1838\n",
      "      t=03  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3286\n",
      "      t=06  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.3067\n",
      "      t=07  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3220\n",
      "      t=08  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.3006\n",
      "      t=09  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3156\n",
      "      t=10  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3094\n",
      "      t=12  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.2888\n",
      "      t=13  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2859\n",
      "      t=14  s'=(1, 3, 1)  r=-0.3258  γ^t r=-0.2830\n",
      "      t=15  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1443\n",
      "      t=16  s'=(3, 3, 1)  r=-0.0099  γ^t r=-0.0084\n",
      "    TOTAL RETURN = -4.181557\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(2, 3, 0)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(2, 3, 1)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1820\n",
      "      t=04  s'=(2, 3, 3)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3098\n",
      "      t=06  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.3067\n",
      "      t=07  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.3036\n",
      "      t=08  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.3188\n",
      "      t=09  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2976\n",
      "      t=10  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2917\n",
      "      t=12  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2888\n",
      "      t=13  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2859\n",
      "      t=14  s'=(1, 3, 0)  r=-0.3258  γ^t r=-0.2830\n",
      "      t=15  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2802\n",
      "      t=16  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2774\n",
      "      t=17  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2746\n",
      "      t=18  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2719\n",
      "      t=19  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2691\n",
      "      t=20  s'=(1, 3, 3)  r=-0.3258  γ^t r=-0.2665\n",
      "      t=21  s'=(1, 3, 2)  r=-0.3455  γ^t r=-0.2798\n",
      "      t=22  s'=(1, 2, 2)  r=-1.4709  γ^t r=-1.1791\n",
      "    TOTAL RETURN = -6.920782\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 2, 2)  r=-1.3130  γ^t r=-1.3130\n",
      "    TOTAL RETURN = -1.312976\n",
      "    Step rewards:\n",
      "      t=00  s'=(2, 3, 2)  r=-0.1876  γ^t r=-0.1876\n",
      "      t=01  s'=(2, 2, 2)  r=-1.3130  γ^t r=-1.2998\n",
      "    TOTAL RETURN = -1.487414\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1580\n",
      "      t=07  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1564\n",
      "      t=08  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1731\n",
      "      t=09  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1533\n",
      "      t=10  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1518\n",
      "      t=11  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1503\n",
      "      t=12  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1663\n",
      "      t=13  s'=(2, 2, 3)  r=-1.3327  γ^t r=-1.1695\n",
      "    TOTAL RETURN = -3.575025\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3098\n",
      "      t=06  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3067\n",
      "      t=07  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3036\n",
      "      t=08  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3006\n",
      "      t=09  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2976\n",
      "      t=10  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2917\n",
      "      t=12  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2888\n",
      "      t=13  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2859\n",
      "      t=14  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2830\n",
      "      t=15  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2802\n",
      "      t=16  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2774\n",
      "      t=17  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2746\n",
      "      t=18  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2719\n",
      "      t=19  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2691\n",
      "      t=20  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2665\n",
      "      t=21  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2638\n",
      "      t=22  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2612\n",
      "      t=23  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2585\n",
      "      t=24  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2560\n",
      "      t=25  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2534\n",
      "      t=26  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2509\n",
      "      t=27  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1279\n",
      "      t=28  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1267\n",
      "      t=29  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1254\n",
      "    TOTAL RETURN = -8.122297\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3129\n",
      "      t=05  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.3098\n",
      "      t=06  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3067\n",
      "      t=07  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1564\n",
      "      t=08  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0091\n",
      "    TOTAL RETURN = -2.378680\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3258\n",
      "      t=01  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.3225\n",
      "      t=02  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.3193\n",
      "      t=03  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.3161\n",
      "      t=04  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1784\n",
      "      t=06  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1580\n",
      "      t=07  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1748\n",
      "      t=08  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1549\n",
      "      t=09  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0090\n",
      "    TOTAL RETURN = -2.119979\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0096\n",
      "    TOTAL RETURN = -0.508033\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1645\n",
      "      t=03  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1628\n",
      "      t=04  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1612\n",
      "      t=05  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1596\n",
      "      t=06  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1766\n",
      "      t=07  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1564\n",
      "      t=08  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1731\n",
      "      t=09  s'=(3, 2, 2)  r=-0.1678  γ^t r=-0.1533\n",
      "      t=10  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2946\n",
      "      t=11  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2917\n",
      "      t=12  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2888\n",
      "      t=13  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2859\n",
      "      t=14  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2830\n",
      "      t=15  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2802\n",
      "      t=16  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2774\n",
      "      t=17  s'=(3, 1, 3)  r=-0.3258  γ^t r=-0.2746\n",
      "      t=18  s'=(3, 1, 2)  r=-0.3258  γ^t r=-0.2719\n",
      "      t=19  s'=(3, 1, 1)  r=-0.3258  γ^t r=-0.2691\n",
      "      t=20  s'=(3, 1, 0)  r=-0.3258  γ^t r=-0.2665\n",
      "      t=21  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1359\n",
      "      t=22  s'=(3, 2, 3)  r=-0.1876  γ^t r=-0.1504\n",
      "      t=23  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1332\n",
      "      t=24  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0078\n",
      "    TOTAL RETURN = -5.152308\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 2, 1)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1661\n",
      "      t=02  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0097\n",
      "    TOTAL RETURN = -0.343645\n",
      "    Step rewards:\n",
      "      t=00  s'=(3, 2, 0)  r=-0.1678  γ^t r=-0.1678\n",
      "      t=01  s'=(3, 3, 0)  r=-0.0099  γ^t r=-0.0098\n",
      "    TOTAL RETURN = -0.177597\n",
      "\n",
      "------------------------------------------------------------\n",
      "PAIR (10, 12)  →  τ_j ≻ τ_i\n",
      "------------------------------------------------------------\n",
      "Trajectory i:\n",
      "  t=00  s=(2, 1, 2)  a=0  s'=(2, 1, 1)\n",
      "  t=01  s=(2, 1, 1)  a=2  s'=(3, 1, 1)\n",
      "  t=02  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=03  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=04  s=(3, 1, 3)  a=0  s'=(3, 1, 2)\n",
      "  t=05  s=(3, 1, 2)  a=2  s'=(3, 1, 2)\n",
      "  t=06  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=07  s=(3, 1, 3)  a=2  s'=(2, 1, 3)\n",
      "  t=08  s=(2, 1, 3)  a=1  s'=(2, 1, 0)\n",
      "  t=09  s=(2, 1, 0)  a=1  s'=(2, 1, 1)\n",
      "  t=10  s=(2, 1, 1)  a=0  s'=(2, 1, 0)\n",
      "  t=11  s=(2, 1, 0)  a=1  s'=(2, 1, 1)\n",
      "  t=12  s=(2, 1, 1)  a=0  s'=(2, 1, 0)\n",
      "  t=13  s=(2, 1, 0)  a=2  s'=(2, 2, 0)\n",
      "  RETURN i = -6.215301\n",
      "\n",
      "Trajectory j:\n",
      "  t=00  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=01  s=(2, 3, 0)  a=0  s'=(2, 3, 3)\n",
      "  t=02  s=(2, 3, 3)  a=0  s'=(2, 3, 2)\n",
      "  t=03  s=(2, 3, 2)  a=1  s'=(2, 3, 3)\n",
      "  t=04  s=(2, 3, 3)  a=2  s'=(1, 3, 3)\n",
      "  t=05  s=(1, 3, 3)  a=0  s'=(1, 3, 2)\n",
      "  t=06  s=(1, 3, 2)  a=0  s'=(1, 3, 1)\n",
      "  t=07  s=(1, 3, 1)  a=1  s'=(1, 3, 2)\n",
      "  t=08  s=(1, 3, 2)  a=0  s'=(1, 3, 1)\n",
      "  t=09  s=(1, 3, 1)  a=1  s'=(1, 3, 2)\n",
      "  t=10  s=(1, 3, 2)  a=1  s'=(1, 3, 3)\n",
      "  t=11  s=(1, 3, 3)  a=0  s'=(1, 3, 2)\n",
      "  t=12  s=(1, 3, 2)  a=0  s'=(1, 3, 1)\n",
      "  t=13  s=(1, 3, 1)  a=0  s'=(1, 3, 0)\n",
      "  t=14  s=(1, 3, 0)  a=1  s'=(1, 3, 1)\n",
      "  t=15  s=(1, 3, 1)  a=2  s'=(2, 3, 1)\n",
      "  t=16  s=(2, 3, 1)  a=2  s'=(3, 3, 1)\n",
      "  RETURN j = -4.181557\n",
      "\n",
      "------------------------------------------------------------\n",
      "PAIR (0, 21)  →  τ_i ≻ τ_j\n",
      "------------------------------------------------------------\n",
      "Trajectory i:\n",
      "  t=00  s=(1, 1, 0)  a=2  s'=(1, 2, 0)\n",
      "  RETURN i = -1.470928\n",
      "\n",
      "Trajectory j:\n",
      "  t=00  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=01  s=(3, 2, 1)  a=0  s'=(3, 2, 0)\n",
      "  t=02  s=(3, 2, 0)  a=1  s'=(3, 2, 1)\n",
      "  t=03  s=(3, 2, 1)  a=1  s'=(3, 2, 2)\n",
      "  t=04  s=(3, 2, 2)  a=0  s'=(3, 2, 1)\n",
      "  t=05  s=(3, 2, 1)  a=1  s'=(3, 2, 2)\n",
      "  t=06  s=(3, 2, 2)  a=1  s'=(3, 2, 3)\n",
      "  t=07  s=(3, 2, 3)  a=0  s'=(3, 2, 2)\n",
      "  t=08  s=(3, 2, 2)  a=1  s'=(3, 2, 3)\n",
      "  t=09  s=(3, 2, 3)  a=0  s'=(3, 2, 2)\n",
      "  t=10  s=(3, 2, 2)  a=2  s'=(3, 1, 2)\n",
      "  t=11  s=(3, 1, 2)  a=0  s'=(3, 1, 1)\n",
      "  t=12  s=(3, 1, 1)  a=1  s'=(3, 1, 2)\n",
      "  t=13  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=14  s=(3, 1, 3)  a=0  s'=(3, 1, 2)\n",
      "  t=15  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=16  s=(3, 1, 3)  a=1  s'=(3, 1, 0)\n",
      "  t=17  s=(3, 1, 0)  a=0  s'=(3, 1, 3)\n",
      "  t=18  s=(3, 1, 3)  a=0  s'=(3, 1, 2)\n",
      "  t=19  s=(3, 1, 2)  a=0  s'=(3, 1, 1)\n",
      "  t=20  s=(3, 1, 1)  a=0  s'=(3, 1, 0)\n",
      "  t=21  s=(3, 1, 0)  a=2  s'=(3, 2, 0)\n",
      "  t=22  s=(3, 2, 0)  a=0  s'=(3, 2, 3)\n",
      "  t=23  s=(3, 2, 3)  a=1  s'=(3, 2, 0)\n",
      "  t=24  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "  RETURN j = -5.152308\n",
      "\n",
      "------------------------------------------------------------\n",
      "PAIR (22, 18)  →  τ_i ≻ τ_j\n",
      "------------------------------------------------------------\n",
      "Trajectory i:\n",
      "  t=00  s=(3, 2, 2)  a=0  s'=(3, 2, 1)\n",
      "  t=01  s=(3, 2, 1)  a=0  s'=(3, 2, 0)\n",
      "  t=02  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "  RETURN i = -0.343645\n",
      "\n",
      "Trajectory j:\n",
      "  t=00  s=(3, 1, 2)  a=2  s'=(3, 1, 2)\n",
      "  t=01  s=(3, 1, 2)  a=1  s'=(3, 1, 3)\n",
      "  t=02  s=(3, 1, 3)  a=1  s'=(3, 1, 0)\n",
      "  t=03  s=(3, 1, 0)  a=0  s'=(3, 1, 3)\n",
      "  t=04  s=(3, 1, 3)  a=1  s'=(3, 1, 0)\n",
      "  t=05  s=(3, 1, 0)  a=0  s'=(3, 1, 3)\n",
      "  t=06  s=(3, 1, 3)  a=1  s'=(3, 1, 0)\n",
      "  t=07  s=(3, 1, 0)  a=2  s'=(3, 2, 0)\n",
      "  t=08  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "  RETURN j = -2.378680\n",
      "\n",
      "------------------------------------------------------------\n",
      "PAIR (20, 7)  →  τ_i ≻ τ_j\n",
      "------------------------------------------------------------\n",
      "Trajectory i:\n",
      "  t=00  s=(3, 2, 0)  a=1  s'=(3, 2, 1)\n",
      "  t=01  s=(3, 2, 1)  a=2  s'=(3, 2, 1)\n",
      "  t=02  s=(3, 2, 1)  a=0  s'=(3, 2, 0)\n",
      "  t=03  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "  RETURN i = -0.508033\n",
      "\n",
      "Trajectory j:\n",
      "  t=00  s=(1, 3, 3)  a=0  s'=(1, 3, 2)\n",
      "  t=01  s=(1, 3, 2)  a=0  s'=(1, 3, 1)\n",
      "  t=02  s=(1, 3, 1)  a=2  s'=(2, 3, 1)\n",
      "  t=03  s=(2, 3, 1)  a=1  s'=(2, 3, 2)\n",
      "  t=04  s=(2, 3, 2)  a=1  s'=(2, 3, 3)\n",
      "  t=05  s=(2, 3, 3)  a=1  s'=(2, 3, 0)\n",
      "  t=06  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=07  s=(2, 3, 0)  a=0  s'=(2, 3, 3)\n",
      "  t=08  s=(2, 3, 3)  a=1  s'=(2, 3, 0)\n",
      "  t=09  s=(2, 3, 0)  a=0  s'=(2, 3, 3)\n",
      "  t=10  s=(2, 3, 3)  a=1  s'=(2, 3, 0)\n",
      "  t=11  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=12  s=(2, 3, 0)  a=1  s'=(2, 3, 1)\n",
      "  t=13  s=(2, 3, 1)  a=0  s'=(2, 3, 0)\n",
      "  t=14  s=(2, 3, 0)  a=1  s'=(2, 3, 1)\n",
      "  t=15  s=(2, 3, 1)  a=0  s'=(2, 3, 0)\n",
      "  t=16  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=17  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=18  s=(2, 3, 0)  a=2  s'=(2, 3, 0)\n",
      "  t=19  s=(2, 3, 0)  a=1  s'=(2, 3, 1)\n",
      "  t=20  s=(2, 3, 1)  a=2  s'=(3, 3, 1)\n",
      "  RETURN j = -3.417259\n",
      "\n",
      "------------------------------------------------------------\n",
      "PAIR (19, 6)  →  τ_j ≻ τ_i\n",
      "------------------------------------------------------------\n",
      "Trajectory i:\n",
      "  t=00  s=(3, 1, 3)  a=0  s'=(3, 1, 2)\n",
      "  t=01  s=(3, 1, 2)  a=2  s'=(3, 1, 2)\n",
      "  t=02  s=(3, 1, 2)  a=0  s'=(3, 1, 1)\n",
      "  t=03  s=(3, 1, 1)  a=0  s'=(3, 1, 0)\n",
      "  t=04  s=(3, 1, 0)  a=2  s'=(3, 2, 0)\n",
      "  t=05  s=(3, 2, 0)  a=0  s'=(3, 2, 3)\n",
      "  t=06  s=(3, 2, 3)  a=1  s'=(3, 2, 0)\n",
      "  t=07  s=(3, 2, 0)  a=0  s'=(3, 2, 3)\n",
      "  t=08  s=(3, 2, 3)  a=1  s'=(3, 2, 0)\n",
      "  t=09  s=(3, 2, 0)  a=2  s'=(3, 3, 0)\n",
      "  RETURN i = -2.119979\n",
      "\n",
      "Trajectory j:\n",
      "  t=00  s=(1, 3, 2)  a=2  s'=(1, 2, 2)\n",
      "  RETURN j = -1.470928\n",
      "\n",
      "✓ Displayed 5 preference pairs\n"
     ]
    }
   ],
   "source": [
    "def trajectory_return_debug(\n",
    "    traj,\n",
    "    mdp,\n",
    "    theta,\n",
    "    gamma=0.99,\n",
    "):\n",
    "    theta = l2_normalize(theta)\n",
    "    ret = 0.0\n",
    "    g = 1.0\n",
    "\n",
    "    print(\"    Step rewards:\")\n",
    "    for t, (_s, _a, sp) in enumerate(traj):\n",
    "        sp_idx = mdp[\"idx_of\"][sp]\n",
    "        r = mdp[\"Phi\"][sp_idx] @ theta\n",
    "        ret += g * r\n",
    "        print(f\"      t={t:02d}  s'={sp}  r={r:.4f}  γ^t r={g*r:.4f}\")\n",
    "        g *= gamma\n",
    "\n",
    "    print(f\"    TOTAL RETURN = {ret:.6f}\")\n",
    "    return ret\n",
    "\n",
    "def test_one_random_traj_per_start_state(\n",
    "    states,\n",
    "    mdp,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    theta_true,\n",
    "    max_horizon=20,\n",
    "):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 1: ONE RANDOM TRAJECTORY PER START STATE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    count = 0\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        if mdp[\"terminal\"][i]:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nSTART STATE {i:03d}: {s}\")\n",
    "\n",
    "        traj = rollout_random_trajectory(\n",
    "            start_state=s,\n",
    "            wall_mask=wall_mask,\n",
    "            goal_yx=goal_yx,\n",
    "            lava_mask=lava_mask,\n",
    "            max_horizon=max_horizon,\n",
    "            rng=rng,\n",
    "        )\n",
    "\n",
    "        if len(traj) == 0:\n",
    "            print(\"  ⚠ Empty trajectory!\")\n",
    "            continue\n",
    "\n",
    "        for t, (s0, a, sp) in enumerate(traj):\n",
    "            print(f\"  t={t:02d}  s={s0}  a={a}  s'={sp}\")\n",
    "\n",
    "        trajectory_return_debug(traj, mdp, theta_true, mdp[\"gamma\"])\n",
    "        count += 1\n",
    "\n",
    "    print(f\"\\n✓ Generated 1 trajectory from {count} starting states\")\n",
    "\n",
    "def test_pairwise_preferences_debug(\n",
    "    trajectories,\n",
    "    mdp,\n",
    "    theta_true,\n",
    "    n_pairs=10,\n",
    "):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 2: PAIRWISE PREFERENCE DEBUG\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    returns = [\n",
    "        trajectory_return_debug(traj, mdp, theta_true, mdp[\"gamma\"])\n",
    "        for traj in trajectories\n",
    "    ]\n",
    "\n",
    "    N = len(trajectories)\n",
    "\n",
    "    shown = 0\n",
    "    for _ in range(n_pairs * 3):  # oversample to skip ties\n",
    "        i, j = rng.choice(N, size=2, replace=False)\n",
    "\n",
    "        if np.isclose(returns[i], returns[j]):\n",
    "            continue\n",
    "\n",
    "        ti, tj = trajectories[i], trajectories[j]\n",
    "        ri, rj = returns[i], returns[j]\n",
    "\n",
    "        pref = \"τ_i ≻ τ_j\" if ri > rj else \"τ_j ≻ τ_i\"\n",
    "\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"PAIR ({i}, {j})  →  {pref}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        print(\"Trajectory i:\")\n",
    "        for t, (s, a, sp) in enumerate(ti):\n",
    "            print(f\"  t={t:02d}  s={s}  a={a}  s'={sp}\")\n",
    "        print(f\"  RETURN i = {ri:.6f}\")\n",
    "\n",
    "        print(\"\\nTrajectory j:\")\n",
    "        for t, (s, a, sp) in enumerate(tj):\n",
    "            print(f\"  t={t:02d}  s={s}  a={a}  s'={sp}\")\n",
    "        print(f\"  RETURN j = {rj:.6f}\")\n",
    "\n",
    "        shown += 1\n",
    "        if shown >= n_pairs:\n",
    "            break\n",
    "\n",
    "    print(f\"\\n✓ Displayed {shown} preference pairs\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# TEST 1: one trajectory per start state\n",
    "# --------------------------------------------------\n",
    "test_one_random_traj_per_start_state(\n",
    "    states=states,\n",
    "    mdp=mdp,\n",
    "    wall_mask=wall_mask,\n",
    "    goal_yx=goal_yx,\n",
    "    lava_mask=lava_mask,\n",
    "    theta_true=theta_true,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Build trajectory pool (for preferences)\n",
    "# --------------------------------------------------\n",
    "traj_pool = generate_trajectory_pool(\n",
    "    states,\n",
    "    mdp[\"terminal\"],\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    n_trajs_per_state=1,   # EXACTLY what you asked\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# TEST 2: pairwise preference inspection\n",
    "# --------------------------------------------------\n",
    "test_pairwise_preferences_debug(\n",
    "    trajectories=traj_pool,\n",
    "    mdp=mdp,\n",
    "    theta_true=theta_true,\n",
    "    n_pairs=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6042cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
