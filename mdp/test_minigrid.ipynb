{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac6ccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BIRL RESULTS (SYMBOLIC-FREE) =====\n",
      "FEATURE_SET : L1.3\n",
      "True θ      : [-0.157952   -0.019744   -0.98720002 -0.009872  ]\n",
      "Learned θ   : [-1.16070895e-04 -9.99483190e-01  2.91536387e-02 -1.35427352e-02]\n",
      "EVD         : 0.05865421972337925\n",
      "\n",
      "===== DEBUG: V(s), Q(s,a), DEMOS =====\n",
      "\n",
      "State 000 (y=1, x=1, d=0) [EMPTY] \n",
      "  V(s) = -1.470928\n",
      "  Q(s,a=0) = -2.738146\n",
      "  Q(s,a=1) = -2.738146\n",
      "  Q(s,a=2) = -1.470928\n",
      "  OPTIMAL ACTION (demo) = 2\n",
      "\n",
      "State 001 (y=1, x=1, d=1) [EMPTY] \n",
      "  V(s) = -2.117643\n",
      "  Q(s,a=0) = -2.117643\n",
      "  Q(s,a=1) = -3.352445\n",
      "  Q(s,a=2) = -2.265609\n",
      "  OPTIMAL ACTION (demo) = 0\n",
      "\n",
      "State 002 (y=1, x=1, d=2) [EMPTY] \n",
      "  V(s) = -2.738146\n",
      "  Q(s,a=0) = -2.738146\n",
      "  Q(s,a=1) = -2.738146\n",
      "  Q(s,a=2) = -3.352445\n",
      "  OPTIMAL ACTION (demo) = 0\n",
      "\n",
      "State 003 (y=1, x=1, d=3) [EMPTY] \n",
      "  V(s) = -2.117643\n",
      "  Q(s,a=0) = -3.352445\n",
      "  Q(s,a=1) = -2.117643\n",
      "  Q(s,a=2) = -2.738146\n",
      "  OPTIMAL ACTION (demo) = 1\n",
      "\n",
      "State 004 (y=1, x=2, d=0) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 005 (y=1, x=2, d=1) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 006 (y=1, x=2, d=2) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 007 (y=1, x=2, d=3) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 008 (y=1, x=3, d=0) [EMPTY] \n",
      "  V(s) = -0.501597\n",
      "  Q(s,a=0) = -1.139910\n",
      "  Q(s,a=1) = -0.501597\n",
      "  Q(s,a=2) = -0.822357\n",
      "  OPTIMAL ACTION (demo) = 1\n",
      "\n",
      "State 009 (y=1, x=3, d=1) [EMPTY] \n",
      "  V(s) = -0.177597\n",
      "  Q(s,a=0) = -0.822357\n",
      "  Q(s,a=1) = -0.842101\n",
      "  Q(s,a=2) = -0.177597\n",
      "  OPTIMAL ACTION (demo) = 2\n",
      "\n",
      "State 010 (y=1, x=3, d=2) [EMPTY] \n",
      "  V(s) = -0.501597\n",
      "  Q(s,a=0) = -0.501597\n",
      "  Q(s,a=1) = -1.139910\n",
      "  Q(s,a=2) = -1.470928\n",
      "  OPTIMAL ACTION (demo) = 0\n",
      "\n",
      "State 011 (y=1, x=3, d=3) [EMPTY] \n",
      "  V(s) = -0.822357\n",
      "  Q(s,a=0) = -0.842101\n",
      "  Q(s,a=1) = -0.822357\n",
      "  Q(s,a=2) = -1.139910\n",
      "  OPTIMAL ACTION (demo) = 1\n",
      "\n",
      "State 012 (y=2, x=1, d=0) [EMPTY] \n",
      "  V(s) = -1.312976\n",
      "  Q(s,a=0) = -2.269013\n",
      "  Q(s,a=1) = -2.265609\n",
      "  Q(s,a=2) = -1.312976\n",
      "  OPTIMAL ACTION (demo) = 2\n",
      "\n",
      "State 013 (y=2, x=1, d=1) [EMPTY] \n",
      "  V(s) = -1.799880\n",
      "  Q(s,a=0) = -1.803318\n",
      "  Q(s,a=1) = -2.726681\n",
      "  Q(s,a=2) = -1.799880\n",
      "  OPTIMAL ACTION (demo) = 2\n",
      "\n",
      "State 014 (y=2, x=1, d=2) [EMPTY] \n",
      "  V(s) = -2.265609\n",
      "  Q(s,a=0) = -2.265609\n",
      "  Q(s,a=1) = -2.269013\n",
      "  Q(s,a=2) = -2.726681\n",
      "  OPTIMAL ACTION (demo) = 0\n",
      "\n",
      "State 015 (y=2, x=1, d=3) [EMPTY] \n",
      "  V(s) = -1.803318\n",
      "  Q(s,a=0) = -2.726681\n",
      "  Q(s,a=1) = -1.803318\n",
      "  Q(s,a=2) = -2.738146\n",
      "  OPTIMAL ACTION (demo) = 1\n",
      "\n",
      "State 016 (y=2, x=2, d=0) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 017 (y=2, x=2, d=1) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 018 (y=2, x=2, d=2) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 019 (y=2, x=2, d=3) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 020 (y=2, x=3, d=0) [EMPTY] \n",
      "  V(s) = -0.177597\n",
      "  Q(s,a=0) = -0.508033\n",
      "  Q(s,a=1) = -0.177597\n",
      "  Q(s,a=2) = -0.343645\n",
      "  OPTIMAL ACTION (demo) = 1\n",
      "\n",
      "State 021 (y=2, x=3, d=1) [EMPTY] \n",
      "  V(s) = -0.009872\n",
      "  Q(s,a=0) = -0.343645\n",
      "  Q(s,a=1) = -0.363389\n",
      "  Q(s,a=2) = -0.009872\n",
      "  OPTIMAL ACTION (demo) = 2\n",
      "\n",
      "State 022 (y=2, x=3, d=2) [EMPTY] \n",
      "  V(s) = -0.177597\n",
      "  Q(s,a=0) = -0.177597\n",
      "  Q(s,a=1) = -0.508033\n",
      "  Q(s,a=2) = -1.312976\n",
      "  OPTIMAL ACTION (demo) = 0\n",
      "\n",
      "State 023 (y=2, x=3, d=3) [EMPTY] \n",
      "  V(s) = -0.343645\n",
      "  Q(s,a=0) = -0.363389\n",
      "  Q(s,a=1) = -0.343645\n",
      "  Q(s,a=2) = -1.139910\n",
      "  OPTIMAL ACTION (demo) = 1\n",
      "\n",
      "State 024 (y=3, x=1, d=0) [EMPTY] \n",
      "  V(s) = -1.155024\n",
      "  Q(s,a=0) = -1.799880\n",
      "  Q(s,a=1) = -1.799880\n",
      "  Q(s,a=2) = -1.155024\n",
      "  OPTIMAL ACTION (demo) = 2\n",
      "\n",
      "State 025 (y=3, x=1, d=1) [EMPTY] \n",
      "  V(s) = -1.488994\n",
      "  Q(s,a=0) = -1.488994\n",
      "  Q(s,a=1) = -2.107657\n",
      "  Q(s,a=2) = -1.799880\n",
      "  OPTIMAL ACTION (demo) = 0\n",
      "\n",
      "State 026 (y=3, x=1, d=2) [EMPTY] \n",
      "  V(s) = -1.799880\n",
      "  Q(s,a=0) = -1.799880\n",
      "  Q(s,a=1) = -1.799880\n",
      "  Q(s,a=2) = -2.107657\n",
      "  OPTIMAL ACTION (demo) = 0\n",
      "\n",
      "State 027 (y=3, x=1, d=3) [EMPTY] \n",
      "  V(s) = -1.488994\n",
      "  Q(s,a=0) = -2.107657\n",
      "  Q(s,a=1) = -1.488994\n",
      "  Q(s,a=2) = -2.269013\n",
      "  OPTIMAL ACTION (demo) = 1\n",
      "\n",
      "State 028 (y=3, x=2, d=0) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 029 (y=3, x=2, d=1) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 030 (y=3, x=2, d=2) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 031 (y=3, x=2, d=3) [LAVA] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 032 (y=3, x=3, d=0) [GOAL] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 033 (y=3, x=3, d=1) [GOAL] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 034 (y=3, x=3, d=2) [GOAL] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n",
      "\n",
      "State 035 (y=3, x=3, d=3) [GOAL] TERMINAL\n",
      "  V(s) =  0.000000\n",
      "  (terminal state, no actions)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.manual_control import ManualControl\n",
    "\n",
    "# ======================================================\n",
    "# Utils\n",
    "# ======================================================\n",
    "def l2_normalize(w, eps=1e-8):\n",
    "    n = np.linalg.norm(w)\n",
    "    return w if n < eps else w / n\n",
    "# ======================================================\n",
    "# Environment\n",
    "# ======================================================\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=MissionSpace(lambda: \"grand mission\"),\n",
    "            grid_size=size,\n",
    "            see_through_walls=True,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # vertical lava column at x=2, y=1..height-2\n",
    "        for y in range(1, height - 1):\n",
    "            self.grid.set(2, y, Lava())\n",
    "\n",
    "        # goal\n",
    "        self.put_obj(Goal(), width - 2, height - 2)\n",
    "\n",
    "        # agent\n",
    "        self.agent_pos = self.agent_start_pos  # (x,y)\n",
    "        self.agent_dir = self.agent_start_dir\n",
    "        self.mission = \"grand mission\"\n",
    "\n",
    "# ======================================================\n",
    "# Feature extraction (state-based, canonical)\n",
    "# ======================================================\n",
    "\n",
    "DIR_TO_VEC = {\n",
    "    0: (1, 0),   # right\n",
    "    1: (0, 1),   # down\n",
    "    2: (-1, 0),  # left\n",
    "    3: (0, -1),  # up\n",
    "}\n",
    "\n",
    "W_MAP = {\n",
    "    \"L1.2\": np.array([-0.05, -2.0, -0.01]),                       # [dist, on_lava, step]\n",
    "    \"L1.3\": np.array([-0.8, -0.1, -5.0, -0.05]),                 # [dist, lava_ahead, on_lava, step]\n",
    "    \"L2.1\": np.array([-0.05, -0.3, -2.0, -0.01]),                 # [dist, num_lava4, on_lava, step]\n",
    "    \"L2.3\": np.array([-0.05, -0.4, -2.0, -0.01]),                 # [dist, lava_nearby8, on_lava, step]\n",
    "    \"L3.1\": np.array([-0.02, -0.02, -0.5, -2.0, -0.01]),          # [dx, dy, lava_ahead, on_lava, step]\n",
    "    \"L4.1\": np.array([-0.05,  0.10, -2.0, -0.01]),                # [dist, min_dist_lava, on_lava, step]\n",
    "}\n",
    "\n",
    "FEATURE_SET = \"L1.3\"   # <<< CHANGE HERE\n",
    "\n",
    "def manhattan(p, q):\n",
    "    return abs(p[0] - q[0]) + abs(p[1] - q[1])\n",
    "\n",
    "def lava_ahead_state(lava_mask: np.ndarray, y: int, x: int, direction: int) -> int:\n",
    "    dx, dy = DIR_TO_VEC[direction]\n",
    "    ny, nx = y + dy, x + dx\n",
    "    if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "        return int(lava_mask[ny, nx])\n",
    "    return 0\n",
    "\n",
    "def on_lava_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    return int(lava_mask[y, x])\n",
    "\n",
    "def num_lava_4_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    cnt = 0\n",
    "    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "        ny, nx = y+dy, x+dx\n",
    "        if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "            cnt += int(lava_mask[ny, nx])\n",
    "    return cnt\n",
    "\n",
    "def lava_nearby_8_state(lava_mask: np.ndarray, y: int, x: int) -> int:\n",
    "    for dy in [-1,0,1]:\n",
    "        for dx in [-1,0,1]:\n",
    "            if dy == 0 and dx == 0:\n",
    "                continue\n",
    "            ny, nx = y+dy, x+dx\n",
    "            if 0 <= ny < lava_mask.shape[0] and 0 <= nx < lava_mask.shape[1]:\n",
    "                if lava_mask[ny, nx]:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "def min_dist_to_lava_state(lava_cells: np.ndarray, y: int, x: int, size: int) -> float:\n",
    "    if lava_cells.size == 0:\n",
    "        return float(2 * size)\n",
    "    return float(np.min(np.abs(lava_cells[:, 0] - y) + np.abs(lava_cells[:, 1] - x)))\n",
    "\n",
    "def phi_from_state(state, goal_yx, lava_mask, lava_cells, size):\n",
    "    \"\"\"\n",
    "    state = (y, x, dir)\n",
    "    goal_yx = (gy, gx)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "    gy, gx = goal_yx\n",
    "\n",
    "    dist = manhattan((y, x), (gy, gx))\n",
    "    step = 1.0\n",
    "\n",
    "    if FEATURE_SET == \"L1.2\":\n",
    "        return np.array([dist, on_lava_state(lava_mask, y, x), step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L1.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.1\":\n",
    "        return np.array([dist,\n",
    "                         num_lava_4_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L2.3\":\n",
    "        return np.array([dist,\n",
    "                         lava_nearby_8_state(lava_mask, y, x),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L3.1\":\n",
    "        dx = gx - x\n",
    "        dy = gy - y\n",
    "        return np.array([dx, dy,\n",
    "                         lava_ahead_state(lava_mask, y, x, direction),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    if FEATURE_SET == \"L4.1\":\n",
    "        return np.array([dist,\n",
    "                         min_dist_to_lava_state(lava_cells, y, x, size),\n",
    "                         on_lava_state(lava_mask, y, x),\n",
    "                         step], dtype=float)\n",
    "\n",
    "    raise ValueError(f\"Unknown FEATURE_SET: {FEATURE_SET}\")\n",
    "\n",
    "def reward_from_state(state, goal_yx, lava_mask, lava_cells, size) -> float:\n",
    "    phi = phi_from_state(state, goal_yx, lava_mask, lava_cells, size)\n",
    "    W = l2_normalize(W_MAP[FEATURE_SET])\n",
    "    return float(W @ phi)\n",
    "\n",
    "# ======================================================\n",
    "# Planning dynamics model (left/right/forward)\n",
    "# ======================================================\n",
    "ACT_LEFT = 0\n",
    "ACT_RIGHT = 1\n",
    "ACT_FORWARD = 2\n",
    "ACTIONS = [ACT_LEFT, ACT_RIGHT, ACT_FORWARD]\n",
    "\n",
    "def build_static_maps(env: SimpleEnv):\n",
    "    \"\"\"\n",
    "    Returns everything in NumPy (y,x) convention.\n",
    "    \"\"\"\n",
    "    size = env.width\n",
    "    wall_mask = np.zeros((size, size), dtype=bool)\n",
    "    lava_mask = np.zeros((size, size), dtype=bool)\n",
    "    goal_yx = None\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            obj = env.grid.get(x, y)  # MiniGrid uses (x,y)\n",
    "            if obj is None:\n",
    "                continue\n",
    "            if isinstance(obj, Wall):\n",
    "                wall_mask[y, x] = True\n",
    "            elif isinstance(obj, Lava):\n",
    "                lava_mask[y, x] = True\n",
    "            elif isinstance(obj, Goal):\n",
    "                goal_yx = (y, x)\n",
    "\n",
    "    if goal_yx is None:\n",
    "        raise RuntimeError(\"Goal not found in grid\")\n",
    "\n",
    "    lava_cells = np.argwhere(lava_mask)\n",
    "    return size, wall_mask, lava_mask, lava_cells, goal_yx\n",
    "\n",
    "def is_terminal_state(state, goal_yx, lava_mask) -> bool:\n",
    "    y, x, _ = state\n",
    "    return (y, x) == goal_yx or bool(lava_mask[y, x])\n",
    "    #return False\n",
    "\n",
    "def step_model(state, action, wall_mask, goal_yx, lava_mask):\n",
    "    \"\"\"\n",
    "    Deterministic transition for planning.\n",
    "    state = (y, x, dir)\n",
    "    \"\"\"\n",
    "    y, x, direction = state\n",
    "\n",
    "    if is_terminal_state(state, goal_yx, lava_mask):\n",
    "        return state, True\n",
    "\n",
    "    if action == ACT_LEFT:\n",
    "        nstate = (y, x, (direction - 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_RIGHT:\n",
    "        nstate = (y, x, (direction + 1) % 4)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    if action == ACT_FORWARD:\n",
    "        dx, dy = DIR_TO_VEC[direction]\n",
    "        ny, nx = y + dy, x + dx\n",
    "\n",
    "        # bounds/wall => no move\n",
    "        if ny < 0 or ny >= wall_mask.shape[0] or nx < 0 or nx >= wall_mask.shape[1]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        if wall_mask[ny, nx]:\n",
    "            nstate = (y, x, direction)\n",
    "            return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "        nstate = (ny, nx, direction)\n",
    "        return nstate, is_terminal_state(nstate, goal_yx, lava_mask)\n",
    "\n",
    "    raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "def enumerate_states(size, wall_mask):\n",
    "    \"\"\"\n",
    "    All non-wall cells for all 4 directions.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if wall_mask[y, x]:\n",
    "                continue\n",
    "            for d in range(4):\n",
    "                states.append((y, x, d))\n",
    "    return states\n",
    "\n",
    "def build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Tabular MDP for BIRL.\n",
    "    \"\"\"\n",
    "    S = len(states)\n",
    "    A = len(ACTIONS)\n",
    "\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "    T = np.zeros((S, A, S))\n",
    "    terminal_mask = np.zeros(S, dtype=bool)\n",
    "    Phi = np.zeros((S, len(W_MAP[FEATURE_SET])))\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        terminal_mask[i] = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        Phi[i] = phi_from_state(s, goal_yx, lava_mask, lava_cells, size)\n",
    "\n",
    "        for a_idx, a in enumerate(ACTIONS):\n",
    "            sp, _ = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            T[i, a_idx, idx_of[sp]] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"idx_of\": idx_of,\n",
    "        \"T\": T,\n",
    "        \"Phi\": Phi,\n",
    "        \"terminal\": terminal_mask,\n",
    "        \"gamma\": gamma,\n",
    "        \"goal_yx\": goal_yx,\n",
    "        \"lava_mask\": lava_mask,\n",
    "        \"wall_mask\": wall_mask,\n",
    "        \"lava_cells\": lava_cells,\n",
    "        \"size\": size,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# Value Iteration\n",
    "# ======================================================\n",
    "\n",
    "def value_iteration(\n",
    "    states,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    lava_cells,\n",
    "    size,\n",
    "    gamma=0.99,\n",
    "    theta=1e-8,\n",
    "    max_iters=20000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reward computed from next state s' (as in your earlier code).\n",
    "    \"\"\"\n",
    "    idx_of = {s: i for i, s in enumerate(states)}\n",
    "    V = np.zeros(len(states), dtype=float)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for i, s in enumerate(states):\n",
    "            if is_terminal_state(s, goal_yx, lava_mask):\n",
    "                continue\n",
    "\n",
    "            best = -1e18\n",
    "            for a in ACTIONS:\n",
    "                sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "                r = reward_from_state(sp, goal_yx, lava_mask, lava_cells, size)\n",
    "                j = idx_of[sp]\n",
    "                q = r + (0.0 if done else gamma * V[j])\n",
    "                if q > best:\n",
    "                    best = q\n",
    "\n",
    "            delta = max(delta, abs(best - V[i]))\n",
    "            V[i] = best\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    pi = np.zeros(len(states), dtype=int)\n",
    "    for i, s in enumerate(states):\n",
    "        if is_terminal_state(s, goal_yx, lava_mask):\n",
    "            pi[i] = ACT_FORWARD\n",
    "            continue\n",
    "\n",
    "        best_a = ACTIONS[0]\n",
    "        best_q = -1e18\n",
    "        for a in ACTIONS:\n",
    "            sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            r = reward_from_state(sp, goal_yx, lava_mask, lava_cells, size)\n",
    "            j = idx_of[sp]\n",
    "            q = r + (0.0 if done else gamma * V[j])\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "        pi[i] = best_a\n",
    "\n",
    "    return V, pi, idx_of\n",
    "\n",
    "def generate_state_action_demos(states, pi, terminal_mask):\n",
    "    demos = []\n",
    "    for i, _s in enumerate(states):\n",
    "        if terminal_mask[i]:\n",
    "            continue\n",
    "        demos.append((i, int(pi[i])))\n",
    "    return demos\n",
    "\n",
    "# ======================================================\n",
    "# BIRL (demo-only, MCMC over theta)\n",
    "# ======================================================\n",
    "class DemoOnlyBIRL:\n",
    "    def __init__(self, T, Phi, gamma, demos, beta=1.0):\n",
    "        self.T = T\n",
    "        self.Phi = Phi\n",
    "        self.gamma = gamma\n",
    "        self.demos = demos\n",
    "        self.beta = beta\n",
    "        self.S, self.A, _ = T.shape\n",
    "        self.D = Phi.shape[1]\n",
    "\n",
    "    def _q_values(self, theta):\n",
    "        \"\"\"\n",
    "        CONSISTENT with next-state reward value iteration:\n",
    "        Q(s,a) = sum_{s'} T[s,a,s'] * ( r[s'] + gamma * V[s'] )\n",
    "        \"\"\"\n",
    "        r = self.Phi @ theta            # r[s]\n",
    "        Q = np.zeros((self.S, self.A))\n",
    "        V = np.zeros(self.S)\n",
    "\n",
    "        # --- Value iteration for V under next-state reward ---\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "            for s in range(self.S):\n",
    "                q_vals = []\n",
    "                for a in range(self.A):\n",
    "                    q_sa = np.sum(self.T[s, a] * (r + self.gamma * V))\n",
    "                    q_vals.append(q_sa)\n",
    "\n",
    "                v_new = max(q_vals)\n",
    "                delta = max(delta, abs(V[s] - v_new))\n",
    "                V[s] = v_new\n",
    "\n",
    "            if delta < 1e-8:\n",
    "                break\n",
    "\n",
    "        # --- Compute Q using converged V ---\n",
    "        for s in range(self.S):\n",
    "            for a in range(self.A):\n",
    "                Q[s, a] = np.sum(self.T[s, a] * (r + self.gamma * V))\n",
    "\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        theta = l2_normalize(theta)\n",
    "        Q = self._q_values(theta)\n",
    "\n",
    "        ll = 0.0\n",
    "        for s, a in self.demos:\n",
    "            logits = self.beta * Q[s]\n",
    "            # stable log-sum-exp would be nicer, but keep simple:\n",
    "            ll += self.beta * Q[s, a] - np.log(np.sum(np.exp(logits)))\n",
    "        return ll\n",
    "\n",
    "    def run(self, num_samples=1000, step_size=0.3, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        theta = l2_normalize(rng.normal(0, 1, self.D))\n",
    "        ll = self.log_likelihood(theta)\n",
    "\n",
    "        best_theta = theta.copy()\n",
    "        best_ll = ll\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            prop = l2_normalize(theta + rng.normal(0, step_size, self.D))\n",
    "            prop_ll = self.log_likelihood(prop)\n",
    "\n",
    "            if prop_ll > ll or rng.random() < np.exp(prop_ll - ll):\n",
    "                theta, ll = prop, prop_ll\n",
    "\n",
    "            if ll > best_ll:\n",
    "                best_theta, best_ll = theta.copy(), ll\n",
    "\n",
    "        return best_theta\n",
    "\n",
    "# ======================================================\n",
    "# Evaluation\n",
    "# ======================================================\n",
    "# ======================================================\n",
    "# Evaluation (CONSISTENT with your Value Iteration)\n",
    "# ======================================================\n",
    "def expected_value_difference(T, gamma, r_true, pi_true, pi_eval, terminal_mask):\n",
    "    \"\"\"\n",
    "    Make policy evaluation consistent with YOUR value_iteration:\n",
    "      - reward is on NEXT state s' (like reward_from_state(sp))\n",
    "      - terminal transitions cut off future (0 if done)\n",
    "      - terminal states are absorbing in step_model (already), but we also skip updating them\n",
    "    Inputs:\n",
    "      T: (S,A,S) deterministic/stochastic transitions\n",
    "      r_true: length S, interpreted as r_next[sp] (reward of NEXT state)\n",
    "      terminal_mask: length S boolean, True if state is terminal\n",
    "    \"\"\"\n",
    "    S = T.shape[0]\n",
    "    def policy_eval(policy):\n",
    "        V = np.zeros(S, dtype=float)\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "            for s in range(S):\n",
    "                if terminal_mask[s]:\n",
    "                    # consistent with VI: terminal value is 0 and no future\n",
    "                    continue\n",
    "\n",
    "                a = int(policy[s])\n",
    "\n",
    "                # Bellman expectation with NEXT-state reward:\n",
    "                # V(s) = sum_{s'} T[s,a,s'] * ( r_true[s'] + gamma * V(s') )\n",
    "                v_new = float(np.sum(T[s, a] * (r_true + gamma * V)))\n",
    "\n",
    "                delta = max(delta, abs(V[s] - v_new))\n",
    "                V[s] = v_new\n",
    "\n",
    "            if delta < 1e-8:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    V_opt  = policy_eval(pi_true)\n",
    "    V_eval = policy_eval(pi_eval)\n",
    "    return float(np.mean(V_opt) - np.mean(V_eval))\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Debug printing (state-based)\n",
    "# =====================================================\n",
    "def debug_print_values_and_demos(\n",
    "    states,\n",
    "    idx_of,\n",
    "    V,\n",
    "    pi,\n",
    "    wall_mask,\n",
    "    goal_yx,\n",
    "    lava_mask,\n",
    "    lava_cells,\n",
    "    size,\n",
    "    gamma=0.99,\n",
    "):\n",
    "    print(\"\\n===== DEBUG: V(s), Q(s,a), DEMOS =====\")\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        y, x, d = s\n",
    "\n",
    "        if wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        term = is_terminal_state(s, goal_yx, lava_mask)\n",
    "        term_str = \"TERMINAL\" if term else \"\"\n",
    "\n",
    "        print(f\"\\nState {i:03d} (y={y}, x={x}, d={d}) [{cell_type}] {term_str}\")\n",
    "        print(f\"  V(s) = {V[i]: .6f}\")\n",
    "\n",
    "        if cell_type == \"WALL\":\n",
    "            print(\"  ⚠️ ERROR: WALL state present in state space!\")\n",
    "            continue\n",
    "        if term:\n",
    "            print(\"  (terminal state, no actions)\")\n",
    "            continue\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            r = reward_from_state(sp, goal_yx, lava_mask, lava_cells, size)\n",
    "            j = idx_of[sp]\n",
    "            q = r + (0.0 if done else gamma * V[j])\n",
    "            print(f\"  Q(s,a={a}) = {q: .6f}\")\n",
    "\n",
    "        print(f\"  OPTIMAL ACTION (demo) = {pi[i]}\")\n",
    "\n",
    "# ======================================================\n",
    "# Optional manual control (no SymbolicObsWrapper)\n",
    "# ======================================================\n",
    "\n",
    "class ManualControlStateReward(ManualControl):\n",
    "    def __init__(self, env, wall_mask, lava_mask, lava_cells, goal_yx, size):\n",
    "        super().__init__(env)\n",
    "        self.wall_mask = wall_mask\n",
    "        self.lava_mask = lava_mask\n",
    "        self.lava_cells = lava_cells\n",
    "        self.goal_yx = goal_yx\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, action):\n",
    "        _obs, _rew, terminated, truncated, _info = self.env.step(action)\n",
    "\n",
    "        x, y = self.env.unwrapped.agent_pos  # env uses (x,y)\n",
    "        d = int(self.env.unwrapped.agent_dir)\n",
    "        s = (y, x, d)\n",
    "\n",
    "        if self.wall_mask[y, x]:\n",
    "            cell_type = \"WALL\"\n",
    "        elif (y, x) == self.goal_yx:\n",
    "            cell_type = \"GOAL\"\n",
    "        elif self.lava_mask[y, x]:\n",
    "            cell_type = \"LAVA\"\n",
    "        else:\n",
    "            cell_type = \"EMPTY\"\n",
    "\n",
    "        phi = phi_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "        R = reward_from_state(s, self.goal_yx, self.lava_mask, self.lava_cells, self.size)\n",
    "\n",
    "        print(f\"STATE {s} [{cell_type}]  φ(s)={phi}  R(s)={R:.4f}\")\n",
    "\n",
    "        return _obs, 0.0, terminated, truncated, _info\n",
    "\n",
    "# ======================================================\n",
    "# Main (BIRL experiment)\n",
    "# ======================================================\n",
    "\n",
    "def birl_main():\n",
    "    # --- build env once for map extraction / planning\n",
    "    planning_env = SimpleEnv(render_mode=None)\n",
    "    planning_env.reset(seed=0)\n",
    "\n",
    "    size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(planning_env)\n",
    "    states = enumerate_states(size, wall_mask)\n",
    "\n",
    "    # --- ground truth planning under your FEATURE_SET and W_MAP\n",
    "    V_true, pi_true, idx_of = value_iteration(\n",
    "        states, wall_mask, goal_yx, lava_mask, lava_cells, size\n",
    "    )\n",
    "\n",
    "    # --- tabular mdp\n",
    "    mdp = build_tabular_mdp(states, wall_mask, goal_yx, lava_mask, lava_cells, size)\n",
    "\n",
    "    # --- demos from optimal policy\n",
    "    demos = generate_state_action_demos(states, pi_true, mdp[\"terminal\"])\n",
    "\n",
    "    # --- run BIRL\n",
    "    birl = DemoOnlyBIRL(mdp[\"T\"], mdp[\"Phi\"], mdp[\"gamma\"], demos, beta=1.0)\n",
    "    theta_hat = birl.run(num_samples=1000, step_size=0.25, seed=0)\n",
    "\n",
    "    # --- learned reward and greedy policy (simple one-step greedy rollout using VI-style Q)\n",
    "    r_hat = mdp[\"Phi\"] @ theta_hat\n",
    "\n",
    "    # compute a policy by running VI with learned weights (cleanest)\n",
    "    # but you can also compute greedy wrt V_hat; keep simple: reuse value_iteration by temporarily swapping W_MAP\n",
    "    # We'll do a small helper instead:\n",
    "    V_hat = np.zeros(len(states))\n",
    "    pi_hat = np.zeros(len(states), dtype=int)\n",
    "\n",
    "    # Evaluate greedy using Bellman backup with fixed V_hat iteration (NEXT-STATE reward)\n",
    "    for _ in range(500):\n",
    "        delta = 0.0\n",
    "        for i, s in enumerate(states):\n",
    "            if mdp[\"terminal\"][i]:\n",
    "                continue\n",
    "\n",
    "            best = -1e18\n",
    "            for a in ACTIONS:\n",
    "                sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "                j = idx_of[sp]\n",
    "\n",
    "                q = r_hat[j] + (0.0 if done else mdp[\"gamma\"] * V_hat[j])\n",
    "\n",
    "                if q > best:\n",
    "                    best = q\n",
    "\n",
    "            delta = max(delta, abs(best - V_hat[i]))\n",
    "            V_hat[i] = best\n",
    "\n",
    "        if delta < 1e-10:\n",
    "            break\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        if mdp[\"terminal\"][i]:\n",
    "            pi_hat[i] = ACT_FORWARD\n",
    "            continue\n",
    "\n",
    "        best_a = ACTIONS[0]\n",
    "        best_q = -1e18\n",
    "        for a in ACTIONS:\n",
    "            sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            j = idx_of[sp]\n",
    "\n",
    "            q = r_hat[j] + (0.0 if done else mdp[\"gamma\"] * V_hat[j])\n",
    "\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "\n",
    "        pi_hat[i] = best_a\n",
    "\n",
    "    # --- true reward vector over states (consistent with normalized theta used in reward_from_state)\n",
    "    r_true = mdp[\"Phi\"] @ l2_normalize(W_MAP[FEATURE_SET])\n",
    "\n",
    "    # --- EVD\n",
    "    evd = expected_value_difference(mdp[\"T\"], mdp[\"gamma\"], r_true, pi_true, pi_hat, mdp[\"terminal\"])\n",
    "    print(\"\\n===== BIRL RESULTS (SYMBOLIC-FREE) =====\")\n",
    "    print(\"FEATURE_SET :\", FEATURE_SET)\n",
    "    print(\"True θ      :\", l2_normalize(W_MAP[FEATURE_SET]))\n",
    "    print(\"Learned θ   :\", theta_hat)\n",
    "    print(\"EVD         :\", evd)\n",
    "\n",
    "    # Optional: debug state-wise V/Q/demos\n",
    "    debug_print_values_and_demos(\n",
    "        states=states,\n",
    "        idx_of=idx_of,\n",
    "        V=V_true,\n",
    "        pi=pi_true,\n",
    "        wall_mask=wall_mask,\n",
    "        goal_yx=goal_yx,\n",
    "        lava_mask=lava_mask,\n",
    "        lava_cells=lava_cells,\n",
    "        size=size,\n",
    "        gamma=mdp[\"gamma\"],\n",
    "    )\n",
    "\n",
    "    # Optional: interactive manual play with state-based printing\n",
    "    # env = SimpleEnv(render_mode=\"human\")\n",
    "    # env.reset(seed=0)\n",
    "    # manual = ManualControlStateReward(env, wall_mask, lava_mask, lava_cells, goal_yx, size)\n",
    "    # manual.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    birl_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c54450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# TEST: Value Iteration Debug Print\n",
    "# (Run this in a NEW notebook cell)\n",
    "# ======================================================\n",
    "\n",
    "def test_value_iteration_debug(\n",
    "    size=5,\n",
    "    gamma=0.99,\n",
    "    max_states=None,   # set to int to limit printing\n",
    "):\n",
    "    # --- Build env (no render)\n",
    "    env = SimpleEnv(render_mode=None, size=size)\n",
    "    env.reset(seed=0)\n",
    "\n",
    "    # --- Extract static maps\n",
    "    size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(env)\n",
    "\n",
    "    # --- Enumerate state space\n",
    "    states = enumerate_states(size, wall_mask)\n",
    "\n",
    "    # --- Run Value Iteration\n",
    "    V, pi, idx_of = value_iteration(\n",
    "        states,\n",
    "        wall_mask,\n",
    "        goal_yx,\n",
    "        lava_mask,\n",
    "        lava_cells,\n",
    "        size,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=====================================================\")\n",
    "    print(\" VALUE ITERATION DEBUG OUTPUT\")\n",
    "    print(\" FEATURE_SET =\", FEATURE_SET)\n",
    "    print(\" gamma =\", gamma)\n",
    "    print(\" #states =\", len(states))\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "    printed = 0\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        if max_states is not None and printed >= max_states:\n",
    "            break\n",
    "\n",
    "        y, x, d = s\n",
    "\n",
    "        # --- semantic cell type\n",
    "        if wall_mask[y, x]:\n",
    "            cell = \"WALL\"\n",
    "        elif (y, x) == goal_yx:\n",
    "            cell = \"GOAL\"\n",
    "        elif lava_mask[y, x]:\n",
    "            cell = \"LAVA\"\n",
    "        else:\n",
    "            cell = \"EMPTY\"\n",
    "\n",
    "        terminal = is_terminal_state(s, goal_yx, lava_mask)\n",
    "\n",
    "        print(\"\\n---------------------------------------------\")\n",
    "        print(f\"State {i:03d} : (y={y}, x={x}, dir={d})\")\n",
    "        print(f\"Cell Type   : {cell}\")\n",
    "        print(f\"Terminal    : {terminal}\")\n",
    "        print(f\"V(s)        : {V[i]: .6f}\")\n",
    "\n",
    "        if terminal:\n",
    "            print(\"  (terminal state — no Q-values)\")\n",
    "            printed += 1\n",
    "            continue\n",
    "\n",
    "        # --- Q(s,a)\n",
    "        for a in ACTIONS:\n",
    "            sp, done = step_model(s, a, wall_mask, goal_yx, lava_mask)\n",
    "            j = idx_of[sp]\n",
    "            r = reward_from_state(sp, goal_yx, lava_mask, lava_cells, size)\n",
    "            q = r + (0.0 if done else gamma * V[j])\n",
    "\n",
    "            print(\n",
    "                f\"  Q(s,a={a}) = {q: .6f}   \"\n",
    "                f\"-> s'={sp}, R(s')={r: .4f}, done={done}\"\n",
    "            )\n",
    "\n",
    "        print(f\"  Optimal a* : {pi[i]}\")\n",
    "        printed += 1\n",
    "\n",
    "    print(\"\\n=====================================================\")\n",
    "    print(\" END VALUE ITERATION DEBUG\")\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Run the test\n",
    "# ======================================================\n",
    "\n",
    "test_value_iteration_debug(\n",
    "    size=5,\n",
    "    gamma=0.99,\n",
    "    max_states=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96202c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fa213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c5cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall, Lava\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.wrappers import SymbolicObsWrapper\n",
    "from minigrid.manual_control import ManualControl\n",
    "\n",
    "\n",
    "def l2_normalize(w, eps=1e-8):\n",
    "    n = np.linalg.norm(w)\n",
    "    return w if n < eps else w / n\n",
    "\n",
    "DIR_TO_VEC = {\n",
    "    0: (1, 0),    # right\n",
    "    1: (0, 1),    # down\n",
    "    2: (-1, 0),   # left\n",
    "    3: (0, -1),   # up\n",
    "}\n",
    "\n",
    "\n",
    "FEATURE_SET = \"L1.3\"\n",
    "\n",
    "W_MAP = {\n",
    "    \"L1.3\": np.array([   # [dist, lava_ahead, on_lava, step]\n",
    "        -0.05,\n",
    "        -0.5,\n",
    "        -2.0,\n",
    "        -0.01\n",
    "    ])\n",
    "}\n",
    "\n",
    "class LinearRewardEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=MissionSpace(lambda: \"Reach goal, avoid lava\"),\n",
    "            grid_size=size,\n",
    "            see_through_walls=True,\n",
    "            max_steps=4 * size * size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # vertical lava column\n",
    "        for i in range(height - 2):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            else:\n",
    "                self.grid.set(2, i, Lava())\n",
    "\n",
    "        self.put_obj(Goal(), width - 2, height - 2)\n",
    "\n",
    "        self.agent_pos = self.agent_start_pos\n",
    "        self.agent_dir = self.agent_start_dir\n",
    "        self.mission = \"Linear reward MiniGrid\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af808f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LinearRewardEnv()\n",
    "\n",
    "env.reset(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_static_maps(env: SimpleEnv):\n",
    "    \"\"\"\n",
    "    Extract:\n",
    "      - size\n",
    "      - wall mask\n",
    "      - lava mask\n",
    "      - goal position\n",
    "    \"\"\"\n",
    "    size = env.width  # square\n",
    "    wall_mask = np.zeros((size, size), dtype=bool)\n",
    "    lava_mask = np.zeros((size, size), dtype=bool)\n",
    "    goal_yx = None\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            obj = env.grid.get(x, y)\n",
    "            if obj is None:\n",
    "                continue\n",
    "            if isinstance(obj, Wall):\n",
    "                wall_mask[y, x] = True\n",
    "            elif isinstance(obj, Lava):\n",
    "                lava_mask[y, x] = True\n",
    "            elif isinstance(obj, Goal):\n",
    "                goal_yx = (y, x)\n",
    "\n",
    "    if goal_yx is None:\n",
    "        raise RuntimeError(\"Goal not found in grid\")\n",
    "\n",
    "    lava_cells = np.argwhere(lava_mask)\n",
    "    return size, wall_mask, lava_mask, lava_cells, goal_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "size, wall_mask, lava_mask, lava_cells, goal_yx = build_static_maps(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf5a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
